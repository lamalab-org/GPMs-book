% token -> added
% supervised learning -> added
% unsupervised learning -> added
% self-supervised learning -> added
% reinforcement learning -> added
% regular expression -> added
% embedding -> added
% loss function -> added
% GPM -> added
% computational scaling -> added
% de novo design -> added
% latent space -> added
% (soft) inductive bias -> added
% (hard) inductive bias -> added
% modality -> added
% latent fusion -> added
% state space -> added
% prompting -> added
% aligning (embeddings/representations) -> added
% gating -> added
% regularization -> added
% context/context window -> added
% agent -> added
% temperature -> added
% benchmark-> added
% semantic search -> added
% chunk -> added
% fused representation -> added
% hallucination -> added
% compositionality -> added 
% dense/sparse -> added

\newglossaryentry{alignmentg}{
  name={alignment},
  description={The process of ensuring that a machine learning model's behavior is consistent with human intentions, values, or task objectives. In the context of \glspl{llm}, alignment often involves \gls{finetuningg} with \glslink{rlhfg}{human feedback} to produce helpful, honest, and harmless outputs. Alignment can also refer to the process of aligning representations or \glspl{embeddingg} across different \glslink{modalityg}{modalities}---such as text and images—--in \glslink{modalityg}{multi-modal} systems, enabling meaningful cross-modal reasoning and retrieval.}
}


\newglossaryentry{agentg}{
  name={agent},
  description={An entity that interacts with an environment by taking actions based on observations to achieve a goal. In \gls{rlg}, agents learn from feedback to optimize behavior over time. In the context of \glspl{llm}, agents can use tools, retrieve external information, and perform multi-step reasoning to complete complex, goal-oriented tasks.}
}

\newglossaryentry{apig}{
  name={application programming interface},
  description={A set of rules and protocols that allow different software systems to communicate with each other. APIs enable developers to access the functionality of external services or applications programmatically, often over a network. For example, the PubChem API \autocite{pubchemAPI} allows programmatic access to chemical compound data, enabling automated retrieval of molecular structures and properties.}
}

\newglossaryentry{benchmarkg}{
  name={benchmark},
  description={A standardized task or dataset used to evaluate and compare the performance of models or methods. Benchmarks help assess progress and identify strengths and weaknesses of different approaches.}
}

\newglossaryentry{bfsg}{
  name={breadth-first search},
  description={A graph traversal algorithm that explores all neighbors of a node before moving to the next level of nodes. It is often used to find the shortest path in unweighted graphs.}
}

\newglossaryentry{bleug}{
  name={bilingual evaluation understudy},
  description={A metric for evaluating the quality of text generated by a model by comparing it to one or more reference texts. BLEU measures \gls{ngramg} overlap between the generated output and the reference. Higher BLEU scores indicate closer matches.}
}

\newglossaryentry{bog}{
  name={Bayesian optimization},
  description={A global optimization strategy for expensive black-box functions. It does not require access to derivatives of the objective function. BO builds a probabilistic surrogate model, typically a \gls{gpg}, to guide the selection of query points by balancing exploration and exploitation.\autocite{frazier2018tutorial}}
}

\newglossaryentry{cnng}{
  name={convolutional neural network},
  description={A type of \gls{nng} designed to process data with a grid-like structure, such as images. CNNs use convolutional layers to automatically learn spatial hierarchies of features from the input.}
}

\newglossaryentry{cotg}{
  name={chain-of-thought}, 
  description={A \gls{promptingg} strategy that involves a series of intermediate natural-language reasoning steps that lead to the final output.\autocite{wei2022chain} CoT encourages an \gls{llm} to explain its reasoning step by step (e.g., by \gls{promptingg} it to \enquote{think step by step}), and it is intended to improve the ability of \glspl{llm} to perform complex reasoning.}
}

\newglossaryentry{compositionalityg}{
  name={compositionality},
  description={When a complex structure can be constructed by combining simpler components, it is said to have the compositional property. In the context of \gls{mlg}, a model or function $f(x,y)$ exhibits compositionality if it can be constructed from $h(x)$ and $k(y)$ as its components: $f(x,y) = g[h(x),k(y)]$, where $h$, $k$, and $g$ are learned functions. This structure allows modularity, parameter sharing, and generalization to novel input combinations by leveraging the learned behavior of the components. In many \gls{dlg} architectures, compositionality is achieved by the hierarchical application of simpler transformations, such as \glspl{fnng}.}
}

\newglossaryentry{comptscalingg}{
  name={computational scaling},
  description={The way in which the computational cost of an algorithm or model increases with problem size (e.g., the number of atoms, data points, or parameters). Understanding scaling behavior is important for assessing feasibility and performance at larger scales. For example, traditional implementations of \gls{dft} scale as $\mathcal{O}(N^3)$ with the number of basis functions, making large systems computationally expensive.}
}

\newglossaryentry{contextwindowg}{
  name={context window},
  description={The span of input \glspl{tokeng} that a model considers at once when generating predictions. In \glspl{llm}, the context window defines how much preceding (and possibly surrounding) text the model can attend to. Larger context windows allow the model to capture longer dependencies but increase computational cost.}
}


\newglossaryentry{damdg}{
  name={dry age-related macular degeneration},
  description={A chronic eye disease that causes gradual loss of central vision due to the thinning of the macula. dAMD is the more common and less severe form of age-related macular degeneration, typically progressing slowly over time.}
}

\newglossaryentry{dataaugmentationg}{
  name={data augmentation},
  description={A technique used to artificially increase the size and diversity of a training dataset by applying transformations or generating variations of the original data. Data augmentation helps improve model generalization and robustness.}
}


\newglossaryentry{datachunktg}{
  name={data chunk},
  description={A contiguous block or segment of data treated as a unit for processing, storage, or transmission. Data chunks are used to divide large datasets into manageable parts---for example, when streaming text or segmenting long sequences. Unlike a \emph{batch}, which typically refers to a set of independent samples processed together in training, a data chunk often preserves sequential or structural continuity within a single sample.}
}

\newglossaryentry{denovodesigng}{
  name={\textit{de novo} design},
  sort={de novo design},
  description={The process of generating novel molecules or materials from scratch, guided by desired properties or objectives rather than by modifying known structures.}
}

\newglossaryentry{densesparseg}{
  name={dense/sparse vector},
  description={Two categories of vector representations used in \gls{mlg} and data processing. \emph{Dense vectors} have most or all elements nonzero and are typically used for learned \glspl{embeddingg}. \emph{Sparse vectors} contain mostly zero values and are common in high-dimensional representations such as \glspl{oheg}. Sparse vectors are more memory-efficient when stored in specialized formats, while dense vectors are preferred for \glslink{nng}{neural computation}, as they contain richer information.}
}


\newglossaryentry{dfsg}{
  name={depth-first search},
  description={A graph traversal algorithm that explores as far as possible along each branch before backtracking. DFS is often used for pathfinding, cycle detection, and analyzing graph structures.}
}

\newglossaryentry{dlg}{
  name={deep learning},
  description={This subfield of \gls{mlg} uses \glspl{nng} with many layers to model complex patterns in data. Deep learning has enabled major advances in image recognition, natural language processing, and molecular modeling.}
}

\newglossaryentry{dorag}{
  name={weight-decomposed low-rank adaptation},
  description={An extension of \gls{lorag} that separates weight adaptation into magnitude and direction components.\autocite{liu2024dora} DoRA achieves efficient \gls{finetuningg} of \glspl{llm} by modifying only the direction of weights while preserving the pre-trained magnitude, improving stability and performance.}
}


\newglossaryentry{dpog}{
  name={direct preference optimization},
  description={A method for \gls{finetuningg} \glspl{llm} based on human preference data without using reinforcement learning. DPO directly optimizes the model (\gls{policyg}) to prefer outputs ranked higher by human annotators, simplifying the \gls{rlhf} pipeline.}
}

\newglossaryentry{dslg}{
  name={domain-specific language},
  description={A programming language tailored to a particular application domain. DSLs offer specialized syntax and abstractions that make it easier to express solutions within that domain, such as chemical synthesis.}
}

\newglossaryentry{eag}{
  name={evolutionary algorithm},
  description={This family of optimization algorithms is inspired by natural selection. EAs evolve a population of candidate solutions over multiple generations using operations such as mutation, crossover, and score-based selection to find high-performing solutions.}
}

\newglossaryentry{ecfpg}{
  name={extended connectivity fingerprint},
  description={A type of molecular fingerprint that represents chemical structures as binary or count vectors based on the local atomic environments. ECFPs are generated by iteratively hashing the neighborhoods of each atom up to a given radius, capturing information about connectivity and substructures. These fingerprints are invariant to atom indexing and are commonly used in \gls{mlg} pipelines for tasks such as virtual screening, molecular similarity, and property modeling. A widely used variant is ECFP4, which uses a radius of 2.}
}

\newglossaryentry{embeddingg}{
  name={embedding},
  description={A representation of discrete or high-dimensional data in a continuous vector space that preserves relevant relationships or structure. Embeddings are commonly used for words, molecules, graphs, and other symbolic data.}
}

\newglossaryentry{erag}{
  name={energy rank alignment},
  description={A method for aligning the training of generative models with energy-based evaluations. ERA encourages the model to assign higher probabilities to lower-energy (more favorable) configurations by matching the model's ranking of samples to their energy scores.}
}

\newglossaryentry{f1g}{
  name={F$_1$ score},
  description={This performance metric for classification tasks is aimed at balancing precision and recall. It is defined as the harmonic mean of precision and recall: $ F_1 = 2 \cdot \frac{\text{precision} \; \times \; \text{recall}}{\text{precision} \; + \; \text{recall}} $. The F$_1$ score ranges from $0$ to $1$. Since the harmonic mean is dominated by the smaller of the two numbers, a high F$_1$ score means that both precision and recall are high, making this metric useful when classes are imbalanced or when both false positives and false negatives are important.}
}

\newglossaryentry{finetuningg}{
  name={fine-tuning},
  description={The process of taking a pre-trained model and continuing its training on a smaller, task-specific dataset to adapt it to a particular application. Fine-tuning updates the model parameters to specialize its behavior while retaining the general knowledge learned during pre-training.}
}


\newglossaryentry{fnng}{
  name={feed-forward neural network},
  description={The simplest class of \glspl{nng} in which information flows in one direction from input to output through a series of connected layers, without cycles or feedback connections.}
}

\newglossaryentry{fusedrepresentationg}{
  name={fused representation},
  description={A joint representation that integrates information from multiple \glslink{modalityg}{modalities} into a single \glslink{latentspaceg}{embedding space}. See \glslink{latentfusiong}{\textbf{latent fusion}}.}
}

\newglossaryentry{gatingg}{
  name={gating mechanism},
  description={A \gls{nng} component that controls the flow of information by modulating one signal using another, typically via element-wise multiplication. After computing a gate vector $g(x)$, the gating mechanism applies it to an input signal $z$ as $z' = g(x) \odot z$, where $\odot$ denotes element-wise multiplication. This allows the model to selectively suppress or pass through different components of $z$ based on the learned gating function $g$. Gating mechanisms are central to architectures like \glspl{lstm}, where they regulate memory updates, retention, and output generation.}
}

\newglossaryentry{gag}{
  name={genetic algorithm},
  description={This subset of \glspl{eag} model candidate solutions as individuals represented by strings of \enquote{genes} (e.g., binary digits or symbols). Unlike broader \glspl{eag}, GAs focus heavily on genetic representations and recombination to explore the search space.}
}

\newglossaryentry{gnng}{
  name={graph neural network},
  description={A type of \gls{nng} architecture designed to operate on graph-structured data. GNNs learn representations by passing and aggregating information between neighboring nodes, making them well-suited for tasks involving chemical structures, such as molecules and crystals.}
}

\newglossaryentry{gpmg}{
  name={general-purpose model},
  description={A model that is designed to generalize across a wide range of tasks and domains with minimal task-specific modifications. General-purpose models are typically pre-trained on vast, diverse datasets using \glslink{ssl}{self-supervised} objectives. They can be efficiently adapted to new tasks via \gls{promptingg} or \gls{finetuningg}. Examples include architectures such as \glspl{llm} and \glspl{vlm}.}
}


\newglossaryentry{gpg}{
  name={Gaussian process},
  description={A nonparametric probabilistic model used to define a distribution over functions. It is commonly used in \gls{bog} and regression to make predictions with uncertainty estimates.}
}

\newglossaryentry{hallucinationg}{
  name={hallucination},
  description={The phenomenon where a model generates output that seems plausible but factually incorrect or unsupported by the input or training data. Hallucinations are common in \glspl{llm} and pose challenges for applications requiring reliability and factual accuracy.}
}

\newglossaryentry{hnswg}{
  name={hierarchical navigable small world},
  description={An efficient algorithm for approximate nearest neighbor search in high-dimensional spaces. It builds a graph-based data structure with multiple layers of navigable small-world graphs, where most nodes can be reached in a few steps through well-connected hubs. This structure allows fast and scalable similarity search. In chemistry, HNSW is often used for rapid retrieval of structurally similar molecules in large compound libraries.}
}

\newglossaryentry{iclg}{
  name={in-context learning},
  description={In this method, \glspl{llm} are adapted to perform new tasks at inference time by conditioning them on examples provided directly in the input prompt, without updating their parameters. The model uses patterns inferred from these examples to predict or generate appropriate outputs for new inputs.}
}

\newglossaryentry{inchig}{
  name={\gls{inchi}},
  sort={inchi},
  description={A textual representation for chemical substances developed by \gls{iupac}, designed to provide a standard and machine-readable representation of molecular structures. InChI encodes information such as connectivity, hydrogen atoms, stereochemistry, and isotopes in a structured sequence of layers. The general format is \texttt{InChI=version/layers}. }
}

\newglossaryentry{inductivebiasg}{
  name={inductive bias},
  description={The set of assumptions a learning algorithm uses to generalize from limited training data to unseen examples. Inductive bias guides which solutions a model is likely to prefer and can arise from model architecture, input representations, or training objectives. \emph{Hard} inductive biases are built into the structure of the model and define the solution space, while \emph{soft} inductive biases, nudging the model to different parts of the solution space, are encouraged by training choices or priors but can be overridden by the data.}
}

\newglossaryentry{inferenceg}{
  name={inference},
  description={In the context of \glspl{llm}, inference is the computational process by which a trained model produces output \glspl{tokeng} given an input sequence. Inference involves executing a forward pass through the model to evaluate the conditional probability distribution over possible next tokens, and then sampling from that distribution using a decoding strategy. Unlike training, inference does not involve gradient computation or parameter updates, and is often optimized for speed and throughput.}
}

\newglossaryentry{irtg}{
  name={item response theory},
  description={This statistical framework was originally developed in psychometrics to model the relationship between a person’s latent ability and their probability of correctly answering test items. In \gls{mlg}, IRT is used to analyze model performance by treating test examples as such items and estimating their difficulty, allowing for more nuanced evaluation than aggregate accuracy.}
}

\newglossaryentry{latentfusiong}{
  name={latent fusion},
  description={A method for integrating information from multiple \glslink{modalityg}{modalities} by combining their representations in a shared \gls{latentspaceg}. Latent fusion enables models to reason jointly over heterogeneous data, such as combining text and molecular structure embeddings for property prediction or generation tasks.}
}


\newglossaryentry{latentspaceg}{
  name={latent space},
  description={An abstract, typically lower-dimensional space where input data is represented after being transformed by a model. Latent spaces often aim to capture meaningful features or structures that are not explicitly present in the original data.}
}

\newglossaryentry{latentstateg}{
  name={latent state},
  description={An internal, unobserved representation maintained by a model to capture relevant information about the input or sequence history. Latent states are used in models like \glspl{rnn} and \glspl{ssm}.}
}

\newglossaryentry{liftg}{
  name={language-interfaced fine-tuning},
  description={A method for fine-tuning models by framing structured tasks such as classification or regression as natural-language \glslink{promptingg}{prompts}.\autocite{dinh2022lift} LIFT enables the use of \glspl{llm} for supervised tasks without modifying the model architecture, leveraging prompt-based interfaces to adapt to diverse formats.}
}

\newglossaryentry{llmg}{
  name={large language model}, 
  description={A \gls{lmg} that has a large number of parameters. There is no agreed-upon rule for the number of parameters that makes a language model large enough to be called a large language model, but this number is usually on the scale of billions. For example, Llama 3, GPT-3, and GPT-4 contain 70B, 175B, and 1.76T parameters, respectively, while Claude 3 Opus is estimated to have 2 trillion parameters. Most current \glspl{llm} are based on the transformer architecture. \glspl{llm} can perform many types of language tasks, such as generating human-like text, understanding context, translation, summarization, and question-answering.}
}

\newglossaryentry{lmg}{
  name={language model}, 
  description={A model that estimates the probability of a \gls{tokeng} or sequence of \glspl{tokeng} occurring in a longer sequence of \glspl{tokeng}. This probability is used to predict the most likely next \gls{tokeng} based on the previous sequence. Language models are trained on large datasets of text, learning the patterns and structures of language to understand, interpret, and generate natural language.}
}

\newglossaryentry{localenvg}{
  name={Local-Env},
  description={A textual representation of crystal structures, inspired by Pauling's rule of parsimony\autocite{pauling_parsimony} and designed to leverage the structural redundancy often found in the local atomic arrangement of crystals. It begins with the crystal's space group, followed by a list of distinct coordination environments, each specified by its Wyckoff label and a corresponding \gls{smilesg} string.}
}
  

\newglossaryentry{lorag}{
  name={low-rank adaptation}, 
  description={A \gls{peft} technique that freezes the pre-trained model weights and decomposes the update matrix into two lower-rank matrices that contain a reduced number of trainable parameters to be optimized during \gls{finetuningg}. \autocite{hu2022lora}}
}

\newglossaryentry{lossfunctiong}{
  name={loss function},
  description={Optimization processes often try to minimize a loss function, which is a mathematical function that quantifies the difference between a model's prediction and the true target. It guides the optimization process during training by indicating how well the model is performing.}
}

\newglossaryentry{lstmg}{
  name={long short-term memory},
  description={A type of \gls{rnng} architecture designed to capture long-range dependencies in sequential data. LSTMs use \glspl{gatingg} to control the flow of information, making them effective for tasks like language modeling, time-series prediction, and speech recognition.}
}

\newglossaryentry{mctsg}{
  name={Monte Carlo tree search},
  description={A heuristic search algorithm used for decision-making in sequential environments, particularly in games and planning problems. MCTS incrementally builds a search tree by simulating many random playouts from different states to estimate action values.\autocite{coulom2006mcst} MCTS typically proceeds through four phases: selection, expansion, simulation, and backpropagation. It has been notably used in systems like AlphaGo \autocite{silver2016alphago} for planning in high-dimensional, sparse-reward environments.}
}

\newglossaryentry{mlg}{
  name={machine learning},
  description={This field of computer science is focused on developing algorithms that enable systems to learn patterns from data and make predictions or decisions without being explicitly programmed.}
}

\newglossaryentry{mlipg}{
  name={machine-learning interatomic potential},
  description={A model that approximates the potential energy surface of atomic systems using \gls{mlg} techniques. MLIPs are typically trained on reference data from quantum mechanical calculations such as \gls{dft}, learning to predict both energies and forces. They serve as a data-driven alternative to classical force fields, enabling more accurate and transferable atomistic simulations at a fraction of the cost of ab initio methods.}
}

\newglossaryentry{modalityg}{
  name={modality},
  description={A form of data characterized by a particular sensory or representational channel, such as text, images, audio, or molecular graphs. In \gls{mlg}, handling multiple modalities enables models to integrate and reason across diverse data sources.}
}

\newglossaryentry{modeltemperatureg}{
  name={model temperature},
  description={The term originates from statistical physics, where temperature controls the entropy of a system. In the context of \gls{mlg}, temperature is a parameter used during sampling from a probabilistic model to control the randomness of the output distribution. Lower temperatures sharpen the distribution, making outputs more deterministic, while higher temperatures flatten it, increasing diversity.  In \glspl{llm}, temperature is commonly tuned during text generation to balance creativity and coherence.}
}


\newglossaryentry{moeg}{
  name={mixture of experts},
  description={A general modeling framework in which multiple specialized submodels, or \emph{experts}, are combined using a function that selects or weights their contributions based on the input.}
}

\newglossaryentry{mpnng}{
  name={message-passing neural network},
  description={A class of \glspl{gnng} that operate on graphs by iteratively updating node representations through the exchange of messages with neighboring nodes. MPNNs are widely used in molecular property prediction and other graph-structured tasks.}
}

\newglossaryentry{ngramg}{
  name={n-gram},
  description={A contiguous sequence of \(n\) \glspl{tokeng} from a given text. N-grams are commonly used in language modeling and text analysis.}
}

\newglossaryentry{nlpg}{
  name={natural language processing}, 
  description={A subfield of computer science that uses \gls{mlg} to enable computers to process and generate human language. The primary tasks in NLP include speech recognition, text classification, natural language understanding, and natural language generation.}
}

\newglossaryentry{nng}{
  name={neural network},
  description={One of the most prevalent model components used in \gls{dlg}, inspired by the structure of the brain. Neural networks are made up of layers of connected units called neurons. Each layer of neurons takes a vector $x$ as input, performs a simple calculation $f(x)$, and passes the result to the next layer. A typical layer can be mathematically represented as $f(x) = \sigma(Wx + b)$, where $W$ is a matrix of learned weights, $b$ is a learned bias term, and $\sigma$ is a non-linear function called the activation function. By stacking many such layers, neural networks can learn to approximate complex functions. See also: \textbf{\gls{compositionalityg}}.}
}


\newglossaryentry{ocrg}{
  name={optical character recognition}, 
  description={A technique used to identify and convert images of printed or handwritten text into a machine-readable text format. This involves segmentation of text regions, character recognition, and post-processing to correct errors and enhance accuracy.}
}

\newglossaryentry{oheg}{
  name={one-hot encoding},
  description={A method for representing categorical variables as binary vectors, where each category is assigned a unique position set to 1, and all others are 0. One-hot encoding is commonly used to input discrete features into machine learning models that require numerical input.}
}


\newglossaryentry{osg}{
  name={operating system},
  description={Software that manages computer resources, providing an interface between hardware and software. The operating system handles tasks such as memory management, process scheduling, and device control.}
}

\newglossaryentry{pddlg}{
  name={planning domain definition Language},
  description={A formal language used to specify planning problems in automated planning. PDDL defines the initial state, goal conditions, and available actions with their preconditions and effects, enabling planners to generate sequences of actions to achieve the desired outcomes. It has been applied to domains such as robotic control and retrosynthetic planning.}
}


\newglossaryentry{peftg}{
  name={parameter-efficient fine-tuning}, 
  description={A methodology to efficiently \glslink{finetuningg}{fine-tune} large pre-trained models without modifying their original parameters. PEFT strategies involve adjusting only a small number of additional model parameters during \gls{finetuningg} on a new, smaller training dataset. This significantly reduces the computational and storage costs while achieving comparable performance to full \gls{finetuningg}. Common PEFT methods include \gls{lora}, \gls{qlora}, and \gls{dora}.}
}

\newglossaryentry{policyg}{
  name={policy},
  description={In the context of \gls{rlg}, a policy defines the agent's behavior by mapping states to actions. It is typically denoted as $\pi(a \mid s)$, representing the conditional probability of taking action $a$ given state $s$. A policy can be deterministic or stochastic, and may be represented by a parameterized function such as a \gls{nng}. The objective of \gls{rl} is to learn a policy that maximizes expected cumulative reward.}
}


\newglossaryentry{ppog}{
  name={proximal policy optimization}, 
  description={A \gls{rl} algorithm used to train \glspl{llm} for alignment. PPO achieves this by adjusting the \gls{policyg} parameters in a way that keeps changes within a predefined safe range to maintain stability and improve learning efficiency. PPO is often used as part of \gls{rlhf}.}
}

\newglossaryentry{promptingg}{
  name={prompting},
  description={The practice of providing input to a \gls{llm} in the form of natural-language instructions, questions, or examples to guide its behavior. Prompting can influence the model’s responses without changing its parameters.}
}

\newglossaryentry{ragg}{
  name={retrieval augmented generation}, 
  description={A technique for improving the quality of text generation by providing \glspl{llm} with access to information retrieved from external knowledge sources. In practice, this means that relevant retrieved text snippets are added to the prompt.}
}

\newglossaryentry{regexg}{
  name={regular expression},
  description={Often abbreviated as \emph{regex}, it is a sequence of characters that defines a search pattern for matching text. Regular expressions are widely used for tasks such as string validation, extraction, and substitution.}
}

\newglossaryentry{rlg}{
  name={reinforcement learning},
  description={In this \gls{mlg} paradigm, an \gls{agentg} learns to take actions in an environment to maximize cumulative reward through trial and error. See also: \textbf{\gls{policyg}}}.
}

\newglossaryentry{rlhfg}{
  name={reinforcement learning from human feedback}, 
  description={This mechanism uses \gls{rl} to align \glspl{llm} with user preferences by \gls{finetuningg} on human feedback. Users are asked to rate the quality of a model's response. Based on this, a preference model is trained and then used in a reinforcement learning setup to optimize the generations of the \gls{llm}.}
}

\newglossaryentry{rnng}{
  name={recurrent neural network},
  description={A type of \gls{nng} designed for processing sequential data by maintaining a \glslink{latentstateg}{hidden state} that captures information from previous time steps. Without special mechanisms, RNNs are limited in capturing long-range dependencies.}
}

\newglossaryentry{rocaucg}{
  name={receiver operating characteristic---area under the curve},
  description={This performance metric is used for binary classifiers and measures the area under the ROC curve, which plots the true positive rate against the false positive rate at various thresholds. A higher ROC-AUC indicates better ability to distinguish between classes, with 1.0 being perfect and 0.5 representing random guessing.}
}

\newglossaryentry{semanticsearchg}{
  name={semantic search},
  description={Rather than retrieving text based on exact keyword matching, semantic search is a technique that retrieves information based on meaning. Semantic search uses vector representations (see \glslink{embeddingg} {\textbf{embedding}}) of queries and documents to capture contextual similarity, enabling more accurate retrieval of relevant results even when different words are used.}
}


\newglossaryentry{selfiesg}{
  name={\gls{selfies}},
  sort={selfies},
  description={A textual representation of molecular structures designed to be 100\% robust, meaning every SELFIES string maps to a valid molecule. Unlike \gls{smilesg}, SELFIES uses a grammar-based system to encode molecular structures in a way that prevents syntactic invalidity, making it especially useful for generative models.}
}

\newglossaryentry{slicesg}{
  name={\gls{slices}},
  sort={slices},
  description={A string-based representation of crystal structures in a human-readable and machine-interpretable form. SLICES aims to facilitate generative modeling in materials science by representing crystalline structures as linear sequences amenable to sequence-based models.}
}

\newglossaryentry{sftg}{
  name={supervised fine-tuning}, 
  description={One of the simplest forms of the \gls{finetuningg} process, in which a pre-trained \gls{llm} is \glslink{finetuningg}{fine-tuned} on a smaller, labeled dataset for a specific task.}
}

\newglossaryentry{smilesg}{
  name={\gls{smiles}},
  sort={SMILES},
  description={A non-unique textual representation of molecular structures, often small organic molecules, using a sequence of ASCII characters. SMILES encodes atoms and bonds in a linear form suitable for storage, search, and \gls{mlg} applications. The term \emph{canonical} smiles refers to a smiles string that is generated using a deterministic algorithm (e.g., by \modelname{RDKit}) to ensure consistency.}
}

\newglossaryentry{sotag}{
  name={state-of-the-art}, 
  description={In the context of \gls{mlg}, this term is used to describe the most advanced models or techniques that represent the highest level of performance achievable today. They are typically the result of extensive research and development and serve as benchmarks for researchers and developers.}
}

\newglossaryentry{statespaceg}{
  name={state space},
  description={The set of all possible internal configurations (states) a system or model can occupy. In \gls{mlg}, the state space defines how the system evolves over time and is used to model sequential behavior.}
}

\newglossaryentry{supervisedg}{
  name={supervised learning},
  description={In this \gls{mlg} paradigm, the model is trained on labeled data, learning to map inputs to known outputs.}
}

\newglossaryentry{sslg}{
  name={self-supervised learning}, 
  description={A \gls{mlg} technique that involves generating labels from the input data itself instead of relying on external labeled data. It has been foundational for the success of \glspl{llm}, as their pre-training task (next word prediction or filling in of masked words) is a self-supervised task.}
}

\newglossaryentry{ssmg}{
  name={selective state space model},
  description={A \glslink{nng}{neural architecture} that models sequential data using \glspl{latentstateg} and learned transitions, while \glslink{gatingg}{selectively controlling} which components of the state are updated at each step. SSMs aim to improve long-range reasoning and efficiency, and are used as alternatives to attention mechanisms in tasks like language modeling.}
}


\newglossaryentry{tokeng}{
  name={token},
  description={A basic unit of text used by \glspl{lmg}, typically corresponding to a word, subword, or character depending on the tokenizer. Models process and generate text as sequences of tokens.}
}

\newglossaryentry{transferabilityg}{
  name={transferability},
  description={The degree to which knowledge learned in one setting---such as a task, domain, or data distribution---can be effectively reused or adapted in another. High transferability indicates that representations or models generalize well to new contexts, and it is a key objective in \gls{transferlearningg}, foundation models, and general-purpose architectures.}
}


\newglossaryentry{transferlearningg}{
  name={transfer learning},
  description={In this \gls{mlg} paradigm, knowledge gained from one task or domain is reused to improve performance on a different, often related, task. Transfer learning typically involves pre-training a model on a large dataset and then fine-tuning it on a smaller, task-specific dataset, making it particularly valuable in low-data settings.}
}

\newglossaryentry{totg}{
  name={tree-of-thought},
  description={This \gls{promptingg} and reasoning framework extends \gls{cotg} by exploring multiple reasoning paths in a tree structure. ToT allows a model to evaluate and revise intermediate steps, enabling more deliberate decision-making in complex tasks.}
}


\newglossaryentry{unsupervisedg}{
  name={unsupervised learning},
  description={In this \gls{mlg} paradigm, the model is trained on unlabeled data to discover hidden patterns or structure, such as clusters or latent variables.}
}

\newglossaryentry{vlmg}{
  name={vision language model},
  description={A \glslink{modalityg}{multi-modal} model that can simultaneously learn from images and texts, generating text outputs.}
}