<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; The Shape and Structure of Chemical Data – General Purpose Models for the Chemical Sciences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./03-architectures.html" rel="next">
<link href="./01-introduction.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-fffb2cfd06bc0bcd22fa5e79382abad9.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02-data_taxonomy.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Shape and Structure of Chemical Data</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">General Purpose Models for the Chemical Sciences</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">General purpose models for the chemical sciences</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-data_taxonomy.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Shape and Structure of Chemical Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Building Principles of GPMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-evals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evaluations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-accelerating_applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Accelerating Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-safety.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implications of GPMs: Education, Safety, and Ethics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-outlook_conclusions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Outlook and Conclusions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#shape-of-scientific-data" id="toc-shape-of-scientific-data" class="nav-link active" data-scroll-target="#shape-of-scientific-data"><span class="header-section-number">2.1</span> Shape of Scientific Data</a>
  <ul>
  <li><a href="#irreducible-complexity" id="toc-irreducible-complexity" class="nav-link" data-scroll-target="#irreducible-complexity"><span class="header-section-number">2.1.1</span> Irreducible complexity</a>
  <ul>
  <li><a href="#emergent-complexity" id="toc-emergent-complexity" class="nav-link" data-scroll-target="#emergent-complexity"><span class="header-section-number">2.1.1.1</span> Emergent complexity</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#scale-of-chemical-data" id="toc-scale-of-chemical-data" class="nav-link" data-scroll-target="#scale-of-chemical-data"><span class="header-section-number">2.2</span> Scale of Chemical Data</a></li>
  <li><a href="#dataset-creation" id="toc-dataset-creation" class="nav-link" data-scroll-target="#dataset-creation"><span class="header-section-number">2.3</span> Dataset Creation</a>
  <ul>
  <li><a href="#filtering" id="toc-filtering" class="nav-link" data-scroll-target="#filtering"><span class="header-section-number">2.3.1</span> Filtering</a></li>
  <li><a href="#sec:syn-data" id="toc-sec:syn-data" class="nav-link" data-scroll-target="#sec\:syn-data"><span class="header-section-number">2.3.2</span> Synthetic Data</a>
  <ul>
  <li><a href="#rule-based-augmentation" id="toc-rule-based-augmentation" class="nav-link" data-scroll-target="#rule-based-augmentation"><span class="header-section-number">2.3.2.1</span> Rule-based augmentation</a></li>
  <li><a href="#generative-augmentation" id="toc-generative-augmentation" class="nav-link" data-scroll-target="#generative-augmentation"><span class="header-section-number">2.3.2.2</span> Generative augmentation</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="quarto-other-links"><h2>Other Links</h2><ul><li><a href="https://lamalab.org/"><i class="bi bi-globe"></i>Visit our website</a></li><li><a href="https://x.com/jablonkagroup"><i class="bi bi-twitter-x"></i>Follow us on X (Twitter)</a></li><li><a href="https://forms.fillout.com/t/eoGA7AhnAKus"><i class="bi bi-person-badge"></i>We are hiring!</a></li><li><a href="mailto:contact@lamalab.org"><i class="bi bi-mailbox"></i>Contact us</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec:data-section" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Shape and Structure of Chemical Data</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="shape-of-scientific-data" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="shape-of-scientific-data"><span class="header-section-number">2.1</span> Shape of Scientific Data</h2>
<p>Data is the essential asset in data-driven techniques. To understand the successes and failures of machine learning (ML) models, it is instructive to explore how the structure of different datasets shapes the learning capabilities of different models. One useful lens for doing so is to consider how complex a system is (i.e., how many variables are needed to describe it) and what fraction of these variables are explicit. One might see the set of variables that are required to describe a system as the state space. A state space encompasses all possible states of a system, similar to concepts in statistical mechanics (SM). However, in contrast to many other problems, we often cannot explicitly enumerate all variables and their potential values in relevant chemical systems. Commonly, many of the essential factors describing a system are implicit (“known unknowns” or “unknown unknowns”).</p>
<section id="irreducible-complexity" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="irreducible-complexity"><span class="header-section-number">2.1.1</span> Irreducible complexity</h3>
<p><a href="#fig:shape_of_data" data-reference-type="ref+Label" data-reference="fig:shape_of_data">1</a> illustrates how the state space of chemistry tends to grow more implicit as we move from describing single atoms or small molecules <em>in vacuo</em>, to real-world systems. For instance, we can completely explain almost all observed phenomena for a hydrogen atom using the position (and atomic numbers) of the hydrogen atom via the Schrödinger equation. As we scale up to larger systems such as macromolecular structures or condensed phases, we have to deal with more “known unknowns” and “unknown unknowns”.[<span class="citation" data-cites="martin2022bridging">Martin (<a href="09-references.html#ref-martin2022bridging" role="doc-biblioref">2022</a>)</span>] For example, it is currently impossible to model a full packed-bed reactor at the atomistic scale because the size of the problem scales with the number of parameters that can be tuned. Often, it becomes infeasible to explicitly label all variables and their values. We can describe such complexity as “irreducible”[<span class="citation" data-cites="Pietsch_2017">Pietsch and Wernecke (<a href="09-references.html#ref-Pietsch_2017" role="doc-biblioref">2017</a>)</span>], in contrast to “emergent” complexity that emerges from systems that can be described with simple equations, such as a double pendulum.</p>
<figure id="fig:shape_of_data" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure1.png" alt="" width="100%" class="figure-img">
<figcaption>
<strong>State space description for chemistry at different scales</strong>. We illustrate how the number of hidden variables (gray) is growing with scale and complexity. For simple systems we can explicitly write down all variables and their values and perfectly describe the system. For more complex systems—closer to practical applications—we can no longer do that. Many more variables cannot be explicitly enumerated.
</figcaption>
</figure>
<p>For phenomena characterized by irreducible complexity, success is often serendipitous. As pointed out by <span class="citation" data-cites="rulev2017serendipity">Rulev (<a href="09-references.html#ref-rulev2017serendipity" role="doc-biblioref">2017</a>)</span>, chemical literature commonly contains terms such as “to our surprise”, “remarkable reactivity”, or “unusual performance”, which may reflect the complexity of scientific questions and the diminishing explainability of observed results.</p>
<section id="emergent-complexity" class="level4" data-number="2.1.1.1">
<h4 data-number="2.1.1.1" class="anchored" data-anchor-id="emergent-complexity"><span class="header-section-number">2.1.1.1</span> Emergent complexity</h4>
<p>In contrast to irreducible complexity, there is a subset of chemical problems for which all relevant parameters can explicitly be listed, but the complexity emerges from the intricate, potentially chaotic, interactions among them. A well-known example is the Belousov-Zhabotinsky reaction, [<span class="citation" data-cites="Cassani2021BZ">Cassani, Monteverde, and Piumetti (<a href="09-references.html#ref-Cassani2021BZ" role="doc-biblioref">2021</a>)</span>] which exhibits oscillations and pattern formation as a result of a complex chemical reaction network. Individual chemical reactions within the network are simple, but their interactions create a dynamic, self-organizing system with properties not seen in the individual components. An example of how fast the parameter space can grow was provided by <span class="citation" data-cites="NEURIPS2024_53704142">Koziarski et al. (<a href="09-references.html#ref-NEURIPS2024_53704142" role="doc-biblioref">2024</a>)</span>, who show that a single reaction type and a few hundred molecular building blocks can create tens of thousands of possible solutions. When scaling up to only five reaction types, the exploration of the entire space can become intractable, estimated at approximately <span class="math inline">\(10^{22}\)</span> solutions. When optimization objectives are involved—finding the shortest synthesis pathway or maximizing the yield—such problems are often NP-hard, meaning that no known polynomial-time algorithms can guarantee optimal solutions, though various heuristic and approximation methods can provide good solutions.</p>
<p>Knowing the ratio between explicit and implicit parameters helps in selecting the appropriate model architecture. If most of the variance is caused by explicit factors, these can be incorporated as priors or constraints in the model, thereby increasing data efficiency. This strategy can, for instance, be applied in the development of force fields where we know the governing equations and their symmetries, and can use them to enforce such symmetries in the model architecture (as hard restrictions to a family of solutions). [<span class="citation" data-cites="unke2021machine">Unke et al. (<a href="09-references.html#ref-unke2021machine" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="Musil_2021">Musil et al. (<a href="09-references.html#ref-Musil_2021" role="doc-biblioref">2021</a>)</span>] However, when the variance is dominated by implicit factors, such constraints can no longer be formulated, as the governing relationships are not known. In those cases, flexible general-purpose model (GPM)s with soft inductive biases—which guide the model toward preferred solutions without enforcing strict constraints on the solution space[<span class="citation" data-cites="wilson2025deep">Wilson (<a href="09-references.html#ref-wilson2025deep" role="doc-biblioref">2025</a>)</span>]—are more suitable. large language model (LLM)s fall into this category.</p>
</section>
</section>
</section>
<section id="scale-of-chemical-data" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="scale-of-chemical-data"><span class="header-section-number">2.2</span> Scale of Chemical Data</h2>
<p>Chemistry is an empirical science in which every prediction bears the burden of proof through experimental validation.[<span class="citation" data-cites="zunger2019beware">Zunger (<a href="09-references.html#ref-zunger2019beware" role="doc-biblioref">2019</a>)</span>] However, there is often a mismatch between the realities of a chemistry lab and the datasets on which ML models for chemistry are trained. Much of current data-driven modeling in chemistry focuses on a few large, structured, and highly-curated datasets where most of the variance is explicit (reducible complexity). Such datasets, <code>QM9</code> for example,[<span class="citation" data-cites="ramakrishnan2014quantum">Ramakrishnan et al. (<a href="09-references.html#ref-ramakrishnan2014quantum" role="doc-biblioref">2014</a>)</span>] often come from quantum-chemical computations. Experimental chemistry, however, tends to have a significantly higher variance and a greater degree of irreducible complexity. In addition, since data generation is often expensive, datasets are small, and because science is about doing new things for the first time, many datasets also contain at least some unique variables.</p>
<p>Considering the largest chemistry text dataset, <code>ChemPile</code>,[<span class="citation" data-cites="mirza2025chempile0">Mirza et al. (<a href="09-references.html#ref-mirza2025chempile0" role="doc-biblioref">2025</a>)</span>] we can look at how the largest subsets fare in comparison to the smallest ones (see <a href="#tab:small_large_datasets" data-reference-type="ref+Label" data-reference="tab:small_large_datasets">1</a>). The largest dataset is approximately three million times larger than the smallest one.</p>
<div id="tab:small_large_datasets">
<table class="caption-top table">
<caption><strong>Token counts for the three largest and smallest datasets in the <code>ChemPile</code>[<span class="citation" data-cites="mirza2025chempile0">Mirza et al. (<a href="09-references.html#ref-mirza2025chempile0" role="doc-biblioref">2025</a>)</span>] collection.</strong> Dominating datasets contribute a large portion of the total token count (a token represents the smallest unit of text that a ML model can process), with the small datasets significantly increasing the diversity.</caption>
<colgroup>
<col style="width: 70%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Dataset</strong></th>
<th style="text-align: right;"><strong>Token count</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td colspan="2" style="text-align: left;"><em>Three largest ChemPile datasets</em></td>
</tr>
<tr class="even">
<td style="text-align: left;">NOMAD crystal structures[<span class="citation" data-cites="scheidgen2023nomad">Scheidgen et al. (<a href="09-references.html#ref-scheidgen2023nomad" role="doc-biblioref">2023</a>)</span>]</td>
<td style="text-align: right;">5,808,052,794</td>
</tr>
<tr class="odd">
<td colspan="2" style="text-align: left;">Open Reaction Database (ORD)[<span class="citation" data-cites="Kearnes_2021">Kearnes et al. (<a href="09-references.html#ref-Kearnes_2021" role="doc-biblioref">2021</a>)</span>] | | reaction prediction |</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>RDKit</code> molecular features</td>
<td style="text-align: right;">5,000,435,822</td>
</tr>
<tr class="odd">
<td colspan="2" style="text-align: left;"><em>Three smallest ChemPile datasets</em></td>
</tr>
<tr class="even">
<td style="text-align: left;">Hydrogen storage materials[<span class="citation" data-cites="hymarcReversibleHydrides">HyMARC (<a href="09-references.html#ref-hymarcReversibleHydrides" role="doc-biblioref">2019</a>)</span>]</td>
<td style="text-align: right;">1,935</td>
</tr>
<tr class="odd">
<td style="text-align: left;">List of amino-acids[<span class="citation" data-cites="alberts2002molecular">Alberts (<a href="09-references.html#ref-alberts2002molecular" role="doc-biblioref">2002</a>)</span>]</td>
<td style="text-align: right;">6,000</td>
</tr>
<tr class="even">
<td colspan="2" style="text-align: left;">ORD[<span class="citation" data-cites="Kearnes_2021">Kearnes et al. (<a href="09-references.html#ref-Kearnes_2021" role="doc-biblioref">2021</a>)</span>] | | recipe yield prediction |</td>
</tr>
</tbody>
</table>
</div>
<p><span id="tab:small_large_datasets" data-label="tab:small_large_datasets"></span></p>
<p>The prevalence of many small, specialized datasets over large ones is commonly referred to as “the long tail problem”.[<span class="citation" data-cites="heidorn2008shedding">Heidorn (<a href="09-references.html#ref-heidorn2008shedding" role="doc-biblioref">2008</a>)</span>]</p>
<figure id="fig:scale_of_data" class="figure">
<img src="media/figures/final_figures/2_DATA_cumulative_histogram.png" alt="" width="100%" class="figure-img">
<figcaption>
<strong>Cumulative token count based on the <code>ChemPile</code> tabular datasets <span class="citation" data-cites="mirza2025chempile0">(<a href="09-references.html#ref-mirza2025chempile0" role="doc-biblioref">Mirza et al. 2025</a>)</span></strong>. We compare the approximate token count for three datasets: <code>Llama-3</code> training dataset,<span class="citation" data-cites="grattafiori2024llama">(<a href="09-references.html#ref-grattafiori2024llama" role="doc-biblioref">Grattafiori et al. 2024</a>)</span> openly available chemistry papers in the <code>ChemPile-Paper</code> dataset, and the <code>ChemPile-LIFT</code> dataset. As can be seen, by aggregating the collection of tabular datasets converted to text format in the <code>ChemPile-LIFT</code> subset, we can achieve the same order of magnitude as the collection of open chemistry papers. However, without smaller datasets, we cannot capture the breadth and complexity of chemistry data, which is essential for training <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span>. The tokenization method for both <code>ChemPile</code> and <code>Llama-3</code> is provided in the respective papers.
</figcaption>
</figure>
<p>This can be seen in <a href="#fig:scale_of_data" data-reference-type="ref+Label" data-reference="fig:scale_of_data">2</a>. We show that while a few datasets are large, the majority of the corpus consists of small but collectively significant and chemically diverse datasets. The actual tail of chemical data is even larger, as <a href="#fig:scale_of_data" data-reference-type="ref+Label" data-reference="fig:scale_of_data">2</a> only shows the distribution for manually curated tabular datasets and not all data actually created in the chemical sciences. Given that every dataset in the long tail relies on unique sources of variance—it is very difficult to leverage this long tail with conventional ML techniques. However, the promise of GPMs such as LLMs is that they can very flexibly integrate and jointly model the diversity of small datasets that exist in the chemical sciences.</p>
</section>
<section id="dataset-creation" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="dataset-creation"><span class="header-section-number">2.3</span> Dataset Creation</h2>
<p>Before a model can be trained or tested, suitable data must first be collected. It is important to note that when working with GPMs, data can—but should not—be directly ingested in a raw format and requires some form of pre-processing. Training GPMs typically requires a large and diverse dataset, compiled in a form that can be efficiently ingested by the training pipeline.</p>
<p>Strategies for doing so can be broadly categorized into two groups (see <a href="#fig:data_protocols" data-reference-type="ref+Label" data-reference="fig:data_protocols">3</a>). One can utilize a “top-down” approach where a large and diverse pool of data—e.g., results from web-crawled resources such as <code>CommonCrawl</code>[<span class="citation" data-cites="commoncrawl"><span>“Common Crawl”</span> (<a href="09-references.html#ref-commoncrawl" role="doc-biblioref">2024</a>)</span>]—is filtered using custom-built procedures (e.g., using regular expressions or classification models). This approach is gaining traction in the development of foundation models such as LLMs.[<span class="citation" data-cites="penedo2023refinedweb">Penedo et al. (<a href="09-references.html#ref-penedo2023refinedweb" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="penedo2024fineweb">Penedo et al. (<a href="09-references.html#ref-penedo2024fineweb" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="guo2025deepseek">Guo et al. (<a href="09-references.html#ref-guo2025deepseek" role="doc-biblioref">2025</a>)</span>] Alongside large filtered datasets, various data augmentation techniques have further increased the performance of GPMs.[<span class="citation" data-cites="maini2024rephrasing">Maini et al. (<a href="09-references.html#ref-maini2024rephrasing" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="pieler2024rephrasing">Pieler et al. (<a href="09-references.html#ref-pieler2024rephrasing" role="doc-biblioref">2024</a>)</span>]</p>
<p>Alternatively, one can take a “bottom-up” approach by specifically creating novel datasets for a given problem—an approach which has been very popular in ML for chemistry.</p>
<p>In practice, a combination of both approaches is often used. In most cases, key techniques include filtering and the generation of synthetic data.</p>
<figure id="fig:data_protocols" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure3.png" alt="" width="100%" class="figure-img">
<figcaption>
<strong>Dataset creation protocols</strong>. In “top-down” approaches, we curate a large corpus of data, which can be used to train GPMs. The “bottom-up” approach starts from a problem definition, and the dataset can be collected via literature mining and experiments. Both approaches can make use of synthetic data to increase the data size and diversity.
</figcaption>
</figure>
<section id="filtering" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="filtering"><span class="header-section-number">2.3.1</span> Filtering</h3>
<p>GPMs are currently trained on very large datasets, enabled by the availability of ever-growing computational resources.[<span class="citation" data-cites="krizhevsky2012imagenet">Krizhevsky, Sutskever, and Hinton (<a href="09-references.html#ref-krizhevsky2012imagenet" role="doc-biblioref">2012</a>)</span>; <span class="citation" data-cites="kaplan2020scaling">Kaplan et al. (<a href="09-references.html#ref-kaplan2020scaling" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="hooker2020hardware">Hooker (<a href="09-references.html#ref-hooker2020hardware" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="dotan2019value0laden">Dotan and Milli (<a href="09-references.html#ref-dotan2019value0laden" role="doc-biblioref">2019</a>)</span>] The Internet has been the primary source of dataset construction for GPMs. While initially the focus was on training on maximally large datasets, empirical evidence has shown that smaller, higher-quality datasets can lead to better results.[<span class="citation" data-cites="gunasekar2023textbooks">Gunasekar et al. (<a href="09-references.html#ref-gunasekar2023textbooks" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="marion2023less">Marion et al. (<a href="09-references.html#ref-marion2023less" role="doc-biblioref">2023</a>)</span>] For example, <span class="citation" data-cites="shao2024deepseekmath0">Shao et al. (<a href="09-references.html#ref-shao2024deepseekmath0" role="doc-biblioref">2024</a>)</span> filtered <code>CommonCrawl</code> for mathematical text using a combination of regular expressions and a custom, iteratively trained classification model[<span class="citation" data-cites="bojanowski2017enriching">Bojanowski et al. (<a href="09-references.html#ref-bojanowski2017enriching" role="doc-biblioref">2017</a>)</span>]. An alternative approach was pursued by <span class="citation" data-cites="thrush2024improving">Thrush, Potts, and Hashimoto (<a href="09-references.html#ref-thrush2024improving" role="doc-biblioref">2024</a>)</span> who introduced a training-free framework. In this method, the pre-training text was chosen by measuring the correlation of each web-domain’s perplexity (a metric that measures how well a language model predicts a sequence of text)—as scored by <span class="math inline">\(90\)</span> publicly-available LLMs—with downstream benchmark accuracy, keeping only the high-correlation domains.</p>
<p>In the chemical domain, <code>ChemPile</code>[<span class="citation" data-cites="mirza2025chempile0">Mirza et al. (<a href="09-references.html#ref-mirza2025chempile0" role="doc-biblioref">2025</a>)</span>] is the only open-source, pre-training scale dataset, that underwent several filtering steps. For example, a large subset of the papers in <code>ChemPile-Paper</code> come from the <code>Europe PMC</code> dataset. To filter for chemistry papers, a custom classification model was trained from scratch using topic-labeled data from the <code>CAMEL</code>[<span class="citation" data-cites="li2023camel">Li et al. (<a href="09-references.html#ref-li2023camel" role="doc-biblioref">2023</a>)</span>] dataset. To evaluate the accuracy of the model (<span class="math inline">\(\text{F1-score}=0.77\)</span>), expert-annotated data was used.</p>
</section>
<section id="sec:syn-data" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="sec:syn-data"><span class="header-section-number">2.3.2</span> Synthetic Data</h3>
<p>Instead of only relying on existing datasets, one can also leverage techniques for generating synthetic data. Generation of synthetic data is often required to augment scarce real-world data, but can also be used to achieve the desired model behavior (e.g., invariance in image-based models).</p>
<p>These approaches can be grouped into rule-based and generative methods. Rule-based methods apply manually defined transformations—such as rotations and mirroring—to present different representations of the same instance to a model. In contrast, generative augmentation creates new data by applying transformations learned through a ML model.</p>
<section id="rule-based-augmentation" class="level4" data-number="2.3.2.1">
<h4 data-number="2.3.2.1" class="anchored" data-anchor-id="rule-based-augmentation"><span class="header-section-number">2.3.2.1</span> Rule-based augmentation</h4>
<p>The transformations applied for generating new data in rule-based approaches vary depending on the modality (e.g., image, text, or audio). The most common application of rule-based techniques is on images, via image transformations such as distortion, rotation, blurring, or cropping.[<span class="citation" data-cites="shorten2019survey">Shorten and Khoshgoftaar (<a href="09-references.html#ref-shorten2019survey" role="doc-biblioref">2019</a>)</span>] In chemistry, tools like <code>RanDepict</code>[<span class="citation" data-cites="brinkhaus2022randepict">Brinkhaus et al. (<a href="09-references.html#ref-brinkhaus2022randepict" role="doc-biblioref">2022</a>)</span>] have been used to create enriched datasets of chemical representations. These tools generate human-like drawings of chemical structures that mimic the common illustrations found in scientific literature or even in patents (e.g., by applying image templates from different publishers, or emulating the style of older manuscripts) and further augment them using conventional image-augmentation techniques.</p>
<p>Rule-based augmentations can also be applied to text. Early approaches involved simple operations like random word swapping, random synonym replacement, and random deletions or insertions, which are often labeled “easy augmentation” methods.[<span class="citation" data-cites="shorten2021text">Shorten, Khoshgoftaar, and Furht (<a href="09-references.html#ref-shorten2021text" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="wei2019eda0">Wei and Zou (<a href="09-references.html#ref-wei2019eda0" role="doc-biblioref">2019</a>)</span>]</p>
<p>In chemistry, text templates have been used.[<span class="citation" data-cites="xie2023darwin">Xie et al. (<a href="09-references.html#ref-xie2023darwin" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="mirza2025chempile0">Mirza et al. (<a href="09-references.html#ref-mirza2025chempile0" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="jablonka2024leveraging">Jablonka et al. (<a href="09-references.html#ref-jablonka2024leveraging" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="vanherck2025assessment">Van Herck et al. (<a href="09-references.html#ref-vanherck2025assessment" role="doc-biblioref">2025</a>)</span>] Such templates define a sentence structure with semantically configurable fields, which are then filled using structured tabular data. However, it is still unclear how to best construct such templates, as studies have shown that the same data shown in different templates can lead to distinct generalization behavior.[<span class="citation" data-cites="gonzales2024evaluating">Gonzales et al. (<a href="09-references.html#ref-gonzales2024evaluating" role="doc-biblioref">2024</a>)</span>]</p>
<p>We can also apply rule-based augmentation for specific molecular representations (for more details on representations see <a href="03-architectures.html#sec:common_representations" data-reference-type="ref+Label" data-reference="sec:common_representations">[sec:common_representations]</a>). For example, the same molecule can be represented with multiple different, yet valid simplified molecular input line entry system (SMILES) strings. <span class="citation" data-cites="bjerrum2017smiles">Bjerrum (<a href="09-references.html#ref-bjerrum2017smiles" role="doc-biblioref">2017</a>)</span> used this technique to augment a predictive model, where multiple SMILES strings were mapped to a single property. When averaging the predictions over multiple SMILES strings, at least a <span class="math inline">\(10\%\)</span> improvement was observed compared to their single SMILES counterparts. Such techniques can be applied to other molecular representations (e.g., International Union of Pure and Applied Chemistry (IUPAC) names or self-referencing embedded strings (SELFIES)), but historically, SMILES has been used more often. As a result, its augmentations have been studied more extensively.[<span class="citation" data-cites="kimber2021maxsmi">Kimber, Gagnebin, and Volkamer (<a href="09-references.html#ref-kimber2021maxsmi" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="born2023chemical">Born et al. (<a href="09-references.html#ref-born2023chemical" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="arus2019randomized">Arús-Pous et al. (<a href="09-references.html#ref-arus2019randomized" role="doc-biblioref">2019</a>)</span>]</p>
<p>A broad array of augmentation techniques has been applied to spectral data—from simple noise addition[<span class="citation" data-cites="ke2018convolutional">Ke et al. (<a href="09-references.html#ref-ke2018convolutional" role="doc-biblioref">2018</a>)</span>; <span class="citation" data-cites="moreno2022application">Moreno-Barea et al. (<a href="09-references.html#ref-moreno2022application" role="doc-biblioref">2022</a>)</span>] to physics-informed augmentations (e.g., through DFT simulations).[<span class="citation" data-cites="oviedo2019fast">Oviedo et al. (<a href="09-references.html#ref-oviedo2019fast" role="doc-biblioref">2019</a>)</span>; <span class="citation" data-cites="gao2020general">Gao et al. (<a href="09-references.html#ref-gao2020general" role="doc-biblioref">2020</a>)</span>]</p>
</section>
<section id="generative-augmentation" class="level4" data-number="2.3.2.2">
<h4 data-number="2.3.2.2" class="anchored" data-anchor-id="generative-augmentation"><span class="header-section-number">2.3.2.2</span> Generative augmentation</h4>
<p>In some cases, however, it is not possible to write down the rules. For instance, it is not obvious how text can be transformed into different styles using rules alone. Recent advances in deep learning have facilitated another, more flexible, approach to synthetic data generation. [<span class="citation" data-cites="maini2024rephrasing">Maini et al. (<a href="09-references.html#ref-maini2024rephrasing" role="doc-biblioref">2024</a>)</span>] A simple technique is to apply contextual augmentation [<span class="citation" data-cites="kobayashi2018contextual">Kobayashi (<a href="09-references.html#ref-kobayashi2018contextual" role="doc-biblioref">2018</a>)</span>], which implies the sampling of synonyms from a probability distribution of a language model (LM). Another technique is “back translation”,[<span class="citation" data-cites="edunov2018understanding">Edunov et al. (<a href="09-references.html#ref-edunov2018understanding" role="doc-biblioref">2018</a>)</span>] a process in which text is translated to another language and then back into the original language to generate semantically similar variants. While this technique is typically used within the same language,[<span class="citation" data-cites="lu2024mathgenie0">Lu et al. (<a href="09-references.html#ref-lu2024mathgenie0" role="doc-biblioref">2024</a>)</span>] it can also be extended to multilingual setups[<span class="citation" data-cites="hong2024cantonmt0">Hong et al. (<a href="09-references.html#ref-hong2024cantonmt0" role="doc-biblioref">2024</a>)</span>].</p>
<p>Other recent approaches have harnessed auto-formalization[<span class="citation" data-cites="NEURIPS2022_d0c6bc64">Wu et al. (<a href="09-references.html#ref-NEURIPS2022_d0c6bc64" role="doc-biblioref">2022</a>)</span>], a LLM-powered approach that can turn natural-language mathematical proofs into computer-verifiable mathematical languages such as <code>Lean</code>[<span class="citation" data-cites="de2015lean">De Moura et al. (<a href="09-references.html#ref-de2015lean" role="doc-biblioref">2015</a>)</span>] or <code>Isabelle</code>[<span class="citation" data-cites="wenzel2008isabelle">Wenzel, Paulson, and Nipkow (<a href="09-references.html#ref-wenzel2008isabelle" role="doc-biblioref">2008</a>)</span>]. Such datasets have been utilized to advance mathematical capabilities in LMs.[<span class="citation" data-cites="xin2024deepseek">Xin et al. (<a href="09-references.html#ref-xin2024deepseek" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="trinh2024solving">Trinh et al. (<a href="09-references.html#ref-trinh2024solving" role="doc-biblioref">2024</a>)</span>]</p>
<p>A drawback of generatively augmented data is that its validity is cumbersome to assess at scale, unless it can be verified automatically by a computer program. For example, it was demonstrated that an increasing ratio of synthetic data can facilitate model collapse.[<span class="citation" data-cites="kazdan2024collapse">Kazdan et al. (<a href="09-references.html#ref-kazdan2024collapse" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="shumailov2024ai">Shumailov et al. (<a href="09-references.html#ref-shumailov2024ai" role="doc-biblioref">2024</a>)</span>]</p>
<p>Having reviewed the data sources and generation methods, we will, in the following, discuss architectures and training approaches for GPMs.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-alberts2002molecular" class="csl-entry" role="listitem">
Alberts, Bruce. 2002. <em>Molecular Biology of the Cell</em>. 4th ed. Garland Science.
</div>
<div id="ref-arus2019randomized" class="csl-entry" role="listitem">
Arús-Pous, Josep, Simon Viet Johansson, Oleksii Prykhodko, Esben Jannik Bjerrum, Christian Tyrchan, Jean-Louis Reymond, Hongming Chen, and Ola Engkvist. 2019. <span>“Randomized SMILES Strings Improve the Quality of Molecular Generative Models.”</span> <em>Journal of Cheminformatics</em> 11: 1–13. <a href="https://doi.org/10.1186/s13321-019-0393-0">https://doi.org/10.1186/s13321-019-0393-0</a>.
</div>
<div id="ref-bjerrum2017smiles" class="csl-entry" role="listitem">
Bjerrum, Esben Jannik. 2017. <span>“SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules.”</span> <em>arXiv Preprint arXiv:1703.07076</em>. <a href="https://doi.org/10.48550/arXiv.1703.07076">https://doi.org/10.48550/arXiv.1703.07076</a>.
</div>
<div id="ref-bojanowski2017enriching" class="csl-entry" role="listitem">
Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. <span>“Enriching Word Vectors with Subword Information.”</span> <em>Transactions of the Association for Computational Linguistics</em> 5: 135–46. <a href="https://doi.org/10.1162/tacl_a_00051">https://doi.org/10.1162/tacl_a_00051</a>.
</div>
<div id="ref-born2023chemical" class="csl-entry" role="listitem">
Born, Jannis, Greta Markert, Nikita Janakarajan, Talia B Kimber, Andrea Volkamer, Marı́a Rodrı́guez Martı́nez, and Matteo Manica. 2023. <span>“Chemical Representation Learning for Toxicity Prediction.”</span> <em>Digital Discovery</em> 2 (3): 674–91. <a href="https://doi.org/10.1039/d2dd00099g">https://doi.org/10.1039/d2dd00099g</a>.
</div>
<div id="ref-brinkhaus2022randepict" class="csl-entry" role="listitem">
Brinkhaus, Henning Otto, Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. 2022. <span>“<span class="nocase">RanDepict: Random chemical structure depiction generator</span>.”</span> <em>Journal of Cheminformatics</em> 14 (1): 31. <a href="https://doi.org/10.1186/s13321-022-00609-4">https://doi.org/10.1186/s13321-022-00609-4</a>.
</div>
<div id="ref-Cassani2021BZ" class="csl-entry" role="listitem">
Cassani, Andrea, Alessandro Monteverde, and Marco Piumetti. 2021. <span>“Belousov–Zhabotinsky Type Reactions: The Non-Linear Behavior of Chemical Systems.”</span> <em>Journal of Mathematical Chemistry</em> 59 (3): 792–826. <a href="https://doi.org/10.1007/s10910-021-01223-9">https://doi.org/10.1007/s10910-021-01223-9</a>.
</div>
<div id="ref-commoncrawl" class="csl-entry" role="listitem">
<span>“Common Crawl.”</span> 2024. <a href="https://commoncrawl.org">https://commoncrawl.org</a>.
</div>
<div id="ref-de2015lean" class="csl-entry" role="listitem">
De Moura, Leonardo, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. 2015. <span>“<span class="nocase">The Lean theorem prover (system description)</span>.”</span> <em>Automated Deduction-CADE-25: 25th International Conference on Automated Deduction</em>, 378–88. <a href="https://doi.org/10.1007/978-3-319-21401-6_26">https://doi.org/10.1007/978-3-319-21401-6_26</a>.
</div>
<div id="ref-dotan2019value0laden" class="csl-entry" role="listitem">
Dotan, Ravit, and S. Milli. 2019. <span>“Value-Laden Disciplinary Shifts in Machine Learning.”</span> <em>FAT*</em>. <a href="https://doi.org/10.1145/3351095.3373157">https://doi.org/10.1145/3351095.3373157</a>.
</div>
<div id="ref-edunov2018understanding" class="csl-entry" role="listitem">
Edunov, Sergey, Myle Ott, Michael Auli, and David Grangier. 2018. <span>“Understanding Back-Translation at Scale.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.1808.09381">https://doi.org/10.48550/arXiv.1808.09381</a>.
</div>
<div id="ref-gao2020general" class="csl-entry" role="listitem">
Gao, Peng, Jun Zhang, Qian Peng, Jie Zhang, and Vassiliki-Alexandra Glezakou. 2020. <span>“General Protocol for the Accurate Prediction of Molecular 13C/1H NMR Chemical Shifts via Machine Learning Augmented DFT.”</span> <em>Journal of Chemical Information and Modeling</em> 60 (8): 3746–54.
</div>
<div id="ref-gonzales2024evaluating" class="csl-entry" role="listitem">
Gonzales, Carmelo, Michael Martin Pieler, Kevin Maik Jablonka, and Santiago Miret. 2024. <span>“<span class="nocase">Evaluating Chemistry Prompts for Large-Language Model Fine-Tuning</span>.”</span> <em>AI for Accelerated Materials Design - NeurIPS 2024</em>. <a href="https://openreview.net/forum?id=cEkUia8neA">https://openreview.net/forum?id=cEkUia8neA</a>.
</div>
<div id="ref-grattafiori2024llama" class="csl-entry" role="listitem">
Grattafiori, Aaron, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, et al. 2024. <span>“<span class="nocase">The Llama 3 Herd of Models</span>.”</span> <em>arXiv Preprint arXiv: 2407.21783</em>. <a href="https://doi.org/10.48550/arXiv.2407.21783">https://doi.org/10.48550/arXiv.2407.21783</a>.
</div>
<div id="ref-gunasekar2023textbooks" class="csl-entry" role="listitem">
Gunasekar, Suriya, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, et al. 2023. <span>“Textbooks Are All You Need.”</span> <em>arXiv Preprint arXiv: 2306.11644</em>. <a href="https://doi.org/10.48550/arXiv.2306.11644">https://doi.org/10.48550/arXiv.2306.11644</a>.
</div>
<div id="ref-guo2025deepseek" class="csl-entry" role="listitem">
Guo, Daya, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, et al. 2025. <span>“<span class="nocase">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</span>.”</span> <em>arXiv Preprint arXiv:2501.12948</em>. <a href="https://doi.org/10.48550/arXiv.2501.12948">https://doi.org/10.48550/arXiv.2501.12948</a>.
</div>
<div id="ref-heidorn2008shedding" class="csl-entry" role="listitem">
Heidorn, P Bryan. 2008. <span>“<span class="nocase">Shedding light on the dark data in the long tail of science</span>.”</span> <em>Library Trends</em> 57 (2): 280–99. <a href="https://doi.org/10.1353/lib.0.0036">https://doi.org/10.1353/lib.0.0036</a>.
</div>
<div id="ref-hong2024cantonmt0" class="csl-entry" role="listitem">
Hong, Kung Yin, Lifeng Han, Riza Batista-Navarro, and Goran Nenadic. 2024. <span>“<span class="nocase">CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data</span>.”</span> <em>arXiv Preprint arXiv: 2403.11346</em>. <a href="https://doi.org/10.48550/arXiv.2403.11346">https://doi.org/10.48550/arXiv.2403.11346</a>.
</div>
<div id="ref-hooker2020hardware" class="csl-entry" role="listitem">
Hooker, Sara. 2020. <span>“The Hardware Lottery.”</span> <em>Communications of the ACM</em>. <a href="https://doi.org/10.1145/3467017">https://doi.org/10.1145/3467017</a>.
</div>
<div id="ref-hymarcReversibleHydrides" class="csl-entry" role="listitem">
HyMARC. 2019. <span>“<span>Hydrogen Storage Materials Database</span>.”</span> <a href="https://www.hymarc.org/home">https://www.hymarc.org/home</a>.
</div>
<div id="ref-jablonka2024leveraging" class="csl-entry" role="listitem">
Jablonka, Kevin Maik, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. 2024. <span>“<span class="nocase">Leveraging large language models for predictive chemistry</span>.”</span> <em>Nature Machine Intelligence</em> 6 (2): 161–69. <a href="https://doi.org/10.1038/s42256-023-00788-1">https://doi.org/10.1038/s42256-023-00788-1</a>.
</div>
<div id="ref-kaplan2020scaling" class="csl-entry" role="listitem">
Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. <span>“<span class="nocase">Scaling Laws for Neural Language Models</span>.”</span> <em>arXiv Preprint arXiv: 2001.08361</em>. <a href="https://doi.org/10.48550/arXiv.2001.08361">https://doi.org/10.48550/arXiv.2001.08361</a>.
</div>
<div id="ref-kazdan2024collapse" class="csl-entry" role="listitem">
Kazdan, Joshua, Rylan Schaeffer, Apratim Dey, Matthias Gerstgrasser, Rafael Rafailov, David L. Donoho, and Sanmi Koyejo. 2024. <span>“<span class="nocase">Collapse or Thrive? Perils and Promises of Synthetic Data in a Self-Generating World</span>.”</span> <em>arXiv Preprint arXiv: 2410.16713</em>. <a href="https://doi.org/10.48550/arXiv.2410.16713">https://doi.org/10.48550/arXiv.2410.16713</a>.
</div>
<div id="ref-ke2018convolutional" class="csl-entry" role="listitem">
Ke, T-W, Aaron S Brewster, Stella X Yu, Daniela Ushizima, Chao Yang, and Nicholas K Sauter. 2018. <span>“A Convolutional Neural Network-Based Screening Tool for x-Ray Serial Crystallography.”</span> <em>Synchrotron Radiation</em> 25 (3): 655–70.
</div>
<div id="ref-Kearnes_2021" class="csl-entry" role="listitem">
Kearnes, Steven M., Michael R. Maser, Michael Wleklinski, Anton Kast, Abigail G. Doyle, Spencer D. Dreher, Joel M. Hawkins, Klavs F. Jensen, and Connor W. Coley. 2021. <span>“<span>The Open Reaction Database</span>.”</span> <em>J. Am. Chem. Soc.</em> 143 (45): 18820–26. <a href="https://doi.org/10.1021/jacs.1c09820">https://doi.org/10.1021/jacs.1c09820</a>.
</div>
<div id="ref-kimber2021maxsmi" class="csl-entry" role="listitem">
Kimber, Talia B, Maxime Gagnebin, and Andrea Volkamer. 2021. <span>“Maxsmi: Maximizing Molecular Property Prediction Performance with Confidence Estimation Using Smiles Augmentation and Deep Learning.”</span> <em>Artificial Intelligence in the Life Sciences</em> 1: 100014. <a href="https://doi.org/10.1016/j.ailsci.2021.100014">https://doi.org/10.1016/j.ailsci.2021.100014</a>.
</div>
<div id="ref-kobayashi2018contextual" class="csl-entry" role="listitem">
Kobayashi, Sosuke. 2018. <span>“<span class="nocase">Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations</span>.”</span> <em>arXiv Preprint arXiv: 1805.06201</em>. <a href="https://doi.org/10.48550/arXiv.1805.06201">https://doi.org/10.48550/arXiv.1805.06201</a>.
</div>
<div id="ref-NEURIPS2024_53704142" class="csl-entry" role="listitem">
Koziarski, Andrei, Michałand Rekesh, Dmytro Shevchuk, Almer van der Sloot, Piotr Gaiński, Yoshua Bengio, Chenghao Liu, Mike Tyers, and Robert Batey. 2024. <span>“<span>RGFN: Synthesizable Molecular Generation Using GFlowNets</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 37: 46908–55. <a href="https://doi.org/10.48550/arXiv.2406.08506">https://doi.org/10.48550/arXiv.2406.08506</a>.
</div>
<div id="ref-krizhevsky2012imagenet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“Imagenet Classification with Deep Convolutional Neural Networks.”</span> <em>Advances in Neural Information Processing Systems</em> 25. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div>
<div id="ref-li2023camel" class="csl-entry" role="listitem">
Li, Guohao, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. <span>“<span class="nocase">CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society</span>.”</span> <em>arXiv Preprint arXiv: 2303.17760</em>. <a href="https://doi.org/10.48550/arXiv.2303.17760">https://doi.org/10.48550/arXiv.2303.17760</a>.
</div>
<div id="ref-lu2024mathgenie0" class="csl-entry" role="listitem">
Lu, Zimu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. 2024. <span>“<span class="nocase">MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs</span>.”</span> <em>Annual Meeting of the Association for Computational Linguistics</em>. <a href="https://doi.org/10.48550/arXiv.2402.16352">https://doi.org/10.48550/arXiv.2402.16352</a>.
</div>
<div id="ref-maini2024rephrasing" class="csl-entry" role="listitem">
Maini, Pratyush, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. 2024. <span>“Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling.”</span> <em>arXiv Preprint arXiv: 2401.16380</em>. <a href="https://doi.org/10.48550/arXiv.2401.16380">https://doi.org/10.48550/arXiv.2401.16380</a>.
</div>
<div id="ref-marion2023less" class="csl-entry" role="listitem">
Marion, Max, Ahmet Üstün, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. 2023. <span>“When Less Is More: Investigating Data Pruning for Pretraining LLMs at Scale.”</span> <em>arXiv Preprint arXiv: 2309.04564</em>. <a href="https://doi.org/10.48550/arXiv.2309.04564">https://doi.org/10.48550/arXiv.2309.04564</a>.
</div>
<div id="ref-martin2022bridging" class="csl-entry" role="listitem">
Martin, Stephen F. 2022. <span>“<span class="nocase">Bridging known and unknown unknowns: From natural products and their mimics to unmet needs in neuroscience</span>.”</span> <em>Accounts of Chemical Research</em> 55 (17): 2397–2408. <a href="https://doi.org/10.1021/acs.accounts.1c00773">https://doi.org/10.1021/acs.accounts.1c00773</a>.
</div>
<div id="ref-mirza2025chempile0" class="csl-entry" role="listitem">
Mirza, Adrian, Nawaf Alampara, Martiño Rı́os-Garcı́a, Mohamed Abdelalim, Jack Butler, Bethany Connolly, Tunca Dogan, et al. 2025. <span>“ChemPile: A 250GB Diverse and Curated Dataset for Chemical Foundation Models.”</span> <em>arXiv Preprint arXiv: 2505.12534</em>. <a href="https://doi.org/10.48550/arXiv.2505.12534">https://doi.org/10.48550/arXiv.2505.12534</a>.
</div>
<div id="ref-moreno2022application" class="csl-entry" role="listitem">
Moreno-Barea, Francisco J, Leonardo Franco, David Elizondo, and Martin Grootveld. 2022. <span>“Application of Data Augmentation Techniques Towards Metabolomics.”</span> <em>Computers in Biology and Medicine</em> 148: 105916.
</div>
<div id="ref-Musil_2021" class="csl-entry" role="listitem">
Musil, Felix, Andrea Grisafi, Albert P. Bartók, Christoph Ortner, Gábor Csányi, and Michele Ceriotti. 2021. <span>“<span class="nocase">Physics-Inspired Structural Representations for Molecules and Materials</span>.”</span> <em>Chemical Reviews</em> 121 (16): 9759–9815. <a href="https://doi.org/10.1021/acs.chemrev.1c00021">https://doi.org/10.1021/acs.chemrev.1c00021</a>.
</div>
<div id="ref-oviedo2019fast" class="csl-entry" role="listitem">
Oviedo, Felipe, Zekun Ren, Shijing Sun, Charles Settens, Zhe Liu, Noor Titan Putri Hartono, Savitha Ramasamy, et al. 2019. <span>“Fast and Interpretable Classification of Small x-Ray Diffraction Datasets Using Data Augmentation and Deep Neural Networks.”</span> <em>Npj Computational Materials</em> 5 (1): 60.
</div>
<div id="ref-penedo2024fineweb" class="csl-entry" role="listitem">
Penedo, Guilherme, Hynek Kydlı́ček, Anton Lozhkov, Margaret Mitchell, Colin A Raffel, Leandro Von Werra, Thomas Wolf, et al. 2024. <span>“<span class="nocase">The fineweb datasets: Decanting the web for the finest text data at scale</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 37: 30811–49. <a href="https://doi.org/10.48550/arXiv.2406.17557">https://doi.org/10.48550/arXiv.2406.17557</a>.
</div>
<div id="ref-penedo2023refinedweb" class="csl-entry" role="listitem">
Penedo, Guilherme, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. <span>“The Refinedweb Dataset for Falcon Llm: Outperforming Curated Corpora with Web Data Only.”</span> <em>Advances in Neural Information Processing Systems</em> 36: 79155–72. <a href="https://doi.org/10.48550/arXiv.2306.01116">https://doi.org/10.48550/arXiv.2306.01116</a>.
</div>
<div id="ref-pieler2024rephrasing" class="csl-entry" role="listitem">
Pieler, Michael, Marco Bellagente, Hannah Teufel, Duy Phung, Nathan Cooper, Jonathan Tow, Paulo Rocha, et al. 2024. <span>“Rephrasing Natural Text Data with Different Languages and Quality Levels for Large Language Model Pre-Training.”</span> <em>arXiv Preprint arXiv:2410.20796</em>. <a href="https://doi.org/10.48550/arXiv.2410.20796">https://doi.org/10.48550/arXiv.2410.20796</a>.
</div>
<div id="ref-Pietsch_2017" class="csl-entry" role="listitem">
Pietsch, Wolfgang, and Jörg Wernecke. 2017. <span>“Introduction: Ten Theses on Big Data and Computability.”</span> In <em>Berechenbarkeit Der Welt?</em>, 37–57. Springer Fachmedien Wiesbaden. <a href="https://doi.org/10.1007/978-3-658-12153-2_2">https://doi.org/10.1007/978-3-658-12153-2_2</a>.
</div>
<div id="ref-ramakrishnan2014quantum" class="csl-entry" role="listitem">
Ramakrishnan, Raghunathan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. 2014. <span>“<span class="nocase">Quantum chemistry structures and properties of 134 kilo molecules</span>.”</span> <em>Scientific Data</em> 1 (1): 1–7. <a href="https://doi.org/10.1038/sdata.2014.22">https://doi.org/10.1038/sdata.2014.22</a>.
</div>
<div id="ref-rulev2017serendipity" class="csl-entry" role="listitem">
Rulev, Alexander Yu. 2017. <span>“<span class="nocase">Serendipity or the art of making discoveries</span>.”</span> <em>New Journal of Chemistry</em> 41 (11): 4262–68. <a href="https://doi.org/10.1039/c7nj00182g">https://doi.org/10.1039/c7nj00182g</a>.
</div>
<div id="ref-scheidgen2023nomad" class="csl-entry" role="listitem">
Scheidgen, Markus, Lauri Himanen, Alvin Noe Ladines, David Sikter, Mohammad Nakhaee, Ádám Fekete, Theodore Chang, et al. 2023. <span>“<span class="nocase">NOMAD: A distributed web-based platform for managing materials science research data</span>.”</span> <em>Journal of Open Source Software</em> 8 (90): 5388. <a href="https://doi.org/10.21105/joss.05388">https://doi.org/10.21105/joss.05388</a>.
</div>
<div id="ref-shao2024deepseekmath0" class="csl-entry" role="listitem">
Shao, Zhihong, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, et al. 2024. <span>“<span class="nocase">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</span>.”</span> <em>arXiv Preprint arXiv: 2402.03300</em>. <a href="https://doi.org/10.48550/arXiv.2402.03300">https://doi.org/10.48550/arXiv.2402.03300</a>.
</div>
<div id="ref-shorten2019survey" class="csl-entry" role="listitem">
Shorten, Connor, and Taghi M Khoshgoftaar. 2019. <span>“<span class="nocase">A survey on image data augmentation for deep learning</span>.”</span> <em>Journal of Big Data</em> 6 (1): 1–48. <a href="https://doi.org/10.1186/s40537-019-0197-0">https://doi.org/10.1186/s40537-019-0197-0</a>.
</div>
<div id="ref-shorten2021text" class="csl-entry" role="listitem">
Shorten, Connor, Taghi M Khoshgoftaar, and Borko Furht. 2021. <span>“<span class="nocase">Text data augmentation for deep learning</span>.”</span> <em>Journal of Big Data</em> 8 (1): 101. <a href="https://doi.org/10.1186/s40537-021-00492-0">https://doi.org/10.1186/s40537-021-00492-0</a>.
</div>
<div id="ref-shumailov2024ai" class="csl-entry" role="listitem">
Shumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. 2024. <span>“AI Models Collapse When Trained on Recursively Generated Data.”</span> <em>Nature</em> 631 (8022): 755–59. <a href="https://doi.org/10.1038/s41586-024-07566-y">https://doi.org/10.1038/s41586-024-07566-y</a>.
</div>
<div id="ref-thrush2024improving" class="csl-entry" role="listitem">
Thrush, Tristan, Christopher Potts, and Tatsunori Hashimoto. 2024. <span>“Improving Pretraining Data Using Perplexity Correlations.”</span> <em>arXiv Preprint arXiv:2409.05816</em>. <a href="https://doi.org/10.48550/arXiv.2409.05816">https://doi.org/10.48550/arXiv.2409.05816</a>.
</div>
<div id="ref-trinh2024solving" class="csl-entry" role="listitem">
Trinh, Trieu H, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. 2024. <span>“<span class="nocase">Solving olympiad geometry without human demonstrations</span>.”</span> <em>Nature</em> 625 (7995): 476–82. <a href="https://doi.org/10.1038/s41586-023-06747-5">https://doi.org/10.1038/s41586-023-06747-5</a>.
</div>
<div id="ref-unke2021machine" class="csl-entry" role="listitem">
Unke, Oliver T, Stefan Chmiela, Huziel E Sauceda, Michael Gastegger, Igor Poltavsky, Kristof T Schutt, Alexandre Tkatchenko, and Klaus-Robert Muller. 2021. <span>“<span class="nocase">Machine learning force fields</span>.”</span> <em>Chemical Reviews</em> 121 (16): 10142–86. <a href="https://doi.org/10.1021/acs.chemrev.0c01111">https://doi.org/10.1021/acs.chemrev.0c01111</a>.
</div>
<div id="ref-vanherck2025assessment" class="csl-entry" role="listitem">
Van Herck, Joren, Marı́a Victoria Gil, Kevin Maik Jablonka, Alex Abrudan, Andy S. Anker, Mehrdad Asgari, Ben Blaiszik, et al. 2025. <span>“<span class="nocase">Assessment of fine-tuned large language models for real-world chemistry and material science applications</span>.”</span> <em>Chemical Science</em> 16 (2): 670–84. <a href="https://doi.org/10.1039/D4SC04401K">https://doi.org/10.1039/D4SC04401K</a>.
</div>
<div id="ref-wei2019eda0" class="csl-entry" role="listitem">
Wei, Jason, and Kai Zou. 2019. <span>“<span class="nocase">EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.1901.11196">https://doi.org/10.48550/arXiv.1901.11196</a>.
</div>
<div id="ref-wenzel2008isabelle" class="csl-entry" role="listitem">
Wenzel, Makarius, Lawrence C Paulson, and Tobias Nipkow. 2008. <span>“<span class="nocase">The isabelle framework</span>.”</span> <em>International Conference on Theorem Proving in Higher Order Logics</em>, 33–38. <a href="https://doi.org/10.1007/978-3-540-71067-7_7">https://doi.org/10.1007/978-3-540-71067-7_7</a>.
</div>
<div id="ref-wilson2025deep" class="csl-entry" role="listitem">
Wilson, Andrew Gordon. 2025. <span>“<span class="nocase">Deep Learning is Not So Mysterious or Different</span>.”</span> <em>arXiv Preprint arXiv: 2503.02113</em>. <a href="https://doi.org/10.48550/arXiv.2503.02113">https://doi.org/10.48550/arXiv.2503.02113</a>.
</div>
<div id="ref-NEURIPS2022_d0c6bc64" class="csl-entry" role="listitem">
Wu, Yuhuai, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. 2022. <span>“<span class="nocase">Autoformalization with Large Language Models</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 35: 32353–68. <a href="https://doi.org/10.48550/arXiv.2205.12615">https://doi.org/10.48550/arXiv.2205.12615</a>.
</div>
<div id="ref-xie2023darwin" class="csl-entry" role="listitem">
Xie, Tong, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, et al. 2023. <span>“<span class="nocase">Darwin series: Domain specific large language models for natural science</span>.”</span> <em>arXiv Preprint arXiv:2308.13565</em>. <a href="https://doi.org/10.48550/arXiv.2308.13565">https://doi.org/10.48550/arXiv.2308.13565</a>.
</div>
<div id="ref-xin2024deepseek" class="csl-entry" role="listitem">
Xin, Huajian, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. 2024. <span>“<span class="nocase">Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data</span>.”</span> <em>arXiv Preprint arXiv:2405.14333</em>. <a href="https://doi.org/10.48550/arXiv.2405.14333">https://doi.org/10.48550/arXiv.2405.14333</a>.
</div>
<div id="ref-zunger2019beware" class="csl-entry" role="listitem">
Zunger, Alex. 2019. <span>“<span class="nocase">Beware of plausible predictions of fantasy materials</span>.”</span> <em>Nature</em> 566 (7745): 447–49. <a href="https://doi.org/10.1038/d41586-019-00676-y">https://doi.org/10.1038/d41586-019-00676-y</a>.
</div>
</div>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./01-introduction.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./03-architectures.html" class="pagination-link" aria-label="Building Principles of GPMs">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Building Principles of GPMs</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><img src="https://raw.githubusercontent.com/lamalab-org/lamalab.github.io/main/static/png-file.png" alt="Lab for AI in Materials Science logo" style="height:1.4rem;vertical-align:middle;margin-right:0.4rem;"></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Copyright © 2025 Lab for AI in Materials Science</p>
</div>
  </div>
</footer>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>