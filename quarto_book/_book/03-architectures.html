<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Building Principles of GPMs – General Purpose Models for the Chemical Sciences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04-evals.html" rel="next">
<link href="./02-data_taxonomy.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b95ce3833fcbbb97f3687e6914a03b85.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-architectures.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Building Principles of GPMs</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">General Purpose Models for the Chemical Sciences</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">General purpose models for the chemical sciences</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-data_taxonomy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Shape and Structure of Chemical Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-architectures.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Building Principles of GPMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-evals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Evaluations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-safety.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implications of GPMs: Education, Safety, and Ethics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-outlook_conclusions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Outlook and Conclusions</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#building-principles-of-gpms" id="toc-building-principles-of-gpms" class="nav-link active" data-scroll-target="#building-principles-of-gpms"><span class="header-section-number">5</span> Building Principles of GPMs</a>
  <ul class="collapse">
  <li><a href="#sec:taxonomy" id="toc-sec:taxonomy" class="nav-link" data-scroll-target="#sec\:taxonomy"><span class="header-section-number">5.1</span> Taxonomy of Foundation Models</a></li>
  <li><a href="#representations" id="toc-representations" class="nav-link" data-scroll-target="#representations"><span class="header-section-number">5.2</span> Representations</a>
  <ul class="collapse">
  <li><a href="#sec:common_representations" id="toc-sec:common_representations" class="nav-link" data-scroll-target="#sec\:common_representations"><span class="header-section-number">5.2.1</span> Common Representations of Molecules and Materials</a></li>
  <li><a href="#tokenization" id="toc-tokenization" class="nav-link" data-scroll-target="#tokenization"><span class="header-section-number">5.2.2</span> Tokenization</a></li>
  <li><a href="#sec:embeddings" id="toc-sec:embeddings" class="nav-link" data-scroll-target="#sec\:embeddings"><span class="header-section-number">5.2.3</span> Embeddings</a></li>
  </ul></li>
  <li><a href="#general-training-workflow" id="toc-general-training-workflow" class="nav-link" data-scroll-target="#general-training-workflow"><span class="header-section-number">5.3</span> General Training Workflow</a></li>
  <li><a href="#sec:pretraining" id="toc-sec:pretraining" class="nav-link" data-scroll-target="#sec\:pretraining"><span class="header-section-number">5.4</span> Pre-training: Learning the Shape of Data</a>
  <ul class="collapse">
  <li><a href="#sec:ssl" id="toc-sec:ssl" class="nav-link" data-scroll-target="#sec\:ssl"><span class="header-section-number">5.4.1</span> Self-Supervision</a></li>
  <li><a href="#families-of-self-supervised-learning" id="toc-families-of-self-supervised-learning" class="nav-link" data-scroll-target="#families-of-self-supervised-learning"><span class="header-section-number">5.4.2</span> Families of Self-Supervised Learning</a></li>
  <li><a href="#generative-methods" id="toc-generative-methods" class="nav-link" data-scroll-target="#generative-methods"><span class="header-section-number">5.4.3</span> Generative Methods</a></li>
  <li><a href="#sec:contrastive_learning" id="toc-sec:contrastive_learning" class="nav-link" data-scroll-target="#sec\:contrastive_learning"><span class="header-section-number">5.4.4</span> Contrastive Learning</a></li>
  </ul></li>
  <li><a href="#the-holy-grail-of-building-good-internal-representation" id="toc-the-holy-grail-of-building-good-internal-representation" class="nav-link" data-scroll-target="#the-holy-grail-of-building-good-internal-representation"><span class="header-section-number">5.5</span> The Holy Grail of Building Good Internal Representation</a></li>
  <li><a href="#sec:fine_tuning_coloring" id="toc-sec:fine_tuning_coloring" class="nav-link" data-scroll-target="#sec\:fine_tuning_coloring"><span class="header-section-number">5.6</span> Fine-Tuning: Learning the Coloring of Data</a></li>
  <li><a href="#sec:rl" id="toc-sec:rl" class="nav-link" data-scroll-target="#sec\:rl"><span class="header-section-number">5.7</span> Post-Supervised Adaptation: Learning to Align and Shape Behavior</a></li>
  <li><a href="#sec:example_architectures" id="toc-sec:example_architectures" class="nav-link" data-scroll-target="#sec\:example_architectures"><span class="header-section-number">5.8</span> Example Architectures</a></li>
  <li><a href="#multimodality" id="toc-multimodality" class="nav-link" data-scroll-target="#multimodality"><span class="header-section-number">5.9</span> Multimodality</a>
  <ul class="collapse">
  <li><a href="#sec:multimodal_chem" id="toc-sec:multimodal_chem" class="nav-link" data-scroll-target="#sec\:multimodal_chem"><span class="header-section-number">5.9.1</span> Multimodal Integration in Chemistry</a></li>
  </ul></li>
  <li><a href="#optimizations" id="toc-optimizations" class="nav-link" data-scroll-target="#optimizations"><span class="header-section-number">5.10</span> Optimizations</a>
  <ul class="collapse">
  <li><a href="#sec:arch-moes" id="toc-sec:arch-moes" class="nav-link" data-scroll-target="#sec\:arch-moes"><span class="header-section-number">5.10.1</span> Mixture-of-Experts</a></li>
  <li><a href="#sec:quantization" id="toc-sec:quantization" class="nav-link" data-scroll-target="#sec\:quantization"><span class="header-section-number">5.10.2</span> Quantization and Mixed Precision</a></li>
  <li><a href="#sec:peft" id="toc-sec:peft" class="nav-link" data-scroll-target="#sec\:peft"><span class="header-section-number">5.10.3</span> Parameter-Efficient Tuning</a></li>
  <li><a href="#distillation" id="toc-distillation" class="nav-link" data-scroll-target="#distillation"><span class="header-section-number">5.10.4</span> Distillation</a></li>
  </ul></li>
  <li><a href="#sec:model_adaptation" id="toc-sec:model_adaptation" class="nav-link" data-scroll-target="#sec\:model_adaptation"><span class="header-section-number">5.11</span> Model Level Adaptation</a></li>
  <li><a href="#sec:agents" id="toc-sec:agents" class="nav-link" data-scroll-target="#sec\:agents"><span class="header-section-number">5.12</span> System-level Integration: Agents</a>
  <ul class="collapse">
  <li><a href="#sec:arch_agents" id="toc-sec:arch_agents" class="nav-link" data-scroll-target="#sec\:arch_agents"><span class="header-section-number">5.12.1</span> Core Components of an Agentic System</a></li>
  <li><a href="#approaches-for-building-agentic-system" id="toc-approaches-for-building-agentic-system" class="nav-link" data-scroll-target="#approaches-for-building-agentic-system"><span class="header-section-number">5.12.2</span> Approaches for building Agentic System</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Building Principles of GPMs</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="building-principles-of-gpms" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Building Principles of GPMs</h1>
<section id="sec:taxonomy" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="sec:taxonomy"><span class="header-section-number">5.1</span> Taxonomy of Foundation Models</h2>
<p>In this review, we focus on <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>, which are commonly trained on vast, often unstructured datasets and are able to generalize to new tasks easily. Currently, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> are the most prominent members of the <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> family, but many of the principles discussed here are transferable across different types of <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>, and we will use the term <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> to highlight these general applications.</p>
<p><span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> can efficiently operate in the low-data regime. In contrast to conventional transfer learning where a model is pre-trained on a task and then adapted to a slightly modified task through fine-tuning, we often do not need to change the weights of a <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span>, since the model can adapt to new problems at inference time through techniques such as <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span>[<span class="citation" data-cites="brown2020language">Brown et al. (<a href="#ref-brown2020language" role="doc-biblioref">2020</a>)</span>] or <span data-acronym-label="rag" data-acronym-form="singular+short">rag</span> (see <a href="#sec:model_adaptation" data-reference-type="ref+Label" data-reference="sec:model_adaptation">1.11</a>).[<span class="citation" data-cites="lewis2020retrieval">Lewis et al. (<a href="#ref-lewis2020retrieval" role="doc-biblioref">2020</a>)</span>]</p>
<p><span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> are not chemistry-specific models. They are models for general use that leverage cross-domain information. Chemistry is deeply intertwined with other scientific fields, including physics, biology, and mathematics. Being able to access all this knowledge at inference time can be critical. In addition, there is a compelling argument to be made for <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>: natural language has evolved to represent all concepts that humans can ponder. Thus, leveraging natural language might be a very productive way to approach scientific discovery. Furthermore, <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> can be integrated with problem-solving tools (<a href="#sec:agents" data-reference-type="ref+Label" data-reference="sec:agents">1.12</a>). Such systems are typically described as being <em>agentic</em> and are key to further automation and integration of <span data-acronym-label="ai" data-acronym-form="singular+short">ai</span> systems in the real world.</p>
<p>In the following, we discuss how such models work and can be trained.</p>
</section>
<section id="representations" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="representations"><span class="header-section-number">5.2</span> Representations</h2>
<p>To interact with any machine, we need to convert the input into numeric values. At its core, all information within a computer is represented as bits (zeros and ones). Bits are grouped into bytes (8 bits), and meaning is assigned to these sequences through encoding schemes like <code>ASCII</code> or <code>UTF-8</code>. Everything—text, a pixel in an image, or even a chemical structure—can be stored as sequences of bytes. For example, “” can be translated into the byte sequence, “H”, “2”, “O”. However, using raw byte sequences for <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> presents significant computational inefficiency as representing chemical entities requires long byte sequences, and models would need to learn complex mappings between arbitrary byte patterns and their meanings (as the encoding schemes are not built around chemical principles). Furthermore, handling variable-length sequences can pose additional challenges for models, as they may struggle to perform well on unseen inputs. [<span class="citation" data-cites="zhou2023algorithms">Zhou et al. (<a href="#ref-zhou2023algorithms" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="baillargeon2022assessing">Baillargeon and Lamontagne (<a href="#ref-baillargeon2022assessing" role="doc-biblioref">2022</a>)</span>]</p>
<p>A more efficient mapping that is built on top of the underlying byte representation is <span data-acronym-label="ohe" data-acronym-form="singular+short">ohe</span>. Instead of working with variable-length byte sequences, we create a fixed vocabulary ({, , }) where each discrete category (in this case, molecule) gets a unique vector: becomes [1, 0, 0], becomes [0, 1, 0], and so on. This provides unambiguous, computationally manageable representations. As the number of categories grows, one-hot vectors become increasingly long and sparse, making them computationally inefficient—particularly for large vocabularies, i.e., many categories. For example, we need a vocabulary of size 118 to model only the unique elements in the periodic table. Now, imagine the vocabulary required for all unique compounds—the size combinatorially explodes. More importantly, while <span data-acronym-label="ohe" data-acronym-form="singular+short">ohe</span> distinguishes molecules or elements, it still treats them as entirely independent. It does not capture any properties of the entity it represents. For example, the ordering of numbers (such as <span class="math inline">\(4&lt;5\)</span>) or chemical similarities (such as being more similar to but less similar to ) would not be preserved. [<span class="citation" data-cites="chuang2018comment">Chuang and Keiser (<a href="#ref-chuang2018comment" role="doc-biblioref">2018</a>)</span>] Embeddings (learned encoding), that we will discuss in <a href="#sec:embeddings" data-reference-type="ref+Label" data-reference="sec:embeddings">1.2.3</a>, solve this through learned, dense vector representations.</p>
<section id="sec:common_representations" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="sec:common_representations"><span class="header-section-number">5.2.1</span> Common Representations of Molecules and Materials</h3>
<p>Before any chemical entity can be converted into a numerical vector—whether through simple <span data-acronym-label="ohe" data-acronym-form="singular+short">ohe</span> or complex learned embeddings—it must first be described in a standardized format (for example, if we are working with materials, it should be able to encode all materials), which is then mapped to encodings.</p>
<p>For complex entities like molecules, materials, and reactions, this choice of what fundamental units to represent (“should we include only atomic numbers?”, “Should we include something about the structure?”, etc.) is among the most consequential decisions in building a model. It determines the inductive biases—the set of assumptions that guide learning algorithms toward specific patterns over others. The landscape of chemical representations reflects different answers to this question, each making distinct trade-offs between simplicity, expressiveness, and computational efficiency.</p>
<div id="tab:molecular-representations">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Representation</strong></th>
<th style="text-align: left;"><strong>Encoded information</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Table&nbsp; - continued from previous page</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Representation</strong></td>
<td style="text-align: left;"><strong>Encoded info</strong></td>
<td style="text-align: left;"><strong>Description</strong></td>
<td style="text-align: left;"><strong>Example</strong></td>
</tr>
</tbody>
</table>
<p>Elemental composition Stoichiometry Always available, but non-unique. C9H8O4 <span data-acronym-label="iupac" data-acronym-form="singular+short">iupac</span> name Stoichiometry, bonding, geometry Universally understood, systematic nomenclature, unmanageable for large molecules, and lacks detailed 3D information. -acetyloxybenzoic acid <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> [<span class="citation" data-cites="weininger1988smiles">Weininger (<a href="#ref-weininger1988smiles" role="doc-biblioref">1988</a>)</span>] Stoichiometry, bonding Massive public corpora and tooling support, however, there are several valid strings per molecule, and it does not contain spatial information.</p>
<p><em>etc.</em><br>
<span data-acronym-label="selfies" data-acronym-form="singular+short">selfies</span> [<span class="citation" data-cites="krenn2020self">Krenn et al. (<a href="#ref-krenn2020self" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="cheng2023group">Cheng et al. (<a href="#ref-cheng2023group" role="doc-biblioref">2023</a>)</span>] Stoichiometry, bonding % syntactic and semantic validity by construction, including meaningful grouping. <code>[C][C][=Branch1][C][=O][O]</code> <code>[C][=C][C][=C][C][=C]</code> <code>[Ring1][=Branch1][C]</code> <code>[=Branch1][C][=O][O]</code> <span data-acronym-label="inchi" data-acronym-form="singular+short">inchi</span> Stoichiometry, bonding Canonical one-to-one identifier; encodes stereochemistry layers. <code>InChI=1S/C9H8O4/c1-6(10)13</code> <code>-8-5-3-2-4-7(8)9(11)12/</code> <code>h2-5H,1H3,(H,11,12)</code> Graphs Stoichiometry, bonding, geometry Strong inductive bias that works with <span data-acronym-label="gnn" data-acronym-form="plural+short">gnns</span>. Symmetry-equivariant variants available. Long-range interactions are implicit.<br>
xyz representation Stoichiometry, geometry Exact spatial detail. It is high dimensional, and orientation alignment is needed. 1.2333 0.5540 0.7792 O -0.6952 -2.7148 -0.7502 O 0.7958 -2.1843 0.8685 O 1.7813 0.8105 -1.4821 O -0.0857 0.6088 0.4403 C … Multimodal Stoichiometry, bonding, geometry, symmetry, periodicity, coarse graining Combines complementary signals; boosts robustness and coverage. It is hard to implement, the complexity scales with the amount of representations, some modalities are data-scarce, and the information encoded totally depends on the modalities included.<br>
<span data-acronym-label="cif" data-acronym-form="singular+short">cif</span> [<span class="citation" data-cites="hall1991crystallographic">Hall, Allen, and Brown (<a href="#ref-hall1991crystallographic" role="doc-biblioref">1991</a>)</span>] Stoichiometry, bonding, geometry, periodicity Standardized and widely supported, however, it carries heterogeneous keyword sets and parser overhead <code>data_Si _symmetry_space_group_name_H-M ’P 1’ _cell_length_a 3.85 …_cell_angle_alpha 60.0 …_symmetry_Int_Tables_number 1 _chemical_formula_structural Si _chemical_formula_sum Si2 _cell_volume 40.33 _cell_formula_units_Z 2 loop_ _symmetry_equiv_pos_site_id _symmetry_equiv_pos_as_xyz 1 ’x, y, z’ loop_ _atom_type_symbol _atom_type_oxidation_number Si0+ 0.0loop_ _atom_site_type_symbol _atom_site_label _atom_site_symmetry_multiplicity _atom_site_fract_x …_atom_site_occupancy Si0+ Si0 1 0.75 0.75 0.75 1.0 Si0+ Si1 1 0.0 0.0 0.0 1.0</code> Condensed <span data-acronym-label="cif" data-acronym-form="singular+short">cif</span> [<span class="citation" data-cites="gruver2024finetuned">Gruver et al. (<a href="#ref-gruver2024finetuned" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="antunes2024crystal">Antunes, Butler, and Grau-Crespo (<a href="#ref-antunes2024crystal" role="doc-biblioref">2024</a>)</span>] Stoichiometry, geometry, symmetry, periodicity Good for crystal generation tasks. It omits occupancies and defects, custom tooling is needed, and only works for crystals <code>3.8 3.8 3.8 59 59 59 Si0+ 0.75 0.75 0.75 Si0+ 0.00 0.00 0.00</code> [<span class="citation" data-cites="Xiao_2023">Xiao et al. (<a href="#ref-Xiao_2023" role="doc-biblioref">2023</a>)</span>] Stoichiometry, bonding, periodicity Invertible, symmetry-invariant and compact for general crystals. However, it carries ambiguity for disordered sites <code>Si Si 0 1 + + + 0 1 + + o 0 1 + o + 0 1 o + +</code> [<span class="citation" data-cites="alampara2024mattext">Alampara, Miret, and Jablonka (<a href="#ref-alampara2024mattext" role="doc-biblioref">2024</a>)</span>] Stoichiometry, bonding, symmetry, coarse graining Treats each coordination polyhedron as a “molecule”, it is transferable and compact; but it ignores long-range order and its reconstruction requires post-processing <code>R-3m Si (2c) [Si][Si]([Si])[Si]</code> Natural-language description [<span class="citation" data-cites="ganose2019robocrystallographer">Ganose and Jain (<a href="#ref-ganose2019robocrystallographer" role="doc-biblioref">2019</a>)</span>] Stoichiometry, bonding, geometry, symmetry, periodicity, coarse graining It is human-readable and more intuitive tokenizable by <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>. However, trying to encode all the information can lead to verbose, ambiguous descriptions. “Silicon crystallizes in the diamond-cubic structure, a lattice you can picture as two face-centred-cubic frameworks gently interpenetrating…”</p>
<p>: <strong>Comparison of common molecular representations</strong>. For the encoded information contained by each representation, we followed the criteria used by <span class="citation" data-cites="alampara2024mattext">Alampara, Miret, and Jablonka (<a href="#ref-alampara2024mattext" role="doc-biblioref">2024</a>)</span>. The examples shown are <em>aspirin</em> for elemental composition, <span data-acronym-label="iupac" data-acronym-form="singular+short">iupac</span> name, <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span>, <span data-acronym-label="selfies" data-acronym-form="singular+short">selfies</span>, , graphs, 3D coordinates; and <em>silicon</em> for , condensed , , , and natural-language description. Two non-canonical <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> are shown to illustrate ambiguity. The examples for 3D coordinates, , and natural-language description are truncated to fit in the table. For the multimodal representation, only one of the possible modalities is shown (<span class="math inline">\(^{13}\)</span>C spectrum).</p>
</div>
<p>However, a common strategy is to represent chemical information as a sequence of characters. This allows us to leverage architectures initially designed for natural language. This approach has found particular success in language modeling for predicting protein structures and functions, where the amino acid sequence, the very foundation of a protein’s structure and function, is easily represented as text.[<span class="citation" data-cites="Rives_2021">Rives et al. (<a href="#ref-Rives_2021" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="Elnaggar_2022">Elnaggar et al. (<a href="#ref-Elnaggar_2022" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="Ruffolo_2024">Ruffolo and Madani (<a href="#ref-Ruffolo_2024" role="doc-biblioref">2024</a>)</span>] The most prevalent string representation for molecules in chemistry is <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span>[<span class="citation" data-cites="weininger1988smiles">Weininger (<a href="#ref-weininger1988smiles" role="doc-biblioref">1988</a>)</span>]. <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> strings essentially provide a linear textual representation of a molecular graph, including information about atoms, bonds, and rings. However, <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> representations have significant limitations. The same molecule can have multiple valid <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> strings (so-called non-canonical representations). Although the existence of non-canonical representations enables data augmentation (see <a href="02-data_taxonomy.html#sec:syn-data" data-reference-type="ref+Label" data-reference="sec:syn-data">[sec:syn-data]</a>), it can also confuse models because the same molecule would have different encodings, each one originating from a different <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> string. In addition, <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> imposes a relatively weak inductive bias; the model must still learn the complex rules of valence and bonding from the grammar of these character sequences. Moreover, <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> does not preserve locality: structural motifs that are directly bonded or physically close to each other in a molecule, can be very far apart in the <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> representation. Nevertheless, <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> is widely used owing to its popularity and the amount of data present in different places (internet, papers, databases) using this representation.</p>
<p>A limitation of <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> is that not every <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> string corresponds to a valid molecule. A more robust alternative is <span data-acronym-label="selfies" data-acronym-form="singular+short">selfies</span>[<span class="citation" data-cites="krenn2020self">Krenn et al. (<a href="#ref-krenn2020self" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="cheng2023group">Cheng et al. (<a href="#ref-cheng2023group" role="doc-biblioref">2023</a>)</span>], where every <span data-acronym-label="selfies" data-acronym-form="singular+short">selfies</span> corresponds to a valid molecule, providing a stronger bias towards chemically plausible structures (chemical validity biases). The <span data-acronym-label="inchi" data-acronym-form="singular+short">inchi</span> is another standardized string representation. Unlike <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span>, <span data-acronym-label="inchi" data-acronym-form="singular+short">inchi</span> strings, as identifiers, are canonical—each molecule has exactly one <span data-acronym-label="inchi" data-acronym-form="singular+short">inchi</span> representation. This eliminates ambiguity, but comes at the cost of human readability and increased string length.</p>
<p>In the realm of materials, no natural representation has emerged. Previous work has indicated that for certain phenomena (e.g., when all structures in a dataset are in the ground state), composition might implicitly encode geometric information [<span class="citation" data-cites="tian2022information">Tian et al. (<a href="#ref-tian2022information" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="Jha_2018">Jha et al. (<a href="#ref-Jha_2018" role="doc-biblioref">2018</a>)</span>; <span class="citation" data-cites="Wang_2021">A. Y.-T. Wang et al. (<a href="#ref-Wang_2021" role="doc-biblioref">2021</a>)</span>]. Material composition alone can be predictive of various material properties and is a widely chosen method to represent materials, depending on the task. When structural information is available, <span data-acronym-label="cif" data-acronym-form="plural+short">cifs</span>, initially proposed as a standard way to archive structural data in crystallography [<span class="citation" data-cites="hall1991crystallographic">Hall, Allen, and Brown (<a href="#ref-hall1991crystallographic" role="doc-biblioref">1991</a>)</span>], is now a widely used representation. [<span class="citation" data-cites="gruver2024finetuned">Gruver et al. (<a href="#ref-gruver2024finetuned" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="antunes2024crystal">Antunes, Butler, and Grau-Crespo (<a href="#ref-antunes2024crystal" role="doc-biblioref">2024</a>)</span>] proposed a condensed version of <span data-acronym-label="cif" data-acronym-form="plural+short">cifs</span>, which includes only the parameters necessary for building the crystal structure in a crystal generation application. <span class="citation" data-cites="ganose2019robocrystallographer">Ganose and Jain (<a href="#ref-ganose2019robocrystallographer" role="doc-biblioref">2019</a>)</span> aimed to create human-readable descriptions by proposing a tool to generate natural-language descriptions of crystal structures automatically. For specific material classes, such as <span data-acronym-label="mof" data-acronym-form="plural+short">mofs</span>, specialized representations like [<span class="citation" data-cites="Bucior_2019">Bucior et al. (<a href="#ref-Bucior_2019" role="doc-biblioref">2019</a>)</span>] have been developed.</p>
<p>Instead of a string, we can represent chemical substances as graphs. Here, we are directly encoding atoms (nodes) and bonds (edges). This representation introduces a much stronger inductive bias: locality biases that explicitly inform the model about atomic connectivity, so the model does not need to learn this fundamental principle from scratch. Symmetry has been incorporated into many of the best-performing graph-based approaches by designing invariant or equivariant representations [<span class="citation" data-cites="Langer_2022">Langer, Goeßmann, and Rupp (<a href="#ref-Langer_2022" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="Musil_2021">Musil et al. (<a href="#ref-Musil_2021" role="doc-biblioref">2021</a>)</span>] and architectures [<span class="citation" data-cites="satorras2021n">Satorras, Hoogeboom, and Welling (<a href="#ref-satorras2021n" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="Batzner_2022">Batzner et al. (<a href="#ref-Batzner_2022" role="doc-biblioref">2022</a>)</span>]. These approaches are efficient in capturing strong symmetry-related inductive biases along with the topology of locality biases.</p>
<p>The optimal choice of representation depends on the specific application. For applications where precise 3D structure matters, such as protein-ligand docking, geometric representations become essential. For <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>, a more natural choice would be text, to take advantage of the better overlap with the pre-training corpus and also to interact with humans. Ultimately, weaker inductive biases (like text) offer greater flexibility and can capture unexpected patterns, but may require more data to learn the fundamental rules. The successful design of inductive biases requires balancing domain knowledge with learning flexibility. Stricter inductive biases (like graphs) incorporate more domain knowledge, leading to greater data efficiency but potentially limiting the model’s ability to discover patterns that contradict our initial assumptions.</p>
<p>Beyond choosing a single optimal representation, modern <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> allows for the simultaneous use of multiple representations. A chemical entity can be described not only by its textual <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> string or its connectivity graph, but also by its experimental spectra (e.g., <span data-acronym-label="nmr" data-acronym-form="singular+short">nmr</span>, <span data-acronym-label="ir" data-acronym-form="singular+short">ir</span>), or even a microscopy image. Each of these modalities provides a complementary layer of information. A more detailed section on using multiple representations is presented in <a href="#sec:multimodal_chem" data-reference-type="ref+Label" data-reference="sec:multimodal_chem">1.9.1</a></p>
</section>
<section id="tokenization" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="tokenization"><span class="header-section-number">5.2.2</span> Tokenization</h3>
<p>Once we have chosen a representation format—whether <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> strings, <span data-acronym-label="cif" data-acronym-form="singular+short">cif</span> files, or chemical formulas—we face another fundamental question: How does a model process these variable-length sequences of characters? One might imagine creating a unique identifier or encoding for every single molecule or string. It is impractical to have a dictionary entry for every sentence in a language due to the similar scaling problems of <span data-acronym-label="ohe" data-acronym-form="singular+short">ohe</span>.</p>
<p>Consider the molecule with the <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> string <code>CN1C=NC=C1C(=O)</code>. We could break down the representation in several different ways: as individual characters (<code>C</code>, <code>N</code>, <code>1</code>, <code>C</code>, <code>=</code>, etc.), as atom-bond pairs (<code>CN</code>, <code>C=</code>, <code>NC</code>), or as chemically meaningful fragments (<code>CN1</code>, <code>C=NC</code>, etc.). Each choice creates a different “language” for the model to learn, with distinct computational and learning implications.</p>
<p>This is where tokenization becomes essential. It is the strategy of breaking down a complex representation (like a <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> string) into a sequence of discrete, manageable units called tokens. The core idea is to find a set of common, reusable building blocks. Instead of learning about countless individual molecules, the model knows about a much smaller, finite vocabulary of these tokens. By learning an encoding for each token, the model gains the ability to understand and construct representations for an immense number of molecules—including those it has never seen before—by combining the meanings of their constituent parts. This compositional approach enables powerful generalization.</p>
<p>The concept of tokenization, or defining the fundamental units of input, extends beyond string-based representations. In images, it could be patches of images. In graph-based models, the analogous decision is how to define the features for each node (atom) and edge (bond). Should a node simply represent an atomic number (a simple “token”), or should it be a more complex sub-structure like a structural motif[<span class="citation" data-cites="bouritsas2022improving">Bouritsas et al. (<a href="#ref-bouritsas2022improving" role="doc-biblioref">2022</a>)</span>] (a richer “token”)? This choice determines the level of chemical knowledge initially provided to the model. Ultimately, the tokenization strategy defines the elementary units for which the model will learn embeddings, setting the stage for learning the powerful and context-aware representations discussed next.</p>
</section>
<section id="sec:embeddings" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="sec:embeddings"><span class="header-section-number">5.2.3</span> Embeddings</h3>
<p>Through training, models can learn to map discrete inputs into continuous spaces where similar items have meaningful relationships (for example, similar items cluster in this continuous space). In the simplest approach, they can be created by training models (so-called models) that take one-hot encoded inputs and predict the probability of words in the context.[<span class="citation" data-cites="mikolov2013efficient">Mikolov, Chen, et al. (<a href="#ref-mikolov2013efficient" role="doc-biblioref">2013</a>)</span>; <span class="citation" data-cites="mikolov2013distributed">Mikolov, Sutskever, et al. (<a href="#ref-mikolov2013distributed" role="doc-biblioref">2013</a>)</span>; <span class="citation" data-cites="Tshitoyan_2019">Tshitoyan et al. (<a href="#ref-Tshitoyan_2019" role="doc-biblioref">2019</a>)</span>] Embeddings are powerful because they learn relationships between entities, allowing for the efficient compression of data and the uncovering of hidden patterns that would otherwise be invisible in the raw data.</p>
<p>The advent of <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> has further underscored the usefulness of high-quality embeddings. These models, trained on vast amounts of chemical data, learn to create powerful, generalizable embeddings that can be adapted to a wide range of downstream tasks, from property prediction (see <a href="05-applications.html#sec:prediction" data-reference-type="ref+Label" data-reference="sec:prediction">[sec:prediction]</a>) to molecular generation (see <a href="05-applications.html#sec:mol_generation" data-reference-type="ref+Label" data-reference="sec:mol_generation">[sec:mol_generation]</a>). The choice of embedding strategy often depends on the specific problem at hand. In the following sections, we describe the process of generating, refining, and using these embeddings through training and different architectures.</p>
</section>
</section>
<section id="general-training-workflow" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="general-training-workflow"><span class="header-section-number">5.3</span> General Training Workflow</h2>
<p>The entire training process of a <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> typically contains multiple steps that can be divided into two broad groups (see <a href="#fig:training_workflow" data-reference-type="ref+Label" data-reference="fig:training_workflow">1</a>). [<span class="citation" data-cites="howard2018universal">Howard and Ruder (<a href="#ref-howard2018universal" role="doc-biblioref">2018</a>)</span>] The first step is pre-training, which is usually done in a self-supervised manner and focuses on learning a data distribution—the underlying set of rules and patterns that make up the data. Imagine all possible arrangements of atoms, both real and unfeasible. The data distribution describes which molecules are “likely” (stable, following chemical rules) and which are “unlikely” or “impossible” (random assortments of atoms).</p>
<figure id="fig:training_workflow" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure4.png" width="100%" class="figure-img">
<figcaption>
<strong>General training workflow through the lens of molecular science</strong>. The figure illustrates the progression from pre-training through fine-tuning to post-training stages. <strong>(1) Pre-training:</strong> The model learns the underlying data distribution from a vast, unlabeled dataset. This is visualized as transforming an unstructured representation space (left, square cloud) into a structured manifold (the Swiss roll). At this stage, the model has learned the “shape” of the data: the fundamental rules that make a molecule chemically valid. However, the representations are not yet specialized for any task. <strong>(2) Fine-tuning:</strong> The model is trained on specific, labeled tasks, such as predicting solubility (flask icon) and toxicity (skull icon). This process “colors” the manifold, adjusting the learned representations so that their position now also correlates with specific properties (e.g., blue for one property profile, red for another). <strong>(3) Post-training Alignment:</strong> The model’s behavior is biased towards desired outcomes. This is visualized as preferentially sampling from a specific region of the colored manifold, such as generating molecules predicted to have high solubility and low toxicity (right, the brighter red region).
</figcaption>
</figure>
<p>Pre-training a model is teaching it to recognize this pattern. By observing millions of valid examples, the model learns the “grammar” of chemistry—the principles that make a molecule physically plausible. A model that has successfully learned the distribution can distinguish a valid structure from noise and can even generate new, chemically sensible examples, much like someone who has learned the rules of a language can form new, grammatically correct sentences.</p>
<p>A model does not learn the data distribution by storing an explicit formula. Instead, during pre-training (see <a href="#sec:pretraining" data-reference-type="ref+Label" data-reference="sec:pretraining">1.4</a> for more details), it learns to create an internal representation—an embedding (see <a href="#sec:embeddings" data-reference-type="ref+Label" data-reference="sec:embeddings">1.2.3</a>). The training process guides the model to map inputs to these embeddings in a structured manner, forming a high-dimensional space, where representations of similar, valid inputs are clustered together.</p>
<p>The second step is post-training, also called fine-tuning, in which the model is adapted to learn task-specific labels and capabilities, essentially “coloring” the learned structure with domain-specific knowledge. Crucially, fine-tuning does not discard the learned distribution but refines it. As shown in <a href="#fig:training_workflow" data-reference-type="ref+Label" data-reference="fig:training_workflow">1</a>, the fundamental shape of the manifold (the Swiss roll) is preserved. The “coloring” process corresponds to adjusting the internal representations so they now also encode task-specific properties. For example, the model learns to map molecules with high solubility to one region of the manifold (e.g., the red area) and those with high toxicity to another. The representation of each molecule is thus enriched, now containing information not just about its structural validity but also about its properties.</p>
<p>Finally, techniques such as <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> are used to align the model’s outputs with preferred choices. This step further refines the learned distribution by biasing the model’s sampling behavior to favor specific modes of the distribution. In terms of the representation space, the model learns to prioritize generating or paying attention to points in desirable regions. As depicted in the post-training panel of <a href="#fig:training_workflow" data-reference-type="ref+Label" data-reference="fig:training_workflow">1</a>, this biases the output towards a specific section of the colored manifold—in this case, perhaps molecules with high solubility (the brighter pink region).</p>
</section>
<section id="sec:pretraining" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="sec:pretraining"><span class="header-section-number">5.4</span> Pre-training: Learning the Shape of Data</h2>
<p>Pre-training establishes the foundational knowledge and capabilities of the model. During pre-training, the model learns general patterns, relationships, and structures from massive datasets (often trillions of tokens, see <a href="#fig:scale_of_data" data-reference-type="ref+Label" data-reference="fig:scale_of_data">[fig:scale_of_data]</a>). The model learns to map input to internal representations or features through so-called <span data-acronym-label="ssl" data-acronym-form="singular+short">ssl</span> objectives like reconstructing corrupted inputs (predicting masked tokens, or predicting future sequences, see <a href="#sec:ssl" data-reference-type="ref+Label" data-reference="sec:ssl">1.4.1</a>).</p>
<p>This large-scale pre-training allows models to capture rich representations of the statistical distributions inherent to the data. These learned distributions capture the fundamental patterns and structure of the domain (scientific language grammar, physical and chemical principles that govern materials). <a href="#fig:training_workflow" data-reference-type="ref+Label" data-reference="fig:training_workflow">1</a> illustrates the distribution captured, from an uninstructed manifold prior to pre-training (if you randomly pick from this manifold, you get noise or non-physical molecules) to a structured manifold, where if you sample from this distribution (the black Swiss roll) you get a valid molecule. For example, the model might learn commonly occurring structures, scientific notations, and scientific terms. Furthermore, it might construct hierarchical relationships between these concepts, such as those between chemical compounds, elements, and their properties. This distributional learning empowers the model to make predictions about new examples by understanding their relation to the learned patterns. Crucially, this ability stems from the development of transferable features, rather than mere data memorization [<span class="citation" data-cites="brown2020language">Brown et al. (<a href="#ref-brown2020language" role="doc-biblioref">2020</a>)</span>].</p>
<p>As illustrated by the Swiss roll in <a href="#fig:training_workflow" data-reference-type="ref+Label" data-reference="fig:training_workflow">1</a>, the pre-training process creates a structured manifold where invalid inputs are mapped far away. Therefore, learning high-quality representations is the concrete computational method for capturing the abstract statistical distribution of the data; the structure of this representation space is the model’s learned approximation of the data’s true shape.</p>
<section id="sec:ssl" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="sec:ssl"><span class="header-section-number">5.4.1</span> Self-Supervision</h3>
<p><span data-acronym-label="ssl" data-acronym-form="plural+short">ssls</span> allows models to learn from unlabeled data by generating “pseudo-labels” from the data’s structure. The original, unlabeled data serves as its own “ground truth”. This differs significantly from supervised learning, a traditional method where models are trained using labeled datasets. In supervised learning, each piece of data is explicitly tagged with the correct output, which the model then learns to predict. Such manual labeling is often an expensive, time-consuming, and domain-specific process. <span data-acronym-label="ssl" data-acronym-form="plural+short">ssls</span> has emerged as a particularly effective strategy for pre-training <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, since natural-language corpora are abundant but rarely annotated. Proxy strategies have then been applied to other types of model architectures as well. The ability to extract structure from data <em>without labels</em> is a key enabler for foundation models and underpins the pre-training phase.</p>
</section>
<section id="families-of-self-supervised-learning" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="families-of-self-supervised-learning"><span class="header-section-number">5.4.2</span> Families of Self-Supervised Learning</h3>
<p><span data-acronym-label="ssl" data-acronym-form="singular+short">ssl</span> encompasses a variety of approaches. While distinct methods exist, they can be grouped into broader families based on their underlying principles. <a href="#fig:types_ssl" data-reference-type="ref+Label" data-reference="fig:types_ssl">2</a> illustrates the two main families: generative and contrastive, along with example pretext tasks for each.</p>
<figure id="fig:types_ssl" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure5.png" width="100%" class="figure-img">
<figcaption>
<strong>Main families in <span data-acronym-label="ssl" data-acronym-form="plural+short">ssls</span></strong>. The figure illustrates the two primary approaches to <span data-acronym-label="ssl" data-acronym-form="singular+short">ssl</span>, each using different strategies to generate pseudo-labels from the data itself. <strong>Generative Methods (Top Panel):</strong> This family focuses on reconstruction and prediction. The model learns representations by generating missing information. Examples shown correspond to the pretext tasks discussed in the text: (1) <em>Predicting masks</em> in a graph, analogous to masked modeling (more details in <a href="#sec:masked_modeling" data-reference-type="ref+Label" data-reference="sec:masked_modeling">[sec:masked_modeling]</a>); (2) <em>Learning from context</em>, which is the basis for next token prediction (more details in <a href="#sec:next_token_prediction" data-reference-type="ref+Label" data-reference="sec:next_token_prediction">1.4.3.2</a>); and (3) <em>Learning to denoise</em>, where the model reconstructs a clean input from a corrupted version. (see <a href="#sec:denoising" data-reference-type="ref+Label" data-reference="sec:denoising">1.4.3.3</a>) <strong>Contrastive Learning (Bottom Panel):</strong> This family learns by comparing samples. The model is trained to pull representations of similar samples together while pushing dissimilar ones apart. Examples include: (1) <em>Aligning embeddings</em> from different augmentations of the same molecule, a core idea in Instance Discrimination (more details in <a href="#sec:instance_discrimination" data-reference-type="ref+Label" data-reference="sec:instance_discrimination">1.4.4.1</a>); (2) <em>Learning to cluster</em> similar molecules together, as in Clustering-based Contrastive Learning (see <a href="#sec:clustering_cl" data-reference-type="ref+Label" data-reference="sec:clustering_cl">1.4.4.2</a>); and (3) <em>Cross-modal alignment</em>, where representations from different data types (e.g., a molecule’s graph and its spectral properties) are learned jointly. (see <a href="#sec:contrastive_learning" data-reference-type="ref+Label" data-reference="sec:contrastive_learning">1.4.4</a> )
</figcaption>
</figure>
</section>
<section id="generative-methods" class="level3" data-number="5.4.3">
<h3 data-number="5.4.3" class="anchored" data-anchor-id="generative-methods"><span class="header-section-number">5.4.3</span> Generative Methods</h3>
<p>This family of methods focuses on learning representations by reconstructing or predicting parts of the input data from other observed parts. The model learns the underlying data distribution by learning to re-generate the missing information. Examples shown in <a href="#fig:types_ssl" data-reference-type="ref+Label" data-reference="fig:types_ssl">2</a> include predicting masked portions of a graph, learning from surrounding text context, and learning to denoise an image.</p>
<p><span id="sec:masked_modeling" data-label="sec:masked_modeling"></span></p>
<section id="masked-modeling" class="level4" data-number="5.4.3.1">
<h4 data-number="5.4.3.1" class="anchored" data-anchor-id="masked-modeling"><span class="header-section-number">5.4.3.1</span> Masked Modeling</h4>
<p>In this method, portions of the input data are intentionally obscured or “masked”. The model’s primary objective is then to reconstruct these hidden segments accurately. [<span class="citation" data-cites="devlin2018bert0">Devlin et al. (<a href="#ref-devlin2018bert0" role="doc-biblioref">2018</a>)</span>] This process can be conceptualized as a “fill-in-the-blanks” task, compelling the model to infer missing information from its context. This enables the model to develop a deep understanding of contextual dependencies of data’s structure and semantics without requiring explicit human-labeled annotations. For chemical data, this could involve masking and predicting tokens in <span data-acronym-label="smiles" data-acronym-form="plural+short">smiless</span>/<span data-acronym-label="selfies" data-acronym-form="plural+short">selfiess</span> strings [<span class="citation" data-cites="chithrananda2020chemberta">Chithrananda, Grand, and Ramsundar (<a href="#ref-chithrananda2020chemberta" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="zhang2025scientific">Zhang et al. (<a href="#ref-zhang2025scientific" role="doc-biblioref">2025</a>)</span>] (i.e., hiding atoms and training the model to guess what is missing), omitting atom or bond types in molecular graphs [<span class="citation" data-cites="mahmood2021masked">Mahmood et al. (<a href="#ref-mahmood2021masked" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="wang2022molecular">Yuyang Wang et al. (<a href="#ref-wang2022molecular" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="reiser2022graph">Reiser et al. (<a href="#ref-reiser2022graph" role="doc-biblioref">2022</a>)</span>], removing atomic coordinates in 3D structures, or masking sites within a crystal lattice.</p>
</section>
<section id="sec:next_token_prediction" class="level4" data-number="5.4.3.2">
<h4 data-number="5.4.3.2" class="anchored" data-anchor-id="sec:next_token_prediction"><span class="header-section-number">5.4.3.2</span> Next Token Prediction</h4>
<p>Many forms of data, such as text, can be represented as sequences of tokens. One of the most powerful <span data-acronym-label="ssl" data-acronym-form="singular+short">ssl</span> tasks for such sequential data is next-token prediction. Here, the core objective is for a model to anticipate and generate the subsequent token in a given sequence, based on the contextual information provided by preceding tokens. Because text unfolds naturally in a sequence, it offers the reference information the model needs in order to learn. This approach has been applied to chemical and material representations by treating molecular string representations (<span data-acronym-label="smiles" data-acronym-form="plural+short">smiless</span>, <span data-acronym-label="selfies" data-acronym-form="plural+short">selfiess</span>, etc.) or material representations as sequences [<span class="citation" data-cites="adilov2021generative">Adilov (<a href="#ref-adilov2021generative" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="wang2023cmolgpt">Ye Wang et al. (<a href="#ref-wang2023cmolgpt" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="schwaller2019molecular">Schwaller et al. (<a href="#ref-schwaller2019molecular" role="doc-biblioref">2019</a>)</span>; <span class="citation" data-cites="alampara2024mattext">Alampara, Miret, and Jablonka (<a href="#ref-alampara2024mattext" role="doc-biblioref">2024</a>)</span>]. During training, the model constantly adjusts itself to maximize the likelihood (trying to make good predictions more probable and bad predictions less probable). In this context, likelihood refers to the probability of observing the actual subsequent token given the preceding tokens in the sequence . This is accomplished by making each prediction based on the preceding input, which establishes the conditional context (see <a href="#eq:nexttoken" data-reference-type="ref+Label" data-reference="eq:nexttoken">[eq:nexttoken]</a>).</p>
<div class="tcolorbox">
<p><span id="eq:nexttoken" data-label="eq:nexttoken"></span></p>
<div class="center">
<div class="minipage">
<p><span class="math display">\[\mathcal{L} =
-\mathbb{E} \left[ \sum_{t=1}^{T}
\log P(\eqnmarkbox[PositiveColor]{pred}{x_t} | \eqnmarkbox[NegativeColor]{context}{x_{&lt;t}})
\right]
\label{eq:cross_entropy}\]</span></p>
</div>
</div>
<ul>
<li><p><strong>Prediction Term</strong> (Blue): The target token <span class="math inline">\(x_t\)</span> that the model is trying to predict at each position.</p></li>
<li><p><strong>Context Tokens</strong> (Maroon): The set of tokens <span class="math inline">\(x_{\text{context}}\)</span> the model uses to make its prediction. The definition of this context depends on the SSL task:</p>
<ul>
<li><p><em>For Masked Modeling:</em> The context is all unmasked tokens in the sequence.</p></li>
<li><p><em>For Next-Token Prediction:</em> The context is the preceding tokens (<span class="math inline">\(x_{&lt;t}\)</span>).</p></li>
</ul></li>
<li><p><strong>Summation</strong> <span class="math inline">\(\sum_{t=1}^{T}\)</span>: The loss is calculated across all token positions in the sequence of length <span class="math inline">\(T\)</span>.</p></li>
<li><p><strong>The Logarithm’s Role</strong>: The negative logarithm (<span class="math inline">\(\log P\)</span>) heavily penalizes highly confident wrong answers (low <span class="math inline">\(P\)</span>, high loss) and lightly rewards confident correct answers (high <span class="math inline">\(P\)</span>, low loss).</p></li>
<li><p><strong>Overall Loss Structure</strong>: Cross-entropy loss that encourages the model to assign high probability to the correct next token at each position, given all previous tokens.</p></li>
</ul>
</div>
</section>
<section id="sec:denoising" class="level4" data-number="5.4.3.3">
<h4 data-number="5.4.3.3" class="anchored" data-anchor-id="sec:denoising"><span class="header-section-number">5.4.3.3</span> Denoising</h4>
<p>Denoising <span data-acronym-label="ssl" data-acronym-form="plural+short">ssls</span> works by intentionally adding noise to the inputs and then training models to reconstruct the original data. In this context, the original, uncorrupted data implicitly serves as the label or target for the training process. In this paradigm, we begin with a clean input, which we can call <span class="math inline">\(x\)</span>. We then apply a random corruption process to create a noisy version, <span class="math inline">\(\tilde{x}\)</span>. The model is then trained to reverse this damage and recover the original, clean <span class="math inline">\(x\)</span> from the corrupted <span class="math inline">\(\tilde{x}\)</span>. This process is formally expressed as sampling a corrupted input <span class="math inline">\(\tilde{x}\sim q(\tilde{x}|x)\)</span> and optimizing the network to predict <span class="math inline">\(x\)</span>. [<span class="citation" data-cites="vincent2010stacked">Vincent et al. (<a href="#ref-vincent2010stacked" role="doc-biblioref">2010</a>)</span>] By learning to recover the clean input, the model is compelled to develop robust representations that are inherently invariant to the types of noise it encounters during training. This directly forces the model to learn the underlying data distribution. To distinguish the original signal from the artificial noise, the model must learn the features of high-probability samples within that distribution. For example, to successfully “denoise” a molecule, it must implicitly understand the rules of chemical plausibility—the very patterns that separate valid structures from random noise. While popular in images [<span class="citation" data-cites="vincent2008extracting">Vincent et al. (<a href="#ref-vincent2008extracting" role="doc-biblioref">2008</a>)</span>; <span class="citation" data-cites="bengio2013generalized">Bengio et al. (<a href="#ref-bengio2013generalized" role="doc-biblioref">2013</a>)</span>], denoising objectives have also been applied to graph representations of molecules [<span class="citation" data-cites="wang2023denoise">Yuyang Wang et al. (<a href="#ref-wang2023denoise" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="ni2024pre">Ni et al. (<a href="#ref-ni2024pre" role="doc-biblioref">2024</a>)</span>]. For instance, one can randomly perturb atoms or edges in a molecular graph and train a graph neural network to predict the original attributes.</p>
</section>
</section>
<section id="sec:contrastive_learning" class="level3" data-number="5.4.4">
<h3 data-number="5.4.4" class="anchored" data-anchor-id="sec:contrastive_learning"><span class="header-section-number">5.4.4</span> Contrastive Learning</h3>
<p>The other main family of <span data-acronym-label="ssl" data-acronym-form="singular+short">ssl</span> techniques is contrastive learning. The objective is to train models to understand data by distinguishing between similar and dissimilar samples. This is achieved by learning an embedding space where representations of samples that are alike in their core chemical properties or identity are pulled closer together. In contrast, representations of samples that are fundamentally different are pushed further apart. [<span class="citation" data-cites="hadsell2006dimensionality">Hadsell, Chopra, and LeCun (<a href="#ref-hadsell2006dimensionality" role="doc-biblioref">2006</a>)</span>]</p>
<p>This process creates meaningful clusters for related concepts while enforcing separation between unrelated ones. In effect, the model learns the data’s underlying distribution by defining the distance between its points. The resulting internal representations become highly robust because they are trained for invariance; the model learns to focus on essential, identity-defining features while disregarding irrelevant variations. This process, often referred to as embedding alignment, ensures that the representations capture the core characteristics shared among similar samples.</p>
<p>There are many contrastive learning approaches with variations in loss functions. A key design choice in contrastive learning is whether to compute the contrastive loss on an instance basis or a cluster basis.</p>
<section id="sec:instance_discrimination" class="level4" data-number="5.4.4.1">
<h4 data-number="5.4.4.1" class="anchored" data-anchor-id="sec:instance_discrimination"><span class="header-section-number">5.4.4.1</span> Instance Discrimination</h4>
<p>Instance Discrimination is arguably the most dominant paradigm in recent contrastive learning. Each instance (sample) in the dataset is treated as its own distinct class. This is typically achieved using contrastive loss functions like see <a href="#eq:infonce" data-reference-type="ref+Label" data-reference="eq:infonce">[eq:infonce]</a>. [<span class="citation" data-cites="oord2018representation">Oord, Li, and Vinyals (<a href="#ref-oord2018representation" role="doc-biblioref">2018</a>)</span>] As detailed in <a href="#eq:infonce" data-reference-type="ref+Label" data-reference="eq:infonce">[eq:infonce]</a>, the loss function is formulated as a categorical cross-entropy loss where the task is to classify the positive sample correctly among a set of negatives plus the positive itself.</p>
<p>In materials and chemistry, this can involve aligning the textual representation of a structure with a graphical representation, image, or other visual method to represent a molecule. The model could also learn from augmentations of a structure, such as being given several valid <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> strings that all describe the identical molecule. Furthermore, this approach can involve contrasting variations of a crystal structure against entirely different molecules or materials, enabling the model to grasp the subtle similarities and stark differences between them.</p>
<div class="tcolorbox">
<div class="center">
<div class="minipage">
<p><span class="math display">\[\label{eq:infonce}
%\mathcal{L}_{\text{InfoNCE}}
\hspace*{-2em}
\mathcal{L} =
%\hspace*{-3em}
-\mathbb{E} \left[ \log \frac{
\eqnmarkbox[PositiveColor]{pos}{\exp({\text{sim}(f(\mathbf{x}_i), f(\mathbf{x}_i^+))/\tikzmarknode{tau1}{\tau}})}
}{
\eqnmarkbox[PositiveColor]{pos2}{\exp({\text{sim}(f(\mathbf{x}_i), f(\mathbf{x}_i^+))/\tau})} +
\eqnmarkbox[NegativeColor]{neg}{\sum_{j=1}^{N} \exp({\text{sim}(f(\mathbf{x}_i), f(\mathbf{x}_j^-))/\tikzmarknode{tau2}{\tau}})}
} \right]\]</span></p>
</div>
</div>
<ul>
<li><p><strong>Positive Pair Term</strong> (Blue): Measures similarity between an anchor sample <span class="math inline">\(\mathbf{x}_i\)</span> and its positive pair <span class="math inline">\(\mathbf{x}_i^+\)</span> (e.g., different view of the same molecule).</p></li>
<li><p><strong>Negative Pairs Term</strong> (Maroon): Sum of similarities between anchor sample <span class="math inline">\(\mathbf{x}_i\)</span> and all negative pairs <span class="math inline">\(\mathbf{x}_j^-\)</span> (e.g., different molecules).</p></li>
<li><p><strong>Temperature Parameter</strong> <span class="math inline">\(\boldsymbol{\tau}\)</span> : Controls the sharpness of the distribution. Lower values make the model more sensitive to hard negatives.</p></li>
<li><p><strong>Overall Loss Structure</strong> : A negative log probability that encourages the model to maximize similarity for positive pairs while minimizing it for negative pairs.</p></li>
</ul>
</div>
</section>
<section id="sec:clustering_cl" class="level4" data-number="5.4.4.2">
<h4 data-number="5.4.4.2" class="anchored" data-anchor-id="sec:clustering_cl"><span class="header-section-number">5.4.4.2</span> Clustering-based Contrastive Learning</h4>
<p>Clustering approaches leverage the idea that similarity often translates to closeness in the feature space. Methods like [<span class="citation" data-cites="caron2018deep">Caron et al. (<a href="#ref-caron2018deep" role="doc-biblioref">2018</a>)</span>] iteratively train a model. First, they group the generated features (internal representation) of a dataset into distinct sets using a common grouping algorithm, such as <span class="math inline">\(k\)</span>-means clustering. Imagine you have a pile of diverse objects; <span class="math inline">\(k\)</span>-means would help you sort them into a predefined number of piles based on their similarities, like color or shape. These assigned groups then act as “pseudo-labels”—temporary, automatically generated labels—to train the network. The supervised training step implicitly contrasts samples from different clusters. The clustering and training steps alternate. Take a dataset of molecular fingerprints as an example. A model can be trained to predict the clustering pattern of this fingerprint data, distinguishing between conformer types or perturbed structures. Thus, the model learns representations that group chemically or structurally similar fingerprints.</p>
</section>
</section>
</section>
<section id="the-holy-grail-of-building-good-internal-representation" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="the-holy-grail-of-building-good-internal-representation"><span class="header-section-number">5.5</span> The Holy Grail of Building Good Internal Representation</h2>
<p>The design of effective pretext tasks—such as specific versions of instance discrimination (identifying unique examples) or denoising (recovering original data from corrupted versions)—is perhaps the holy grail. This is precisely where deep domain expertise becomes invaluable.</p>
<p>The pretext tasks must be meaningful, preserving the core identity of the molecule or material while introducing sufficient diversity to challenge the model and allow it to learn robust invariances.</p>
<p>For instance, a suboptimal technique would be to shuffle all the atoms in the text representation of a molecule. This would destroy the molecule’s chemical meaning, which would hinder the model’s ability to learn chemically meaningful features. Good augmentations typically enable richer features by providing additional layers of information to learn from, such as generating different low-energy conformers or using non-canonical string representations.</p>
<section id="parallels-between-generative-and-contrastive-objectives" class="level4" data-number="5.5.0.1">
<h4 data-number="5.5.0.1" class="anchored" data-anchor-id="parallels-between-generative-and-contrastive-objectives"><span class="header-section-number">5.5.0.1</span> Parallels between Generative and Contrastive Objectives</h4>
<p>While it might seem that generative and contrastive <span data-acronym-label="ssl" data-acronym-form="plural+short">ssls</span> methods optimize different things, their underlying goals can often be equivalent. A generative masked language model learns the conditional probability (see <a href="#eq:infonce" data-reference-type="ref+Label" data-reference="eq:infonce">[eq:infonce]</a>) , aiming to assign a high probability to the correct masked token by effectively discriminating it from other vocabulary tokens. The loss in contrastive learning can be viewed as a log-loss for a <span class="math inline">\((K+1)\)</span>-way classification task (see <a href="#eq:infonce" data-reference-type="ref+Label" data-reference="eq:infonce">[eq:infonce]</a>). Here, the model learns to identify the positive pair <span class="math inline">\(f(x_i^+)\)</span> as matching <span class="math inline">\(f(x_i)\)</span> from a set including <span class="math inline">\(f(x_i^+)\)</span> and <span class="math inline">\(K\)</span> negative features <span class="math inline">\(f(x_j^-)\)</span>. Both approaches effectively learn to select the “correct” item (a token or a positive feature) from a set of candidates based on the provided context or an anchor. To do so, they must effectively build strong internal representations.</p>
</section>
<section id="pre-training-beyond-ssl" class="level4" data-number="5.5.0.2">
<h4 data-number="5.5.0.2" class="anchored" data-anchor-id="pre-training-beyond-ssl"><span class="header-section-number">5.5.0.2</span> Pre-training beyond <span data-acronym-label="ssl" data-acronym-form="singular+short">ssl</span></h4>
<p>Pre-training cannot be performed using <span data-acronym-label="ssl" data-acronym-form="singular+short">ssl</span> on a single modality alone. For example, in models that consider multiple input formats (multimodality, as explained in detail in <a href="#sec:multimodal_chem" data-reference-type="ref+Label" data-reference="sec:multimodal_chem">1.9.1</a>), alignments between different modalities (e.g., text-image, text-graph) serve as a pre-training step.[<span class="citation" data-cites="weng2022vlm">Weng (<a href="#ref-weng2022vlm" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="girdhar2023imagebind0">Girdhar et al. (<a href="#ref-girdhar2023imagebind0" role="doc-biblioref">2023</a>)</span>] General-purpose force fields are commonly trained in a supervised manner on relaxation and simulation trajectories.[<span class="citation" data-cites="batatia2022mace">Batatia et al. (<a href="#ref-batatia2022mace" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="wood2025uma0">Wood et al. (<a href="#ref-wood2025uma0" role="doc-biblioref">2025</a>)</span>] Thus, the model learns a representation of connectivity patterns to energies. However, these representations also implicitly encode structural patterns (commonly observed coordination environments) and their correlations with each other and with abstract properties. A distinct and powerful pre-training paradigm moves away from real-world data entirely, instead training models like on millions of synthetically generated datasets to become general-purpose learning algorithms. This allows them to perform in-context learning on new, small datasets in a single forward pass, often outperforming traditional methods. [<span class="citation" data-cites="hollmann2025accurate">Hollmann et al. (<a href="#ref-hollmann2025accurate" role="doc-biblioref">2025</a>)</span>]<br>
The core principle remains: <em>learning on large datasets to build generalizable internal representations before task-specific fine-tuning.</em></p>
</section>
</section>
<section id="sec:fine_tuning_coloring" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="sec:fine_tuning_coloring"><span class="header-section-number">5.6</span> Fine-Tuning: Learning the Coloring of Data</h2>
<p>While pre-training enables models to learn general structural representations of chemical data, fine-tuning refines these representations for specific downstream tasks. If pre-training can be conceptualized as learning the “structure” of chemical knowledge, fine-tuning can be viewed as learning to “color” this structure with task-specific knowledge and capabilities (see <a href="#fig:training_workflow" data-reference-type="ref+Label" data-reference="fig:training_workflow">1</a>). This specialization process transforms general-purpose internal representations into powerful task-specific predictors while retaining the foundational knowledge acquired during pre-training.</p>
<p>Fine-tuning adapts pre-trained model parameters through continued training on domain-specific datasets. This typically requires substantially less data than pre-training. To make this process even more efficient, a common strategy is to “freeze” the majority of the model’s layers and only train a small subset of the final layers (see <a href="#sec:peft" data-reference-type="ref+Label" data-reference="sec:peft">1.10.3</a>). Fine-tuning is particularly valuable in chemistry, where datasets are often limited in size. Traditionally, addressing chemistry-specific problems required heavily engineered and specialized algorithms that directly incorporated chemical knowledge into model architectures. However, fine-tuned <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, for example, have shown comparable or superior performance to these specialized techniques, particularly when data is limited [<span class="citation" data-cites="jablonka2024leveraging">Jablonka et al. (<a href="#ref-jablonka2024leveraging" role="doc-biblioref">2024</a>)</span>]. The efficiency of fine-tuning stems from the transferability of chemical knowledge embedded during pre-training, where the model has already learned to spot patterns in molecular structure, reactivity, and chemical terminology sequences. With a large amount of data <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> compress a lot of such sequence relationships into its weights.</p>
</section>
<section id="sec:rl" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="sec:rl"><span class="header-section-number">5.7</span> Post-Supervised Adaptation: Learning to Align and Shape Behavior</h2>
<p>Pre-training and fine-tuning equip the model with a learned distribution, which represents its knowledge about what outputs are plausible or likely. Post-training does not erase this knowledge; instead, it biases this distribution towards preferred outcomes—such as task-specific goals. The new, desired behavior of the model (called the policy, <span class="math inline">\(\pi\)</span>, in <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span>) comes from this refined distribution. This shift has a subtle but crucial effect on the internal representations.</p>
<p>Post-training alignment workflows commonly use <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span>, as the classic loss-minimization approaches—simply fine-tuning on more “correct” examples—can struggle to capture more nuanced, hard-to-label objectives[<span class="citation" data-cites="Huan2025mathLLM">Huan et al. (<a href="#ref-Huan2025mathLLM" role="doc-biblioref">2025</a>)</span>]; when the goal is to steer the model toward more intangible qualities, formulating loss functions and collecting a pre-labeled dataset become very challenging. In <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span>-based alignment, the model is treated as an agent that takes actions (generates text in the case of an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>) in a trial-and-error environment and receives a reward signal based on the actions it chooses. The <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> objective is to maximize this reward by changing the model’s behavior. In the case of <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>, this means compelling it to generate text with the preferred properties. This process transforms the model into a goal-oriented one, where the goal can be to generate stable molecules, solve tasks step by step, or utilize tools, depending on the reward function.</p>
<p>During alignment, the foundational embeddings for basic concepts (e.g., a carbon atom) learned during pre-training remain largely intact. This initial state is critical; without a robust, pre-trained <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>, the <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> process would be forced to blindly explore an intractably vast space, making it highly unlikely to discover preferred sequences (that it could then reinforce).</p>
<p>The mapping from an input to its final representation is adjusted to become “reward-aware”. For example, the representation of a molecule might now encode not just its chemical structure, but also its potential to become a high-reward final molecule (stable and soluble molecule) [<span class="citation" data-cites="narayanan2025training">Narayanan et al. (<a href="#ref-narayanan2025training" role="doc-biblioref">2025</a>)</span>]. The representation space retains its overall shape (<a href="#fig:training_workflow" data-reference-type="ref+Label" data-reference="fig:training_workflow">1</a>), but the model learns a new way to navigate it, guided by the reward.</p>
<div class="tcolorbox">
<p><span id="eq:rl" data-label="eq:rl"></span></p>
<div class="center">
<div class="minipage">
<p><span class="math display">\[\label{eq:rl_objective}
\hspace*{-1em}
\pi(a|s) = \eqnmarkbox[PolicyColor]{policy}{P_{\text{LLM}}} \left( \eqnmarkbox[ActionColor]{action}{\text{next tokens}} \mid \eqnmarkbox[StateColor]{context}{\text{context}} \right)
\rightarrow \text{Maximize } \eqnmarkbox[RewardColor]{reward}{\mathbb{E}[R]}\]</span></p>
</div>
</div>
<ul>
<li><p><strong>State (<span class="math inline">\(s\)</span>)</strong> (Blue): The sequence of tokens generated so far, including the original prompt and any partial response. Represents the current context that the model uses to make decisions.</p></li>
<li><p><strong>Action (<span class="math inline">\(a\)</span>)</strong> (Maroon): The next token that the model chooses to generate from its vocabulary. This is the discrete decision the agent makes at each step.</p></li>
<li><p><strong>Policy (<span class="math inline">\(\pi\)</span> or <span class="math inline">\(P_{\text{LLM}}\)</span>)</strong> (Red): The <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> itself, whose parameters define the probability distribution over possible following tokens given the current state. This is what gets optimized during training.</p></li>
<li><p><strong>Reward (<span class="math inline">\(R\)</span>)</strong> (Peach): A numerical score assigned to complete generated sequences, measuring how well the output achieves the desired goal. Used only during training to guide parameter updates.</p></li>
<li><p><strong>Expectation (<span class="math inline">\(\mathbb{E}\)</span>)</strong> (Peach): The average cumulative reward, calculated over many possible sequences that could be generated by the current policy.</p></li>
<li><p><strong>Training Objective</strong>: Adjust the model’s parameters to maximize the expected cumulative reward (<span class="math inline">\(\mathbb{E}[R]\)</span>) over complete sequences, enabling the generation of high-quality outputs.</p></li>
</ul>
</div>
<section id="the-challenge-of-reward-design" class="level4" data-number="5.7.0.1">
<h4 data-number="5.7.0.1" class="anchored" data-anchor-id="the-challenge-of-reward-design"><span class="header-section-number">5.7.0.1</span> The Challenge of Reward Design</h4>
<p>A critical factor for the success of this framework is the design of the reward function. The training process is most stable and effective when rewards are <em>verifiable</em> and based on objective, computable metrics. In contrast, training with <em>sparse</em> rewards (where feedback is infrequent) or <em>fuzzy</em> signals (where the goal is subjective or ill-defined) makes the credit assignment problem significantly more difficult. This is a central challenge in aligning models with complex human preferences, as crafting precise reward functions that capture the full nuance of a desired behavior remains an active area of research [<span class="citation" data-cites="ouyang2022training">Ouyang et al. (<a href="#ref-ouyang2022training" role="doc-biblioref">2022</a>)</span>].</p>
</section>
<section id="the-llm-as-a-policy" class="level4" data-number="5.7.0.2">
<h4 data-number="5.7.0.2" class="anchored" data-anchor-id="the-llm-as-a-policy"><span class="header-section-number">5.7.0.2</span> The <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> as a Policy</h4>
<p>When using a <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> as the agent in <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span>, the policy (see <a href="#eq:rl_objective" data-reference-type="ref+Label" data-reference="eq:rl_objective">[eq:rl_objective]</a>) is the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> itself. Consider teaching a model to design multi-step synthetic routes for pharmaceutical compounds, using a retrosynthetic strategy. The <strong>state</strong> (<span class="math inline">\(s\)</span>) represents the synthetic plan generated so far. Initially, the state consists of just the target molecule but evolves to include each proposed step in the route. Each <strong>action</strong> (<span class="math inline">\(a\)</span>) is the next retrosynthetic decision—for example, which bonds to break or what reagents to use. The <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> serves as the policy (<span class="math inline">\(\pi\)</span>), using its parameters to determine the probability of choosing different possible actions given the current context. To put it mathematically, this would be <span class="math inline">\(\pi(a|s) = P_{\text{LLM}}(\text{next synthetic step}|\text{current plan})\)</span> (see <a href="#eq:rl_objective" data-reference-type="ref+Label" data-reference="eq:rl_objective">[eq:rl_objective]</a>). The model leverages its chemical knowledge to identify the most promising decisions. The <strong>reward</strong> (<span class="math inline">\(R\)</span>) scores the completed retrosynthetic route based on practical criteria that could be the number of steps, predicted yield, reagent cost, etc. This score can directly come from the feedback of real chemists (<span data-acronym-label="rlhf" data-acronym-form="singular+short">rlhf</span>), or from a small model trained to predict human preference scores or pre-defined criteria.</p>
<p>Theoretical work in reinforcement learning has shown that the complexity of such problems scales quadratically with the size of the action space [<span class="citation" data-cites="dann2015sample">Dann and Brunskill (<a href="#ref-dann2015sample" role="doc-biblioref">2015</a>)</span>]. At each step, the model must choose from tens of thousands of possible tokens, and the number of possible sequences (and therefore actions) grows exponentially. Without pre-training, this would make the learning process computationally prohibitive. Pre-training provides a strong initialization that effectively constrains the action space to reasonable chemical language and valid synthetic steps, dramatically reducing the exploration requirements (see how pre-training creates a structured manifold in <a href="#fig:training_workflow" data-reference-type="ref+Label" data-reference="fig:training_workflow">1</a>).</p>
<p>Recent developments have revealed that <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> training can elicit reasoning capabilities that were previously thought to require explicit programming or extensive domain-specific architectures. Models trained with <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> demonstrate the ability to decompose complex problems, perform backtracking when approaches fail, and engage in multi-step planning without being explicitly taught these strategies. [<span class="citation" data-cites="xu2025towards">Xu et al. (<a href="#ref-xu2025towards" role="doc-biblioref">2025</a>)</span>]</p>
</section>
<section id="updating-the-llm-policy" class="level4" data-number="5.7.0.3">
<h4 data-number="5.7.0.3" class="anchored" data-anchor-id="updating-the-llm-policy"><span class="header-section-number">5.7.0.3</span> Updating the LLM Policy</h4>
<p>After the model takes actions (generates a sequence of tokens), the reward it receives for the chosen actions is used to update the <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> parameters using an <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> algorithm, such as <span data-acronym-label="ppo" data-acronym-form="singular+short">ppo</span> [<span class="citation" data-cites="schulman2017proximal">Schulman et al. (<a href="#ref-schulman2017proximal" role="doc-biblioref">2017</a>)</span>]. <span data-acronym-label="ppo" data-acronym-form="singular+short">ppo</span> works by encouraging the model to favor actions (outputs) that lead to higher rewards, but it also includes a mechanism to constrain how much the model’s behavior can change in a single update. Specifically, it introduces a penalty term that discourages the <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> policy from deviating too far from its original, pre-trained distribution. This ensures the model does not “forget” its foundational knowledge about language or chemistry while it is learning to pursue the reward, thus biasing the distribution rather than completely overwriting it. The result is a controlled shift: the model becomes more aligned without losing what it already knows.</p>
</section>
<section id="inference-and-sampling-from-the-adapted-model" class="level4" data-number="5.7.0.4">
<h4 data-number="5.7.0.4" class="anchored" data-anchor-id="inference-and-sampling-from-the-adapted-model"><span class="header-section-number">5.7.0.4</span> Inference and Sampling from the Adapted Model</h4>
<p>The <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> training process permanently updates the weights of the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>. When we sample from this model, we are drawing from this new, biased distribution. For a given context (state), the probabilities for tokens (actions) that were historically part of high-reward sequences are now intrinsically higher. At the same time, pathways that led to low rewards are suppressed. The model is now inherently more likely to generate outputs that align with the preferences and goals encoded in the reward function.</p>
</section>
</section>
<section id="sec:example_architectures" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="sec:example_architectures"><span class="header-section-number">5.8</span> Example Architectures</h2>
<p>While much effort is currently invested in building foundation models based on transformer-based <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, the foundation model paradigm is not limited to this model class.</p>
<p>In the chemical domain, where heterogeneous data such as <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> and graphs for molecular structures prevail, the use of a diverse array of architectures is expected. The architectures shown in <a href="#fig:architectures" data-reference-type="ref+Label" data-reference="fig:architectures">3</a> are examples of foundational backbones that we discuss in the following sections.</p>
<figure id="fig:architectures" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure6.png" width="100%" class="figure-img">
<figcaption>
<strong>Blueprint for <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> architectures</strong>. This diagram illustrates four distinct neural network architectures (<span data-acronym-label="lstm" data-acronym-form="singular+short">lstm</span>, Transformer, Mamba, and <span data-acronym-label="gnn" data-acronym-form="singular+short">gnn</span>), highlighting their unique approaches to input representation, information processing, and output pooling. <span data-acronym-label="lstm" data-acronym-form="plural+short">lstms</span> sequentially process tokens, accumulating information in a final hidden state with an inductive bias towards order. Transformers, conversely, use multi-head attention and positional encodings to capture global interactions simultaneously, offering minimal inductive bias but enabling rich contextual understanding. Mamba combines local convolutional processing with selective state space modeling to efficiently focus on chemically relevant parts, also typically using a final hidden state. <span data-acronym-label="gnn" data-acronym-form="plural+short">gnns</span> leverage the inherent graph structure of molecules, where atoms are nodes and bonds are edges, to model local chemical environments through message passing, followed by graph-level pooling to create a unified representation. Each approach offers unique strengths in how it captures molecular features, ranging from sequential to global, and graph-based relationships.
</figcaption>
</figure>
<section id="lstm" class="level4" data-number="5.8.0.1">
<h4 data-number="5.8.0.1" class="anchored" data-anchor-id="lstm"><span class="header-section-number">5.8.0.1</span> <span data-acronym-label="lstm" data-acronym-form="singular+short">lstm</span></h4>
<p><span data-acronym-label="lstm" data-acronym-form="singular+short">lstm</span> networks [<span class="citation" data-cites="hochreiter1997long">Hochreiter and Schmidhuber (<a href="#ref-hochreiter1997long" role="doc-biblioref">1997</a>)</span>] are well-suited for processing sequential data, such as text or time series. <a href="#fig:architectures" data-reference-type="ref+Label" data-reference="fig:architectures">3</a> illustrates how chemical information is processed to predict <span data-acronym-label="lstm" data-acronym-form="plural+short">lstms</span>.</p>
<ul>
<li><p><strong>Input</strong>: Molecules are represented as tokenized sequences (e.g., <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> strings like “COCCl”), processed one token at a time. Each token corresponds to an atom.</p></li>
<li><p><strong>Processing</strong>: Information flows sequentially through <span data-acronym-label="lstm" data-acronym-form="singular+short">lstm</span> blocks where each hidden state (<span class="math inline">\(h_{1}\)</span>, <span class="math inline">\(h_{2}\)</span>, <span class="math inline">\(h_{3}\)</span>, <span class="math inline">\(h_{4}\)</span>) accumulates information about the molecule. The memory cell maintains chemical context through gating mechanisms. The inductive bias is sequential processing—assuming chemical properties emerge from analyzing tokens in order.</p></li>
<li><p><strong>Pooling</strong>: The final hidden state (<span class="math inline">\(h_{4}\)</span>) captures the entire molecular information after processing the complete sequence. This last state serves as the molecular representation for the downstream task.</p></li>
</ul>
<p><span data-acronym-label="lstm" data-acronym-form="plural+short">lstms</span> process information in a strict sequence. For the model to connect the first word to the last, that information must pass through every single step in between. The cost of “talking” across the sequence grows with the sequence length. Furthermore, the entire history of the sequence must be compressed into a single, fixed-size hidden state.</p>
<p>An <span data-acronym-label="xlstm" data-acronym-form="singular+short">xlstm</span> overcomes this with two key changes. <span data-acronym-label="xlstm" data-acronym-form="singular+short">xlstm</span> uses enhanced gates (act like filters to control what information flows) to precisely revise its memory. Second, instead of a single memory bottleneck, it uses a parallel “matrix memory”. This provides multiple “slots” to store different pieces of information at the same time. This structure allows it to process information in parallel, making it much more efficient. adapts this architecture for biological and chemical sequences, demonstrating proficiency in generative tasks and in-context learning for DNA, proteins, and small molecules.[<span class="citation" data-cites="SchmidingerSSSH25">Schmidinger et al. (<a href="#ref-SchmidingerSSSH25" role="doc-biblioref">2025</a>)</span>]</p>
</section>
<section id="transformer" class="level4" data-number="5.8.0.2">
<h4 data-number="5.8.0.2" class="anchored" data-anchor-id="transformer"><span class="header-section-number">5.8.0.2</span> Transformer</h4>
<p>Transformers [<span class="citation" data-cites="vaswani2017attention">Vaswani et al. (<a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span>] are also designed for sequential data, but are particularly powerful in capturing long-range dependencies and rich contextual relationships within sequences. Their core “attention mechanism” allows them to weigh the importance of different parts of the input simultaneously (quadratic computational scaling—if you double the length of the sequence, the amount of work the model needs to do quadruples). Effectively, they can be thought of as a fully connected graph model,[<span class="citation" data-cites="velivckovic2023everything">Veličković (<a href="#ref-velivckovic2023everything" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="joshi2025transformers">Joshi (<a href="#ref-joshi2025transformers" role="doc-biblioref">2025</a>)</span>] where each representation of a token is connected to every other token and can impact its representation.</p>
<ul>
<li><p><strong>Input</strong>: Similar to <span data-acronym-label="lstm" data-acronym-form="plural+short">lstms</span>, data is tokenized and often enhanced with positional encodings (see <a href="#fig:architectures" data-reference-type="ref+Label" data-reference="fig:architectures">3</a>, the tokenized sequence is added with positional information, e.g., using a sinusoidal signal—the red-blue spectrum) to maintain information about where in a sequence a token is placed (the attention mechanism itself does not preserve this information).</p></li>
<li><p><strong>Processing</strong>: Uses attention mechanisms, where every atom/token attends to every other token simultaneously. This enables the capture of long-range interactions between distant elements of the sequence, regardless of their sequential distance. The <span data-acronym-label="fnn" data-acronym-form="singular+short">fnn</span> transforms these attention-weighted representations. To get a more robust and comprehensive understanding of the relationships within a sequence, models don’t just rely on a single way of “paying attention”. Instead, they employ multiple independent “attention heads” known as multi-head attention.</p></li>
<li><p><strong>Pooling</strong>: Uses an aggregated representation or special token that combines information from all tokens, enabling global molecular property prediction.</p></li>
</ul>
</section>
<section id="mamba" class="level4" data-number="5.8.0.3">
<h4 data-number="5.8.0.3" class="anchored" data-anchor-id="mamba"><span class="header-section-number">5.8.0.3</span> Mamba</h4>
<p>Mamba[<span class="citation" data-cites="gu2023mamba0">Gu and Dao (<a href="#ref-gu2023mamba0" role="doc-biblioref">2023</a>)</span>] is designed to be highly efficient (linear computational scaling) and effective at modeling very long sequences, offering a potentially more scalable alternative to Transformers for certain sequential tasks (for example, modelling very long protein sequences or polymer chains, while retaining strong performance in capturing dependencies.</p>
<ul>
<li><p><strong>Input</strong>: Sequences similar to <span data-acronym-label="lstm" data-acronym-form="singular+short">lstm</span>.</p></li>
<li><p><strong>Processing</strong>: First applies convolution to capture local contexts, creating representations that incorporate neighboring information. These contextualized tokens are then processed through a <span data-acronym-label="ssm" data-acronym-form="singular+short">ssm</span>. An <span data-acronym-label="ssm" data-acronym-form="singular+short">ssm</span> is a type of sequence model that efficiently captures and summarizes long-range dependencies by tracking an evolving internal “state” (evolving representation of all the relevant information) based on inputs. This <span data-acronym-label="ssm" data-acronym-form="singular+short">ssm</span> dynamically focuses on relevant parts. The inductive bias combines local patterns (through convolution) with efficient selective attention for handling long-range dependencies.</p></li>
<li><p><strong>Pooling</strong>: Uses the final hidden state (<span class="math inline">\(h_{4}\)</span>) similar to <span data-acronym-label="lstm" data-acronym-form="singular+short">lstm</span>, but this state contains selectively processed information that more efficiently captures important features.</p></li>
</ul>
<p>This architectural approach has been successfully applied to chemical foundation models, demonstrating <span data-acronym-label="sota" data-acronym-form="singular+short">sota</span> results in tasks like molecular property prediction and generation while maintaining fast inference on a large dataset of <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> samples.[<span class="citation" data-cites="soares2025mamba-based">Soares, Vital Brazil, et al. (<a href="#ref-soares2025mamba-based" role="doc-biblioref">2025</a>)</span>]</p>
</section>
<section id="gnn" class="level4" data-number="5.8.0.4">
<h4 data-number="5.8.0.4" class="anchored" data-anchor-id="gnn"><span class="header-section-number">5.8.0.4</span> <span data-acronym-label="gnn" data-acronym-form="singular+short">gnn</span></h4>
<p><span data-acronym-label="gnn" data-acronym-form="singular+short">gnn</span> is an architecture that complements graph representations (see the section discussing graph-based representation <a href="#sec:common_representations" data-reference-type="ref+Label" data-reference="sec:common_representations">1.2.1</a>). Molecules are represented as graphs, where atoms are nodes and bonds are edges. <span data-acronym-label="gnn" data-acronym-form="plural+short">gnns</span> operate on these graphs by processing node and edge representations. Based on how the nodes are connected through edges, the information in these representations is updated multiple times. This procedure is called message passing (see <a href="#fig:architectures" data-reference-type="ref+Label" data-reference="fig:architectures">3</a>). Information from neighbors is aggregated, and this aggregation occurs for all nodes and sometimes also for edges.</p>
<ul>
<li><p><strong>Input</strong>: Graphs, which are collections of nodes (e.g., atoms) and edges (e.g., bonds).</p></li>
<li><p><strong>Processing</strong>: Uses message passing through multiple aggregation steps (message would be the information in node or edge at the current stage, and aggregation can be different types of operations like adding information, taking mean, etc, depending on the architecture choice). Each node updates its representation based on messages from its bonded neighbors. The inductive bias is the graph structure itself, which naturally aligns with chemical bonding patterns.</p></li>
<li><p><strong>Pooling</strong>: Graph-level pooling (e.g., taking the mean of all node representations) aggregates information from all atoms and bonds to create a unified molecular representation, respecting the molecular graph structure.</p></li>
</ul>
<p>These architectures cannot solve all problems equally well because they are tailored to different data structures. <span data-acronym-label="lstm" data-acronym-form="singular+short">lstm</span> and Mamba inherently excel at processing sequential data; Transformers need to learn the structure, Transformers are powerful at capturing global relationships across the entire input, whereas <span data-acronym-label="gnn" data-acronym-form="plural+short">gnns</span> are designed for graph-structured information. Forcing one type to handle data optimally it was not intended for, often leads to suboptimal performance, inefficiency, or requires extensive, task-specific adaptations that dilute its “general-purpose” nature.[<span class="citation" data-cites="alampara2024mattext">Alampara, Miret, and Jablonka (<a href="#ref-alampara2024mattext" role="doc-biblioref">2024</a>)</span>]</p>
</section>
</section>
<section id="multimodality" class="level2" data-number="5.9">
<h2 data-number="5.9" class="anchored" data-anchor-id="multimodality"><span class="header-section-number">5.9</span> Multimodality</h2>
<p>Multimodal capabilities enable systems to process and understand multiple types of data simultaneously. Unlike traditional unimodal models, which work with a single data type (e.g., text-only or image-only), multimodal models can integrate and reason across different modalities, such as text, images, molecular structures, and spectroscopic data.</p>
<p>The core principle behind multimodal models lies in learning shared representations across different data types. The challenge of creating this shared representation can be addressed through several architectural strategies, each with a different approach to learning the joint distribution of multimodal data. One dominant strategy is joint embedding alignment, where separate, specialized encoders are used for each modality (e.g., a <span data-acronym-label="gnn" data-acronym-form="singular+short">gnn</span> for molecular structures and a Transformer for text). These encoders independently map their respective inputs into their own high-dimensional vector spaces. The key learning objective, often driven by contrastive learning (see <a href="#sec:contrastive_learning" data-reference-type="ref+Label" data-reference="sec:contrastive_learning">1.4.4</a>), is to align these separate spaces.</p>
<p>Another common approach is input-level fusion, where different data types are tokenized into a common format and fed into a single, unified architecture. For instance, a molecular structure might be converted into a <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> string, an image into a sequence of patches, and text into its standard tokens. These disparate token sequences are then concatenated and processed by a single large model, typically a Transformer. This architecture allows the model’s attention mechanism to learn correlations between modalities at a fundamental level directly—an image patch can “attend” to a word in the description, for instance. A more recent and highly efficient variant is adapter-based integration, where a powerful, pre-trained unimodal model (models that take a single type of representation) (like an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>) is frozen, and a small “adapter network” (see discussion about adapter in <a href="#sec:model_adaptation" data-reference-type="ref+Label" data-reference="sec:model_adaptation">1.11</a>) is trained to project the embeddings from a secondary modality (e.g., a molecule) into the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>’s existing latent space. This adapter effectively learns to translate the new data type into the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>’s native “language”, leveraging the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>’s vast pre-existing knowledge without the need for complete re-training. For instance, a model might learn that the textual description “benzene ring” corresponds to a specific visual pattern in molecular diagrams and produces characteristic peaks in <span data-acronym-label="nmr" data-acronym-form="singular+short">nmr</span> spectroscopy. This cross-modal understanding enables more comprehensive and contextually rich analysis than any single modality alone could provide.</p>
<section id="sec:multimodal_chem" class="level3" data-number="5.9.1">
<h3 data-number="5.9.1" class="anchored" data-anchor-id="sec:multimodal_chem"><span class="header-section-number">5.9.1</span> Multimodal Integration in Chemistry</h3>
<p>A molecule’s <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> string alone might not reveal its 3-D conformational preferences. A spectrum alone could suggest many different molecular structures. However, coupling these modalities with textual knowledge (e.g., “the sample was prepared by X method”) could narrow down possibilities. Multimodal models have the potential to emulate a human expert who simultaneously considers spectral patterns, chemical rules, and prior knowledge to deduce a structure. Another motivation is to create generalist <span data-acronym-label="ai" data-acronym-form="singular+short">ai</span> models. Instead of having multiple independent models—one for spectral analysis, another for molecule property prediction, and another for text mining—a single model could handle diverse tasks by understanding multiple data types. In this way, a researcher can ask a question in natural language, provide a molecule (in the form of a structure file or image) as context, and receive a helpful answer that leverages both structural and textual knowledge.</p>
<p>[<span class="citation" data-cites="edwards2022translation">Edwards et al. (<a href="#ref-edwards2022translation" role="doc-biblioref">2022</a>)</span>] adapted the transformer for chemical language by training on scientific text and <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> strings, using a masking objective to reconstruct masked segments. This approach treats <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> as a “language”, enabling to generate both valid molecules and fluent text. Similarly, [<span class="citation" data-cites="taylor2022galactica">Taylor et al. (<a href="#ref-taylor2022galactica" role="doc-biblioref">2022</a>)</span>], an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>, also incorporated <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> into its training. Later, the [<span class="citation" data-cites="liu2023molxpt0">Zequn Liu et al. (<a href="#ref-liu2023molxpt0" role="doc-biblioref">2023</a>)</span>] model used “paired” examples (<span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> and textual description) by replacing chemical names in scientific texts with their corresponding <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> strings and description. This pre-training approach enables to learn the context of molecules within text and achieve zero-shot text-to-molecule generation (see <a href="05-applications.html#sec:mol_generation" data-reference-type="ref+Label" data-reference="sec:mol_generation">[sec:mol_generation]</a> for more details on this application).</p>
<p>Contrastive learning emerged as an alternative, aligning separate text and molecule encoders in a shared embedding space. The principle of learning here is the same as that explained in <a href="#%7Bsec:contrastive_learning%7D" data-reference-type="ref+Label" data-reference="{sec:contrastive_learning}">[{sec:contrastive_learning}]</a>. [<span class="citation" data-cites="Liu2023multi0modal">S. Liu et al. (<a href="#ref-Liu2023multi0modal" role="doc-biblioref">2023</a>)</span>] aligns separate text and molecule encoders in a shared space using paired data. This dual-encoder approach enables tasks such as retrieving molecules from text queries and shows strong zero-shot generalization for chemical concepts. Another notable study is [<span class="citation" data-cites="sanchez2023cloome">Sanchez-Fernandez et al. (<a href="#ref-sanchez2023cloome" role="doc-biblioref">2023</a>)</span>], which used contrastive learning to embed bioimaging data (microscopy images of cell assays) and chemical structures of small molecules into a shared space. Multimodal learning also enables the determination of molecular structure from spectroscopic data. Models trained on large datasets of simulated spectra [<span class="citation" data-cites="alberts2024unraveling">Alberts et al. (<a href="#ref-alberts2024unraveling" role="doc-biblioref">2024</a>)</span>], which combine multiple spectral inputs, could accurately translate spectra into molecular structures. [<span class="citation" data-cites="chacko2024spectro">Chacko et al. (<a href="#ref-chacko2024spectro" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="mirza2024elucidating">A. Mirza and Jablonka (<a href="#ref-mirza2024elucidating" role="doc-biblioref">2024</a>)</span>]</p>
<p>Beyond just prediction, some multimodal models aim for cross-modal generation, creating one type of data from another (e.g., generating an <span data-acronym-label="ir" data-acronym-form="singular+short">ir</span> spectrum from a molecular structure). <span class="citation" data-cites="takeda2023multi">Takeda et al. (<a href="#ref-takeda2023multi" role="doc-biblioref">2023</a>)</span> developed a multimodal foundation model for materials design, integrating <span data-acronym-label="selfies" data-acronym-form="singular+short">selfies</span> strings, <span data-acronym-label="dft" data-acronym-form="singular+short">dft</span> properties, and optical absorption spectra. Their approach involves encoding each type of data separately into a shared, compressed representation space. Then, a network learns to combine these compressed representations to understand the connections between them. This pre-training on a big dataset of samples enables both combined representations (joint embeddings summarizing all modalities) and cross-modal generation, allowing tasks like predicting a spectrum from a molecule or generating a molecule from desired properties, effectively learning the relationships between structure, spectra, and quantum properties.</p>
<p>Their approach involves encoding each type of data (like molecular structure or properties) separately into a shared, compressed representation. Then, a network learns to combine these compressed representations to understand the connections between them.</p>
<p>A more recent approach is the integration of molecular encoders with pre-trained <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>. Models like [<span class="citation" data-cites="cao2023instructmol0">Cao et al. (<a href="#ref-cao2023instructmol0" role="doc-biblioref">2023</a>)</span>] and [<span class="citation" data-cites="li2024seeing">Li et al. (<a href="#ref-li2024seeing" role="doc-biblioref">2024</a>)</span>] use an “adapter” (see discussion about in <a href="#sec:model_adaptation" data-reference-type="ref+Label" data-reference="sec:model_adaptation">1.11</a>) to project molecular information into the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>’s existing knowledge space. This two-stage process first projects molecule representations into the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>’s token space through pre-training on molecule-description pairs. Subsequently, instruction tuning on diverse chemistry tasks (e.g., <span data-acronym-label="qa" data-acronym-form="singular+short">qa</span>, reaction reasoning) enables the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> to leverage molecular inputs, significantly enhancing its performance on chemistry-specific problems.</p>
<p>The latest generation of foundation models is often natively multimodal, designed from the ground up to process text, images, and other data types seamlessly. Natively multimodal systems are characterized by a single, unified neural network trained end-to-end on a diverse range of data modalities. This approach contrasts with previous methods that would stitch together separate models for each data type, enabling a more seamless and nuanced understanding of context and relationships across different informational forms. In the scientific domain, natively multimodal systems are still being explored. However, evaluations suggest that these models are not yet robust for solving complex scientific research tasks.[<span class="citation" data-cites="alampara2024probing">Alampara et al. (<a href="#ref-alampara2024probing" role="doc-biblioref">2024</a>)</span>]</p>
</section>
</section>
<section id="optimizations" class="level2" data-number="5.10">
<h2 data-number="5.10" class="anchored" data-anchor-id="optimizations"><span class="header-section-number">5.10</span> Optimizations</h2>
<p>As <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> continue to grow in size and complexity, optimization techniques become critical for making these models practically deployable while maintaining their accuracy. This section discusses three key optimization approaches that have particular promise for chemistry foundation models: <span data-acronym-label="moe" data-acronym-form="singular+short">moe</span> architectures for efficient scaling, quantization, and mixed precision for memory and computational efficiency, and knowledge distillation for creating specialized, lightweight models.</p>
<section id="sec:arch-moes" class="level3" data-number="5.10.1">
<h3 data-number="5.10.1" class="anchored" data-anchor-id="sec:arch-moes"><span class="header-section-number">5.10.1</span> Mixture-of-Experts</h3>
<p><span data-acronym-label="moe" data-acronym-form="singular+short">moe</span> is a neural network architecture that uses multiple specialized “expert” networks instead of one single, monolithic model. The core idea is to divide the vast problem space—the embedding space of all possible inputs—into more manageable, homogeneous regions. A region is considered “homogeneous” not because all inputs within it are identical, but because they share similar characteristics and can be processed using a consistent set of rules. For instance, in a chemistry model, one expert might specialize in organic molecules, while another focuses on inorganic crystals; each expert sees a more consistent, or homogeneous, set of problems. This division of labor is managed by a gating network, which acts like a smart dispatcher. This gating network is itself a small neural network, often referred to as a trainable router, because it learns during training how best to route the data to the most appropriate expert, thereby improving its decisions over time. <span data-acronym-label="moe" data-acronym-form="singular+short">moe</span> models achieve efficiency through selectively activating only the specific experts needed for a given task, rather than activating the entire neural network for every task. Modern transformer models using <span data-acronym-label="moe" data-acronym-form="singular+short">moe</span> layers can scale to billions of parameters while maintaining manageable computational costs, as demonstrated by models like , which uses eight experts with sparsity.</p>
<p><span class="citation" data-cites="shazeer2017outrageously">Shazeer et al. (<a href="#ref-shazeer2017outrageously" role="doc-biblioref">2017</a>)</span> demonstrated that using a sparsely-gated <span data-acronym-label="moe" data-acronym-form="singular+short">moe</span> layer can expand a network’s capacity (by over 1000 times) with only minor increases in computation. In this architecture, each expert is typically a <span data-acronym-label="fnn" data-acronym-form="singular+short">fnn</span>, and a trainable router determines which tokens are sent to which experts, allowing only a subset of the total parameters to be active for any given input.</p>
<p>An <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> for science with <span data-acronym-label="moe" data-acronym-form="singular+short">moe</span> architecture ( [<span class="citation" data-cites="sun2024scidfm">Sun et al. (<a href="#ref-sun2024scidfm" role="doc-biblioref">2024</a>)</span>]) shows that the results of expert selection vary with data from different disciplines, i.e., activating distinct experts for chemistry vs.&nbsp;other disciplines. They consist of multiple “expert” subnetworks, each potentially specializing in different facets of chemical knowledge or types of chemical tasks. A routing mechanism directs inputs to the most relevant expert(s). This allows the foundation model to be more adaptable and perform across the broad chemical landscape. The <span data-acronym-label="moe" data-acronym-form="singular+short">moe</span> concept has also been adapted for physical simulations. The family employs a <span data-acronym-label="moe" data-acronym-form="singular+short">moe</span> architecture with linear experts to build accurate yet computationally efficient interatomic potentials [<span class="citation" data-cites="wood2025uma0">Wood et al. (<a href="#ref-wood2025uma0" role="doc-biblioref">2025</a>)</span>]. This approach builds route based on global system properties (e.g., elemental composition, charge, spin) rather than per-token.</p>
<p>Extending this concept, a recent multi-view <span data-acronym-label="moe" data-acronym-form="singular+short">moe</span> model ( [<span class="citation" data-cites="shirasuna2024multi">Soares, Shirasuna, et al. (<a href="#ref-shirasuna2024multi" role="doc-biblioref">2025</a>)</span>]) treats entire, distinct chemical models as individual “experts”. Rather than routing tokens within one large model, a gating network learns to create a combined molecular representation by dynamically weighting the embeddings from each expert model. This method showed strong performance on , a widely used benchmark suite for molecular property prediction, outperforming competitors on 9 of 11 tasks.</p>
<p>Training <span data-acronym-label="moe" data-acronym-form="singular+short">moe</span> models can be a complex process. The gating mechanism must be carefully learned to balance expert usage and instability, or some experts may end up underutilized (most of the data would be processed by a subset of networks). <span class="citation" data-cites="fedus2022switch">(<a href="#ref-fedus2022switch" role="doc-biblioref">Fedus, Zoph, and Shazeer 2022</a>)</span> For chemistry tasks, an additional challenge is to ensure that each expert has access to sufficient relevant chemical data to specialize. If the data is sparse, some experts may not learn meaningful functions. Despite these hurdles, <span data-acronym-label="moe" data-acronym-form="plural+short">moes</span> remain a promising optimization strategy to handle the breadth of chemical space.</p>
</section>
<section id="sec:quantization" class="level3" data-number="5.10.2">
<h3 data-number="5.10.2" class="anchored" data-anchor-id="sec:quantization"><span class="header-section-number">5.10.2</span> Quantization and Mixed Precision</h3>
<p>Quantization is a technique for making models more computationally efficient by reducing their numerical precision. In experimental science, precision often relates to the number of significant figures in a measurement; a highly precise value, such as <span class="math inline">\(3.14159\)</span>, carries more information than a rounded one, like <span class="math inline">\(3.14\)</span>. Similarly, a model’s knowledge is stored in its weights, which are organized into large matrices of numbers. Standard models typically use high-precision formats, such as 32-bit floating-point, which can represent a wide range of numbers with many decimal places to store weights. During inference, these weight matrices are multiplied by the input data to produce a prediction. Quantization involves converting these numbers into a lower-precision format, such as 8-bit integers, which are whole numbers with a much smaller range. This process is similar to rounding down your experimental data—it simplifies the numbers, uses less memory, and allows calculations to run much faster.</p>
<p><span class="citation" data-cites="dettmers2022gpt3">Dettmers et al. (<a href="#ref-dettmers2022gpt3" role="doc-biblioref">2022</a>)</span> introduced an 8-bit inference approach (<code>LLM.int8</code>) enabling models as large as (175B parameters) to run with no loss in predictive performance (less than <span class="math inline">\(50\%\)</span> GPU-memory usage). A key insight in this paper is that while most numbers in a model can be safely rounded, a few “outlier” values with large magnitudes are critical for performance.</p>
<p>A different, yet related, strategy is mixed-precision quantization.[<span class="citation" data-cites="micikevicius2017mixed">Micikevicius et al. (<a href="#ref-micikevicius2017mixed" role="doc-biblioref">2017</a>)</span>] Instead of applying a single precision format (like 8-bit) across the entire model, this approach uses a mix of different precisions for different parts of the network. The guiding principle is that some layers of the model might be more sensitive to rounding errors than others.</p>
<p>Many chemistry applications, particularly in automated laboratory setups, require deployment on edge devices—local computing hardware, such as the controllers for robotic arms or the onboard computers in analytical instruments—or cloud platforms with limited computational resources. Quantization can be a valuable optimization tool for reducing computational burden while increasing inference speed, which is crucial for real-time applications.</p>
</section>
<section id="sec:peft" class="level3" data-number="5.10.3">
<h3 data-number="5.10.3" class="anchored" data-anchor-id="sec:peft"><span class="header-section-number">5.10.3</span> Parameter-Efficient Tuning</h3>
<p>While full fine-tuning is computationally expensive, memory-intensive, and results in a complete, multi-gigabyte copy of the model for every new task. <span data-acronym-label="peft" data-acronym-form="singular+short">peft</span> methods offer a solution to this problem by freezing the vast majority of the trained model’s weights and only training a very small number of new parameters. This is conceptually similar to attaching a small, specialized probe to a large, complex analytical instrument; you adapt its function for a new task without re-engineering the entire machine.</p>
<p>A prominent and widely used <span data-acronym-label="peft" data-acronym-form="singular+short">peft</span> technique is <span data-acronym-label="lora" data-acronym-form="singular+short">lora</span>.[<span class="citation" data-cites="hu2022lora">Hu et al. (<a href="#ref-hu2022lora" role="doc-biblioref">2022</a>)</span>] The key insight of <span data-acronym-label="lora" data-acronym-form="singular+short">lora</span> is that the change needed to adapt a pre-trained weight matrix for a new task can be approximated effectively using much smaller matrices. <span data-acronym-label="lora" data-acronym-form="singular+short">lora</span> freezes the original model weights and introduces small trainable rank-decomposition matrices into each transformer layer, significantly reducing the number of trainable parameters. Because these new matrices contain far fewer parameters-often less than 0.1% of the original model-the computational and memory requirements for training are drastically reduced.</p>
<p>These optimization strategies can be combined with quantization (see <a href="#sec:quantization" data-reference-type="ref+Label" data-reference="sec:quantization">1.10.2</a>) for even greater efficiency. <span class="citation" data-cites="dettmers2023qlora">Dettmers et al. (<a href="#ref-dettmers2023qlora" role="doc-biblioref">2023</a>)</span> introduced <span data-acronym-label="qlora" data-acronym-form="singular+short">qlora</span>. In this approach, the large pre-trained model is first quantized down to a very low precision (typically 4-bit), dramatically shrinking its memory footprint. Then, the lightweight <span data-acronym-label="lora" data-acronym-form="singular+short">lora</span> adapters are added and fine-tuned. The impact of this is profound: <span data-acronym-label="qlora" data-acronym-form="singular+short">qlora</span> enables the fine-tuning of massive models—such as a 70-billion-parameter model—on a single, consumer-grade GPU.</p>
</section>
<section id="distillation" class="level3" data-number="5.10.4">
<h3 data-number="5.10.4" class="anchored" data-anchor-id="distillation"><span class="header-section-number">5.10.4</span> Distillation</h3>
<p>Knowledge distillation is a technique that aims to transfer the learning of a large pre-trained model (the “teacher model”) to a smaller “student model”. [<span class="citation" data-cites="hinton2015distilling">Hinton, Vinyals, and Dean (<a href="#ref-hinton2015distilling" role="doc-biblioref">2015</a>)</span>] The computationally more efficient “student model” is trained to mimic the behavior (e.g., output probabilities or internal representations) of the larger teacher model. This allows the rich, nuanced understanding learned by the large foundation model to be compressed into a more compact and faster student model.[<span class="citation" data-cites="sanh2019distilbert">Sanh et al. (<a href="#ref-sanh2019distilbert" role="doc-biblioref">2019</a>)</span>]</p>
<p>For example, recent work introduced a method for transferring general-purpose representations from machine learning force field (<span data-acronym-label="mlip" data-acronym-form="singular+short">mlip</span>) foundation models to smaller, faster <span data-acronym-label="mlip" data-acronym-form="plural+short">mlips</span> specialized to specific regions of chemical space. Formulating the approach as a knowledge distillation procedure where the student <span data-acronym-label="mlip" data-acronym-form="singular+short">mlip</span> is trained to match the Hessians of the energy predictions of the teacher foundation model. [<span class="citation" data-cites="amin2025towards">Amin, Raja, and Krishnapriyan (<a href="#ref-amin2025towards" role="doc-biblioref">2025</a>)</span>] Their specialized <span data-acronym-label="mlip" data-acronym-form="plural+short">mlips</span> achieved up to 20 times faster inference than the original foundation model while retaining, and in some cases exceeding, its performance.</p>
<p>Effective distillation requires that the teacher model is both competent at the task and that its knowledge is representable by the student. If the teacher is too large or complex compared to the student, the student may struggle to emulate it, leading to degraded performance. [<span class="citation" data-cites="liu2024wisdom">Zichang Liu et al. (<a href="#ref-liu2024wisdom" role="doc-biblioref">2024</a>)</span>]</p>
</section>
</section>
<section id="sec:model_adaptation" class="level2" data-number="5.11">
<h2 data-number="5.11" class="anchored" data-anchor-id="sec:model_adaptation"><span class="header-section-number">5.11</span> Model Level Adaptation</h2>
<p>Although promising, <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> such as <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> rarely work straight out of the box for specialized tasks and often need customization. This is especially true for complex scientific problems where data is a limiting factor. By simply prompting an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>—for example, by asking a question or giving instructions—one can observe that these models perform much better on general tasks than on those related to chemistry. This difference arises because <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> are not typically trained on domain-specific chemical tasks and therefore they lack the necessary knowledge and reasoning skills.</p>
<p>To bridge this gap, two complementary families of approaches exist. First approach involves adapting the model’s knowledge or behavior directly. The simplest method is to embed information directly in the prompt, for instance by providing examples (<span data-acronym-label="icl" data-acronym-form="singular+short">icl</span>) [<span class="citation" data-cites="brown2020language">Brown et al. (<a href="#ref-brown2020language" role="doc-biblioref">2020</a>)</span>] or by introducing intermediate reasoning steps (<span data-acronym-label="cot" data-acronym-form="singular+short">cot</span>) [<span class="citation" data-cites="wei2022chain">Wei et al. (<a href="#ref-wei2022chain" role="doc-biblioref">2022</a>)</span>]. However, not all problems can be solved in these ways, and sometimes it is necessary to tune the model to new data which updates its parameters. The second approach involves coupling the model into a larger system that can interact with external sources of information and tools.</p>
<div id="tab:model_adaptation">
<table class="caption-top table">
<caption><strong>Model Adaptation Approaches Overview:</strong> This table provides a rough overview of the estimated time, data, and <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> knowledge required for each approach. Each method (listed in the first column) is paired with a triplet that includes: an approximate implementation time, the estimated dataset size, and the level of <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> expertise needed. These estimates assume that you have at least a bachelor’s level of understanding of chemistry and at least some computational background.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Model adaptation</strong></th>
<th style="text-align: left;"><strong>Time</strong></th>
<th style="text-align: left;"><strong>Data</strong></th>
<th style="text-align: left;"><strong>ML knowledge</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Pre-training</td>
<td style="text-align: left;">Weeks</td>
<td style="text-align: left;">1M–1B+</td>
<td style="text-align: left;">Very High</td>
</tr>
<tr class="even">
<td style="text-align: left;">Zero-shot prompting</td>
<td style="text-align: left;">Minutes</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">None</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Few-shot Prompting</td>
<td style="text-align: left;">Hours</td>
<td style="text-align: left;">&lt;10 examples</td>
<td style="text-align: left;">None</td>
</tr>
<tr class="even">
<td style="text-align: left;">Fine-tuning</td>
<td style="text-align: left;">Days</td>
<td style="text-align: left;">&lt;10k</td>
<td style="text-align: left;">High</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Coupling into systems</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">RAG</td>
<td style="text-align: left;">Days</td>
<td style="text-align: left;">100k–1M+</td>
<td style="text-align: left;">Low</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Tool-Augmentation</td>
<td style="text-align: left;">Days</td>
<td style="text-align: left;">None / 10k+</td>
<td style="text-align: left;">Low</td>
</tr>
</tbody>
</table>
</div>
<p><span id="tab:model_adaptation" data-label="tab:model_adaptation"></span></p>
<section id="sec:prompting" class="level4" data-number="5.11.0.1">
<h4 data-number="5.11.0.1" class="anchored" data-anchor-id="sec:prompting"><span class="header-section-number">5.11.0.1</span> Prompting</h4>
<p><span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> have demonstrated the ability to perform a wide range of tasks based solely on prompt instructions—without the need for fine-tuning [<span class="citation" data-cites="radford2019language">Radford et al. (<a href="#ref-radford2019language" role="doc-biblioref">2019</a>)</span>]. This ability, for <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> to complete tasks without any additional information is often referred to as zero-shot prompting. By providing task-specific examples directly within the input prompt, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can draw analogies and generalize to new tasks, a capability known as <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> [<span class="citation" data-cites="brown2020language">Brown et al. (<a href="#ref-brown2020language" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="chowdhery2023palm">Chowdhery et al. (<a href="#ref-chowdhery2023palm" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="openai2023gpt04">OpenAI et al. (<a href="#ref-openai2023gpt04" role="doc-biblioref">2023</a>)</span>]. In <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span>, the model is presented with a few demonstration examples alongside a query, all within the same input—a technique known as few-shot prompting. The model’s parameters remain unchanged; instead, it is expected that the model can recognize patterns within the prompt and generate an appropriate response [<span class="citation" data-cites="von2023transformers">Von Oswald et al. (<a href="#ref-von2023transformers" role="doc-biblioref">2023</a>)</span>]. <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> enables models to learn on the fly, reducing the barrier to entry for users without deep <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> expertise. However, because the model does not retain memory between queries, the learned knowledge is temporary and is subsequently lost in subsequent queries. Additionally, <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> tends to struggle with tasks that require multi-step reasoning [<span class="citation" data-cites="brown2020language">Brown et al. (<a href="#ref-brown2020language" role="doc-biblioref">2020</a>)</span>]. To address this limitation, task decomposition techniques have been introduced, with the earliest being <span data-acronym-label="cot" data-acronym-form="singular+short">cot</span> [<span class="citation" data-cites="wei2022chain">Wei et al. (<a href="#ref-wei2022chain" role="doc-biblioref">2022</a>)</span>]. Rather than relying solely on examples, this approach enriches the prompt with a series of reasoning steps that guide the model toward the correct answer [<span class="citation" data-cites="wei2022chain">Wei et al. (<a href="#ref-wei2022chain" role="doc-biblioref">2022</a>)</span>]. Considering that prompting approaches do not require an in-depth understanding of machine learning, they have proven very useful for a range of chemical tasks, including chemical data extraction, <span data-acronym-label="qa" data-acronym-form="singular+short">qa</span>, and property prediction [<span class="citation" data-cites="liu2025integrating">H. Liu et al. (<a href="#ref-liu2025integrating" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="zheng2023chatgpt">Zheng et al. (<a href="#ref-zheng2023chatgpt" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="mirza2024large">Adrian Mirza et al. (<a href="#ref-mirza2024large" role="doc-biblioref">2025</a>)</span>].</p>
</section>
<section id="sec:fine-tuning" class="level4" data-number="5.11.0.2">
<h4 data-number="5.11.0.2" class="anchored" data-anchor-id="sec:fine-tuning"><span class="header-section-number">5.11.0.2</span> Fine-tuning</h4>
<p>What separates fine-tuning from the other approaches discussed is that it directly changes the weights of the model as well as its broad applicability across different model architectures, not just <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>. Apart from changing the weights (see <a href="#sec:fine_tuning_coloring" data-reference-type="ref+Label" data-reference="sec:fine_tuning_coloring">1.6</a>), unlike techniques like <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> or prompt engineering that are limited to <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, fine-tuning can be applied to a wide variety of architectures, including other transformer-based models, <span data-acronym-label="gnn" data-acronym-form="plural+short">gnns</span>, <span data-acronym-label="cnn" data-acronym-form="plural+short">cnns</span>, and others. The fine-tuning strategy depends on the size and complexity of the target dataset as well as the pre-trained model. For many tasks, especially when using a powerful pre-trained model, it is often sufficient to freeze the entire model except for the final layer and only train that layer’s parameters. However, as the target task diverges more significantly from the pre-trained model’s original objectives, more adaptation may be necessary. This can include replacing specific layers in the model to better suit the new task. For instance, in autoencoder architectures, it’s common to freeze the encoder and replace the decoder. In <span data-acronym-label="gnn" data-acronym-form="plural+short">gnns</span>, the graph convolutional layers are typically frozen, while the final fully connected layers are replaced and re-trained. In some cases, it may be necessary to fine-tune the entire model, an especially resource-intensive process for <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, whose parameters can be in billions. To speed up this process, methods like <span data-acronym-label="peft" data-acronym-form="singular+short">peft</span> have been developed (see more details in <a href="#sec:peft" data-reference-type="ref+Label" data-reference="sec:peft">1.10.3</a>). Despite these innovations, one key limitation of fine-tuning remains: adapting to a new modality, which often requires architectural changes or switching to a different model. However, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> offer a unique workaround. Many regression or classification tasks can be reformulated into a text-based format, allowing a single language model to be fine-tuned across a wide range of tasks. This is known as <span data-acronym-label="lift" data-acronym-form="singular+short">lift</span> [<span class="citation" data-cites="dinh2022lift">Dinh et al. (<a href="#ref-dinh2022lift" role="doc-biblioref">2022</a>)</span>], which enables us to utilize a single <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> for a diverse set of tasks.</p>
<p>Beyond adapting a model’s internal knowledge through prompting or fine-tuning, its capabilities can be expanded by coupling it with external resources. This approach transforms a static model into a dynamic problem-solver that can access up-to-date information and perform actions in the world. This practice of designing and delivering task-relevant information is often referred to as context engineering. The necessary context can be provided through several complementary approaches that operate during inference time. This is achieved by coupling <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> into a system of resources.</p>
</section>
</section>
<section id="sec:agents" class="level2" data-number="5.12">
<h2 data-number="5.12" class="anchored" data-anchor-id="sec:agents"><span class="header-section-number">5.12</span> System-level Integration: Agents</h2>
<p>While powerful, <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> are fundamentally static entities. Their knowledge is frozen at the time of training, and they lack the ability to interact with the world beyond the information they process. They cannot browse the web for the latest research, execute code to perform a calculation, or control a robot to run an experiment. To overcome these limitations and apply the reasoning capabilities of <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> to complex, multistep scientific problems, so-called <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-based agents have emerged.</p>
<p>An <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-based agent is a system that leverages an <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> as its core “brain” but couples it with a set of tools to perceive and act upon its environment. To use a tool, the agent simply generates text containing the tool’s name and its required inputs (arguments). The framework managing the agent recognizes this specific text, executes the corresponding tool, and then feeds the result back to the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>. This transforms the model from a passive generator of text into an active problem-solver that can formulate plans, execute actions, observe the results, and adapt its strategy accordingly. For chemists and material scientists, this paradigm shift is profound. It moves from asking a model a question to giving it a research goal, which it can then pursue autonomously.</p>
<p><a href="#fig:agent-loop" data-reference-type="ref+Label" data-reference="fig:agent-loop">4</a> illustrates the fundamental components of an agentic framework, often conceptualized through the interacting modules of perception, cognition, and execution. It is important to note that this is one possible way to formalize an agent’s architecture; other organizational structures exist. Rather than a strict, sequential loop, these components represent a set of capabilities that the agent’s core <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> can dynamically draw upon to achieve complex objectives.</p>
<figure id="fig:agent-loop" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure7.png" width="100%" class="figure-img">
<figcaption>
<strong>The execution-cognition-perception capabilities of an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-agent:</strong> This figure illustrates how agents orchestrate complex problems. At the core, an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-agent coordinates multiple capabilities. Upon prompting, the agent can execute tools. Tools include but are not limited to running code, searching for information, or calling functions. These actions feed into the agent’s perception system, which transforms raw data into structured representations (in combination, for example, agents can obtain figures’ information by executing <span data-acronym-label="ocr" data-acronym-form="singular+short">ocr</span> tools on paper). The cognitive architecture underneath serves as the “agent’s brain”, utilizing both memory systems (long-term knowledge storage and short-term contextual awareness) alongside reasoning mechanisms and planning strategies. This creates a dynamic setup, where execution produces observations, cognition interprets those observations and formulates plans, and new actions are taken based on improved understanding.
</figcaption>
</figure>
<section id="sec:arch_agents" class="level3" data-number="5.12.1">
<h3 data-number="5.12.1" class="anchored" data-anchor-id="sec:arch_agents"><span class="header-section-number">5.12.1</span> Core Components of an Agentic System</h3>
<p>An agent is a system composed of several key components that work in concert.</p>
<section id="cognitive-engine" class="level4" data-number="5.12.1.1">
<h4 data-number="5.12.1.1" class="anchored" data-anchor-id="cognitive-engine"><span class="header-section-number">5.12.1.1</span> Cognitive Engine</h4>
<p>This is typically a powerful <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>. It is responsible for all high-level reasoning, including understanding the user’s objective, breaking it down into smaller, manageable steps (see planning <a href="05-applications.html#sec:planning" data-reference-type="ref+Label" data-reference="sec:planning">[sec:planning]</a>), and deciding which tools to use to accomplish each step.</p>
</section>
<section id="sec:tool_augmentation" class="level4" data-number="5.12.1.2">
<h4 data-number="5.12.1.2" class="anchored" data-anchor-id="sec:tool_augmentation"><span class="header-section-number">5.12.1.2</span> Tool Augmentations (Execution)</h4>
<p>Tools are external programs or functions that the agent can call upon to perform actions. They allow agents to interact with the world beyond their internal knowledge. [<span class="citation" data-cites="schick2023toolformer">Schick et al. (<a href="#ref-schick2023toolformer" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="parisi2022talm">Parisi, Zhao, and Fiedel (<a href="#ref-parisi2022talm" role="doc-biblioref">2022</a>)</span>]. Tool augmentation can range from simple tools, such as calculators, to more complex systems that involve web searches, code execution, and integration with robots [<span class="citation" data-cites="darvish2025organa">Darvish et al. (<a href="#ref-darvish2025organa" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="chan2024mle">Chan et al. (<a href="#ref-chan2024mle" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="wei2025browsecomp">Wei et al. (<a href="#ref-wei2025browsecomp" role="doc-biblioref">2025</a>)</span>]. In a chemical context, tools can be as simple as a stoichiometry calculator or as complex as a Python script that runs a <span data-acronym-label="dft" data-acronym-form="singular+short">dft</span> simulation using specialized software, a search <span data-acronym-label="api" data-acronym-form="singular+short">api</span> for querying chemical databases like , or a controller for a robotic synthesis platform [<span class="citation" data-cites="boiko2023autonomous">Boiko et al. (<a href="#ref-boiko2023autonomous" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="darvish2025organa">Darvish et al. (<a href="#ref-darvish2025organa" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="bran2024augmenting">Bran et al. (<a href="#ref-bran2024augmenting" role="doc-biblioref">2024</a>)</span>].</p>
</section>
<section id="sec:rag" class="level4" data-number="5.12.1.3">
<h4 data-number="5.12.1.3" class="anchored" data-anchor-id="sec:rag"><span class="header-section-number">5.12.1.3</span> Memory &amp; <span data-acronym-label="rag" data-acronym-form="singular+short">rag</span></h4>
<p>Agents need to maintain context over long and complex tasks. The memory module provides this capability. Short-term memory is often handled within the finite context window (i.e., number of tokens an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> can process), keeping track of the immediate chain of thought and recent actions. While short-term memory (e.g., context) is transient, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>’s model weights serve as long-term memory. However, these weights often lead to reduced performance on knowledge-intensive scientific tasks and increased susceptibility to hallucinations—generating incorrect or fabricated information [<span class="citation" data-cites="marcus2020next">Marcus (<a href="#ref-marcus2020next" role="doc-biblioref">2020</a>)</span>]. One effective way to address this limitation is to pair the model with an external knowledge base.[<span class="citation" data-cites="lewis2020retrieval">Lewis et al. (<a href="#ref-lewis2020retrieval" role="doc-biblioref">2020</a>)</span>] Such Long-term memory can be implemented using external databases (e.g., vector stores) where the agent can store and retrieve key findings, successful strategies, or experimental results from past interactions, enabling it to learn and improve over time [<span class="citation" data-cites="chen2023chemist">K. Chen et al. (<a href="#ref-chen2023chemist" role="doc-biblioref">2023</a>)</span>]. The widely adopted execution of Long-term memory is <span data-acronym-label="rag" data-acronym-form="singular+short">rag</span>. <span data-acronym-label="rag" data-acronym-form="singular+short">rag</span> works by retrieving a set of relevant documents from a designated knowledge database based on the input query. These retrieved documents are then concatenated with the original prompt and passed to the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>, which generates the final output. In scientific applications, this is particularly valuable, as the system can be continuously updated with the latest research and discoveries. In the field of chemistry, <span data-acronym-label="rag" data-acronym-form="singular+short">rag</span> has primarily been used to answer domain-specific questions based on scientific literature and assist in experimental design [<span class="citation" data-cites="chen2023chemist">K. Chen et al. (<a href="#ref-chen2023chemist" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="skarlinski2024language">Skarlinski et al. (<a href="#ref-skarlinski2024language" role="doc-biblioref">2024</a>)</span>].</p>
</section>
</section>
<section id="approaches-for-building-agentic-system" class="level3" data-number="5.12.2">
<h3 data-number="5.12.2" class="anchored" data-anchor-id="approaches-for-building-agentic-system"><span class="header-section-number">5.12.2</span> Approaches for building Agentic System</h3>
<p>A well-known approach for building <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-based agents is called <span data-acronym-label="react" data-acronym-form="singular+short">react</span>[<span class="citation" data-cites="yao2023react">Yao et al. (<a href="#ref-yao2023react" role="doc-biblioref">2023</a>)</span>]. In <span data-acronym-label="react" data-acronym-form="singular+short">react</span>, the agent repeatedly goes through a cycle of thinking, performing an action, and then reasoning about the tool output. This structured problem-solving is achieved by prompting the model to generate its response following a specific “Think”, “Act”, “Observe” format. First, the agent considers the problem it needs to solve, focusing on its primary objective. It devises a plan, identifies any missing information, and determines which tool can help it move forward. Next, the agent acts by selecting and utilizing the appropriate tool with the necessary information. For instance, if it needs to find a compound’s boiling point, it might use a tool that searches a chemical database using the compound’s name or its <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> string. After that, the agent observes the outcome by examining the tool’s output. This output then becomes new information for the agent. The agent then repeats the cycle, taking this new observation into account as it plans its following action. This loop continues until the agent reaches its main goal. This repeating process helps the agent deal with mistakes, adjust to unexpected results, and break down a big task, like “finding a better catalyst for this reaction”, into smaller, manageable steps that involve using tools.</p>
<p>While a single agent can effectively tackle linear problems—tasks that can be solved through a predictable sequence of steps—complex scientific discovery often requires diverse expertise and collaborative problem-solving. This has led to the development of multi-agent systems, which move beyond a single cognitive engine to orchestrate a team of agents that work together [<span class="citation" data-cites="wu2023autogen">Wu et al. (<a href="#ref-wu2023autogen" role="doc-biblioref">2023</a>)</span>]. These systems can solve tasks that are too complex or multifaceted for any single agent to handle alone by enabling agents to communicate, delegate, and debate. [<span class="citation" data-cites="lazaridou2020emergent">Lazaridou and Baroni (<a href="#ref-lazaridou2020emergent" role="doc-biblioref">2020</a>)</span>] Several collaborative paradigms have emerged, each offering unique advantages:</p>
<section id="sec:multi-agent" class="level4" data-number="5.12.2.1">
<h4 data-number="5.12.2.1" class="anchored" data-anchor-id="sec:multi-agent"><span class="header-section-number">5.12.2.1</span> Specialization and Division of Labor</h4>
<p>Just as a human research group has members with different roles, multi-agent systems can be composed of specialized agents. For example, in a chemistry context, a “Planner” agent might design a high-level research plan, a “Literature Searcher” agent could retrieve relevant papers, a “Computational Chemist” agent could run <span data-acronym-label="dft" data-acronym-form="singular+short">dft</span> simulations, and a “Safety Expert” agent could check proposed reaction steps for hazards.[<span class="citation" data-cites="Zou2025ElAgente">Zou et al. (<a href="#ref-Zou2025ElAgente" role="doc-biblioref">2025</a>)</span>] This division of labor shows this role-playing approach to be highly effective for complex tasks like software development, where agents take on roles such as “programmer”, “tester”, and “documenter” [<span class="citation" data-cites="qian2024chatdevcommunicativeagentssoftware">Qian et al. (<a href="#ref-qian2024chatdevcommunicativeagentssoftware" role="doc-biblioref">2024</a>)</span>].</p>
</section>
<section id="refinement-of-answers" class="level4" data-number="5.12.2.2">
<h4 data-number="5.12.2.2" class="anchored" data-anchor-id="refinement-of-answers"><span class="header-section-number">5.12.2.2</span> Refinement of Answers</h4>
<p>A key weakness of single <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> is their tendency to hallucinate or pursue a flawed line of reasoning. Multi-agent systems can mitigate this by introducing criticism and debate. In this paradigm, one agent might propose a solution (e.g., a synthetic pathway), while a “Critic” agent is tasked with finding flaws in the proposal. This adversarial or collaborative process forces the system to refine its ideas, correct errors, and explore alternatives, leading to more robust and reliable outcomes [<span class="citation" data-cites="liang2024encouragingdivergentthinkinglarge">Liang et al. (<a href="#ref-liang2024encouragingdivergentthinkinglarge" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="du2023improving">Du et al. (<a href="#ref-du2023improving" role="doc-biblioref">2023</a>)</span>].</p>
</section>
<section id="context-compression-through-parallelism" class="level4" data-number="5.12.2.3">
<h4 data-number="5.12.2.3" class="anchored" data-anchor-id="context-compression-through-parallelism"><span class="header-section-number">5.12.2.3</span> Context Compression through Parallelism</h4>
<p>A significant operational challenge for any <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-based system is the finite context window. As a task becomes more complex, the conversational history can grow cluttered with irrelevant details, degrading the model’s performance.[<span class="citation" data-cites="chirkova2025provence0">Chirkova et al. (<a href="#ref-chirkova2025provence0" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="lee2024long1context">Lee et al. (<a href="#ref-lee2024long1context" role="doc-biblioref">2024</a>)</span>] Multi-agent systems offer a powerful solution to this problem through a strategy that can be described as context compression. By assigning sub-tasks to specialized agents, the system allows each agent to operate in parallel with its own clean, dedicated context window. For example, a “Literature Searcher” agent’s context is filled only with search queries and retrieved text, while a “Computational Chemist” agent’s context contains only simulation inputs and results. These sub-agents essentially act as filters; they process large amounts of information and then “compress” their findings into concise summaries or structured data. These distilled insights are then passed back to a lead agent or aggregated. This not only dramatically speeds up information gathering but also ensures that the primary reasoning process is not diluted by excessive or irrelevant information, leading to higher quality and more reliable outcomes [<span class="citation" data-cites="Breunig2025HowToFixYourContext">Breunig (<a href="#ref-Breunig2025HowToFixYourContext" role="doc-biblioref">2025</a>)</span>].</p>
</section>
<section id="swarm-intelligence-and-parallel-exploration" class="level4" data-number="5.12.2.4">
<h4 data-number="5.12.2.4" class="anchored" data-anchor-id="swarm-intelligence-and-parallel-exploration"><span class="header-section-number">5.12.2.4</span> Swarm Intelligence and Parallel Exploration</h4>
<p>Inspired by natural systems like ant colonies, some multi-agent approaches use a “swarm” of less-specialized agents to explore a vast problem space in parallel. Instead of assigning fixed roles, a multitude of agents can independently investigate different hypotheses or search different regions of a chemical space. Their collective findings can then be aggregated to identify the most promising solutions. This is particularly powerful for optimization and discovery tasks, such as high-throughput virtual screening or materials design, where the goal is to efficiently search an enormous number of possibilities [<span class="citation" data-cites="chen2023agentversefacilitatingmultiagentcollaboration">W. Chen et al. (<a href="#ref-chen2023agentversefacilitatingmultiagentcollaboration" role="doc-biblioref">2023</a>)</span>].</p>
<p>It is also crucial to distinguish between the foundational model itself (e.g., the GPT-4 <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>) and the “system” with which the user interacts (e.g., ChatGPT). Such systems are not merely the raw model; they incorporate additional layers for safety, prompt management, and some of the adaptation techniques discussed in this section. Understanding this distinction is key to understanding how a static model is transformed into a dynamic and useful tool.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-adilov2021generative" class="csl-entry" role="listitem">
Adilov, Sanjar. 2021. <span>“<span class="nocase">Generative Pre-Training from Molecules</span>.”</span> <em>ChemRxiv Preprint</em>, September. <a href="https://doi.org/10.26434/chemrxiv-2021-5fwjd">https://doi.org/10.26434/chemrxiv-2021-5fwjd</a>.
</div>
<div id="ref-alampara2024mattext" class="csl-entry" role="listitem">
Alampara, Nawaf, Santiago Miret, and Kevin Maik Jablonka. 2024. <span>“<span class="nocase">MatText: Do language models need more than text &amp; scale for materials modeling?</span>”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2406.17295">https://doi.org/10.48550/arXiv.2406.17295</a>.
</div>
<div id="ref-alampara2024probing" class="csl-entry" role="listitem">
Alampara, Nawaf, Mara Schilling-Wilhelmi, Martiño Rı́os-Garcı́a, Indrajeet Mandal, Pranav Khetarpal, Hargun Singh Grover, NM Krishnan, and Kevin Maik Jablonka. 2024. <span>“<span class="nocase">Probing the limitations of multimodal language models for chemistry and materials research</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2411.16955">https://doi.org/10.48550/arXiv.2411.16955</a>.
</div>
<div id="ref-alberts2024unraveling" class="csl-entry" role="listitem">
Alberts, Marvin, Oliver Schilter, Federico Zipoli, Nina Hartrampf, and Teodoro Laino. 2024. <span>“Unraveling Molecular Structure: A Multimodal Spectroscopic Dataset for Chemistry.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2407.17492">https://doi.org/10.48550/arXiv.2407.17492</a>.
</div>
<div id="ref-amin2025towards" class="csl-entry" role="listitem">
Amin, Ishan, Sanjeev Raja, and Aditi Krishnapriyan. 2025. <span>“<span class="nocase">Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2501.09009">https://doi.org/10.48550/arXiv.2501.09009</a>.
</div>
<div id="ref-antunes2024crystal" class="csl-entry" role="listitem">
Antunes, Luis M., Keith T. Butler, and Ricardo Grau-Crespo. 2024. <span>“Crystal Structure Generation with Autoregressive Large Language Modeling.”</span> <em>Nature Communications</em> 15 (1). <a href="https://doi.org/10.1038/s41467-024-54639-7">https://doi.org/10.1038/s41467-024-54639-7</a>.
</div>
<div id="ref-baillargeon2022assessing" class="csl-entry" role="listitem">
Baillargeon, Jean-Thomas, and Luc Lamontagne. 2022. <span>“Assessing the Impact of Sequence Length Learning on Classification Tasks for Transformer Encoder Models.”</span> <em>The Florida AI Research Society</em>. <a href="https://doi.org/10.32473/flairs.37.1.135283">https://doi.org/10.32473/flairs.37.1.135283</a>.
</div>
<div id="ref-batatia2022mace" class="csl-entry" role="listitem">
Batatia, Ilyes, D. Kov’acs, G. Simm, C. Ortner, and Gábor Csányi. 2022. <span>“MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields.”</span> <em>Neural Information Processing Systems</em>. <a href="https://doi.org/10.48550/arXiv.2206.07697">https://doi.org/10.48550/arXiv.2206.07697</a>.
</div>
<div id="ref-Batzner_2022" class="csl-entry" role="listitem">
Batzner, Simon, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P. Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E. Smidt, and Boris Kozinsky. 2022. <span>“<span class="nocase">E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials</span>.”</span> <em>Nature Communications</em> 13 (1). <a href="https://doi.org/10.1038/s41467-022-29939-5">https://doi.org/10.1038/s41467-022-29939-5</a>.
</div>
<div id="ref-bengio2013generalized" class="csl-entry" role="listitem">
Bengio, Yoshua, Li Yao, Guillaume Alain, and Pascal Vincent. 2013. <span>“Generalized Denoising Auto-Encoders as Generative Models.”</span> <em>Advances in Neural Information Processing Systems</em> 26. <a href="https://doi.org/10.48550/arXiv.1305.6663">https://doi.org/10.48550/arXiv.1305.6663</a>.
</div>
<div id="ref-boiko2023autonomous" class="csl-entry" role="listitem">
Boiko, Daniil A, Robert MacKnight, Ben Kline, and Gabe Gomes. 2023. <span>“<span class="nocase">Autonomous chemical research with large language models</span>.”</span> <em>Nature</em> 624 (7992): 570–78. <a href="https://doi.org/10.1038/s41586-023-06792-0">https://doi.org/10.1038/s41586-023-06792-0</a>.
</div>
<div id="ref-bouritsas2022improving" class="csl-entry" role="listitem">
Bouritsas, Giorgos, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein. 2022. <span>“Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 45 (1): 657–68. <a href="https://doi.org/10.1109/TPAMI.2022.3154319">https://doi.org/10.1109/TPAMI.2022.3154319</a>.
</div>
<div id="ref-bran2024augmenting" class="csl-entry" role="listitem">
Bran, Andres M., Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and Philippe Schwaller. 2024. <span>“Augmenting Large Language Models with Chemistry Tools.”</span> <em>Nature Machine Intelligence</em> 6 (5). <a href="https://doi.org/10.1038/s42256-024-00832-8">https://doi.org/10.1038/s42256-024-00832-8</a>.
</div>
<div id="ref-Breunig2025HowToFixYourContext" class="csl-entry" role="listitem">
Breunig, Drew. 2025. <span>“How to Fix Your Context.”</span> <a href="https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html">https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html</a>.
</div>
<div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“<span class="nocase">Language models are few-shot learners</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901. <a href="https://doi.org/10.48550/arXiv.2005.14165">https://doi.org/10.48550/arXiv.2005.14165</a>.
</div>
<div id="ref-Bucior_2019" class="csl-entry" role="listitem">
Bucior, Benjamin J., Andrew S. Rosen, Maciej Haranczyk, Zhenpeng Yao, Michael E. Ziebel, Omar K. Farha, Joseph T. Hupp, J. Ilja Siepmann, Alán Aspuru-Guzik, and Randall Q. Snurr. 2019. <span>“<span class="nocase">Identification Schemes for Metal-Organic Frameworks To Enable Rapid Search and Cheminformatics Analysis</span>.”</span> <em>Crystal Growth &amp; Design</em> 19 (11): 6682–97. <a href="https://doi.org/10.1021/acs.cgd.9b01050">https://doi.org/10.1021/acs.cgd.9b01050</a>.
</div>
<div id="ref-cao2023instructmol0" class="csl-entry" role="listitem">
Cao, He, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. 2023. <span>“InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery.”</span> <em>arXiv Preprint arXiv: 2311.16208</em>. <a href="https://doi.org/10.48550/arXiv.2311.16208">https://doi.org/10.48550/arXiv.2311.16208</a>.
</div>
<div id="ref-caron2018deep" class="csl-entry" role="listitem">
Caron, Mathilde, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. 2018. <span>“<span class="nocase">Deep Clustering for Unsupervised Learning of Visual Features</span>.”</span> <em>arXiv Preprint arXiv: 1807.05520</em>. <a href="https://doi.org/10.48550/arXiv.1807.05520">https://doi.org/10.48550/arXiv.1807.05520</a>.
</div>
<div id="ref-chacko2024spectro" class="csl-entry" role="listitem">
Chacko, Edwin, Rudra Sondhi, Arnav Praveen, Kylie L Luska, and Rodrigo Alejandro Vargas Hernandez. 2024. <span>“Spectro: A Multi-Modal Approach for Molecule Elucidation Using IR and NMR Data.”</span> <em>ChemRxiv Preprint</em>. <a href="https://doi.org/10.26434/chemrxiv-2024-37v2j">https://doi.org/10.26434/chemrxiv-2024-37v2j</a>.
</div>
<div id="ref-chan2024mle" class="csl-entry" role="listitem">
Chan, Jun Shern, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, et al. 2024. <span>“Mle-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering.”</span> <em>arXiv Preprint arXiv:2410.07095</em>. <a href="https://doi.org/10.48550/arXiv.2410.07095">https://doi.org/10.48550/arXiv.2410.07095</a>.
</div>
<div id="ref-chen2023chemist" class="csl-entry" role="listitem">
Chen, Kexin, Junyou Li, Kunyi Wang, Yuyang Du, Jiahui Yu, Jiamin Lu, Lanqing Li, et al. 2023. <span>“Chemist-x: Large Language Model-Empowered Agent for Reaction Condition Recommendation in Chemical Synthesis.”</span> <em>arXiv Preprint arXiv:2311.10776</em>. <a href="https://doi.org/10.48550/arXiv.2311.10776">https://doi.org/10.48550/arXiv.2311.10776</a>.
</div>
<div id="ref-chen2023agentversefacilitatingmultiagentcollaboration" class="csl-entry" role="listitem">
Chen, Weize, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, et al. 2023. <span>“AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2308.10848">https://doi.org/10.48550/arXiv.2308.10848</a>.
</div>
<div id="ref-cheng2023group" class="csl-entry" role="listitem">
Cheng, Austin H, Andy Cai, Santiago Miret, Gustavo Malkomes, Mariano Phielipp, and Alán Aspuru-Guzik. 2023. <span>“Group SELFIES: A Robust Fragment-Based Molecular String Representation.”</span> <em>Digital Discovery</em> 2 (3): 748–58. <a href="https://doi.org/10.1039/D3DD00012E">https://doi.org/10.1039/D3DD00012E</a>.
</div>
<div id="ref-chirkova2025provence0" class="csl-entry" role="listitem">
Chirkova, Nadezhda, Thibault Formal, Vassilina Nikoulina, and Stéphane Clinchant. 2025. <span>“Provence: Efficient and Robust Context Pruning for Retrieval-Augmented Generation.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2501.16214">https://doi.org/10.48550/arXiv.2501.16214</a>.
</div>
<div id="ref-chithrananda2020chemberta" class="csl-entry" role="listitem">
Chithrananda, Seyone, Gabriel Grand, and Bharath Ramsundar. 2020. <span>“<span class="nocase"><span>ChemBERTa</span>: <span>Large</span>-<span>Scale</span> <span>Self</span>-<span>Supervised</span> <span>Pretraining</span> for <span>Molecular</span> <span>Property</span> <span>Prediction</span></span>.”</span> <em>Arxiv</em>, October. <a href="https://doi.org/10.48550/arXiv.2010.09885">https://doi.org/10.48550/arXiv.2010.09885</a>.
</div>
<div id="ref-chowdhery2023palm" class="csl-entry" role="listitem">
Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2023. <span>“Palm: Scaling Language Modeling with Pathways.”</span> <em>Journal of Machine Learning Research</em> 24 (240): 1–113. <a href="https://doi.org/10.48550/arXiv.2204.02311">https://doi.org/10.48550/arXiv.2204.02311</a>.
</div>
<div id="ref-chuang2018comment" class="csl-entry" role="listitem">
Chuang, Kangway V, and Michael J Keiser. 2018. <span>“Comment on <span>‘Predicting Reaction Performance in c–n Cross-Coupling Using Machine Learning’</span>.”</span> <em>Science</em> 362 (6416): eaat8603. <a href="https://doi.org/10.1126/science.aat8603">https://doi.org/10.1126/science.aat8603</a>.
</div>
<div id="ref-dann2015sample" class="csl-entry" role="listitem">
Dann, Christoph, and Emma Brunskill. 2015. <span>“Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning.”</span> <em>Advances in Neural Information Processing Systems</em> 28. <a href="https://doi.org/10.48550/arXiv.1510.08906">https://doi.org/10.48550/arXiv.1510.08906</a>.
</div>
<div id="ref-darvish2025organa" class="csl-entry" role="listitem">
Darvish, Kourosh, Marta Skreta, Yuchi Zhao, Naruki Yoshikawa, Sagnik Som, Miroslav Bogdanovic, Yang Cao, et al. 2025. <span>“ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization.”</span> <em>Matter</em> 8 (2). <a href="https://doi.org/10.1016/j.matt.2024.10.015">https://doi.org/10.1016/j.matt.2024.10.015</a>.
</div>
<div id="ref-dettmers2022gpt3" class="csl-entry" role="listitem">
Dettmers, Tim, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. <span>“Gpt3. Int8 (): 8-Bit Matrix Multiplication for Transformers at Scale.”</span> <em>Advances in Neural Information Processing Systems</em> 35: 30318–32. <a href="https://doi.org/10.48550/arXiv.2208.07339">https://doi.org/10.48550/arXiv.2208.07339</a>.
</div>
<div id="ref-dettmers2023qlora" class="csl-entry" role="listitem">
Dettmers, Tim, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. <span>“Qlora: Efficient Finetuning of Quantized Llms.”</span> <em>Advances in Neural Information Processing Systems</em> 36: 10088–115. <a href="https://doi.org/10.48550/arXiv.2305.14314">https://doi.org/10.48550/arXiv.2305.14314</a>.
</div>
<div id="ref-devlin2018bert0" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. <span>“<span class="nocase">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>.”</span> <em>arXiv Preprint arXiv: 1810.04805</em>. <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>.
</div>
<div id="ref-dinh2022lift" class="csl-entry" role="listitem">
Dinh, Tuan, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dimitris Papailiopoulos, and Kangwook Lee. 2022. <span>“<span class="nocase"><span>LIFT</span>: <span>Language</span>-<span>Interfaced</span> <span>Fine</span>-<span>Tuning</span> for <span>Non</span>-language <span>Machine</span> <span>Learning</span> <span>Tasks</span></span>.”</span> <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em> 35: 11763–84. <a href="https://doi.org/10.48550/arXiv.2206.06565">https://doi.org/10.48550/arXiv.2206.06565</a>.
</div>
<div id="ref-du2023improving" class="csl-entry" role="listitem">
Du, Yilun, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2023. <span>“Improving Factuality and Reasoning in Language Models Through Multiagent Debate.”</span> <em>Forty-First International Conference on Machine Learning</em>. <a href="https://doi.org/10.48550/arXiv.2305.14325">https://doi.org/10.48550/arXiv.2305.14325</a>.
</div>
<div id="ref-edwards2022translation" class="csl-entry" role="listitem">
Edwards, Carl, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. 2022. <span>“Translation Between Molecules and Natural Language.”</span> <em>Arxiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2204.11817">https://doi.org/10.48550/arXiv.2204.11817</a>.
</div>
<div id="ref-Elnaggar_2022" class="csl-entry" role="listitem">
Elnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, et al. 2022. <span>“<span class="nocase">ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning</span>.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 44 (10): 7112–27. <a href="https://doi.org/10.1109/tpami.2021.3095381">https://doi.org/10.1109/tpami.2021.3095381</a>.
</div>
<div id="ref-fedus2022switch" class="csl-entry" role="listitem">
Fedus, William, Barret Zoph, and Noam Shazeer. 2022. <span>“Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.”</span> <em>Journal of Machine Learning Research</em> 23 (120): 1–39. <a href="https://doi.org/10.48550/arXiv.2101.03961">https://doi.org/10.48550/arXiv.2101.03961</a>.
</div>
<div id="ref-ganose2019robocrystallographer" class="csl-entry" role="listitem">
Ganose, Alex M, and Anubhav Jain. 2019. <span>“<span class="nocase">Robocrystallographer: automated crystal structure text descriptions and analysis</span>.”</span> <em>MRS Communications</em> 9 (3): 874–81. <a href="https://doi.org/10.1557/mrc.2019.94">https://doi.org/10.1557/mrc.2019.94</a>.
</div>
<div id="ref-girdhar2023imagebind0" class="csl-entry" role="listitem">
Girdhar, Rohit, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. <span>“ImageBind: One Embedding Space to Bind Them All.”</span> <em>arXiv Preprint arXiv: 2305.05665</em>. <a href="https://doi.org/10.48550/arXiv.2305.05665">https://doi.org/10.48550/arXiv.2305.05665</a>.
</div>
<div id="ref-gruver2024finetuned" class="csl-entry" role="listitem">
Gruver, Nate, Anuroop Sriram, Andrea Madotto, Andrew Gordon Wilson, C. Lawrence Zitnick, and Zachary Ulissi. 2024. <span>“Fine-<span>Tuned</span> <span>Language</span> <span>Models</span> <span>Generate</span> <span>Stable</span> <span>Inorganic</span> <span>Materials</span> as <span>Text</span>.”</span> <em>Arxiv Preprint arXiv: 2402.04379</em>, February. <a href="https://doi.org/10.48550/arXiv.2402.04379">https://doi.org/10.48550/arXiv.2402.04379</a>.
</div>
<div id="ref-gu2023mamba0" class="csl-entry" role="listitem">
Gu, Albert, and Tri Dao. 2023. <span>“Mamba: Linear-Time Sequence Modeling with Selective State Spaces.”</span> <em>arXiv Preprint arXiv: 2312.00752</em>. <a href="https://doi.org/10.48550/arXiv.2312.00752">https://doi.org/10.48550/arXiv.2312.00752</a>.
</div>
<div id="ref-hadsell2006dimensionality" class="csl-entry" role="listitem">
Hadsell, Raia, Sumit Chopra, and Yann LeCun. 2006. <span>“Dimensionality Reduction by Learning an Invariant Mapping.”</span> <em>2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06)</em> 2: 1735–42. <a href="https://doi.org/10.1109/CVPR.2006.100">https://doi.org/10.1109/CVPR.2006.100</a>.
</div>
<div id="ref-hall1991crystallographic" class="csl-entry" role="listitem">
Hall, S. R., F. H. Allen, and I. D. Brown. 1991. <span>“The Crystallographic Information File (<span>CIF</span>): A New Standard Archive File for Crystallography.”</span> <em>Acta Crystallographica Section A</em> 47 (6): 655–85. <a href="https://doi.org/10.1107/S010876739101067X">https://doi.org/10.1107/S010876739101067X</a>.
</div>
<div id="ref-hinton2015distilling" class="csl-entry" role="listitem">
Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. <span>“<span class="nocase">Distilling the knowledge in a neural network</span>.”</span> <em>arXiv Preprint arXiv:1503.02531</em>. <a href="https://doi.org/10.48550/arXiv.1503.02531">https://doi.org/10.48550/arXiv.1503.02531</a>.
</div>
<div id="ref-hochreiter1997long" class="csl-entry" role="listitem">
Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. <span>“Long Short-Term Memory.”</span> <em>Neural Computation</em> 9 (8): 1735–80. <a href="https://doi.org/10.1162/neco.1997.9.8.1735">https://doi.org/10.1162/neco.1997.9.8.1735</a>.
</div>
<div id="ref-hollmann2025accurate" class="csl-entry" role="listitem">
Hollmann, Noah, Samuel Müller, Lennart Purucker, Arjun Krishnakumar, Max Körfer, Shi Bin Hoo, Robin Tibor Schirrmeister, and Frank Hutter. 2025. <span>“Accurate Predictions on Small Data with a Tabular Foundation Model.”</span> <em>Nature</em> 637 (8045): 319–26. <a href="https://doi.org/10.1038/s41586-024-08328-6">https://doi.org/10.1038/s41586-024-08328-6</a>.
</div>
<div id="ref-howard2018universal" class="csl-entry" role="listitem">
Howard, Jeremy, and Sebastian Ruder. 2018. <span>“<span class="nocase">Universal language model fine-tuning for text classification</span>.”</span> <em>arXiv Preprint arXiv:1801.06146</em>. <a href="https://doi.org/10.48550/arXiv.1801.06146">https://doi.org/10.48550/arXiv.1801.06146</a>.
</div>
<div id="ref-hu2022lora" class="csl-entry" role="listitem">
Hu, Edward J, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2022. <span>“Lora: Low-Rank Adaptation of Large Language Models.”</span> <em>ICLR</em> 1 (2): 3. <a href="https://doi.org/10.48550/arXiv.2106.09685">https://doi.org/10.48550/arXiv.2106.09685</a>.
</div>
<div id="ref-Huan2025mathLLM" class="csl-entry" role="listitem">
Huan, Maggie, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, and Xiang Yue. 2025. <span>“Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning.”</span> <em>arXiv Preprint</em>, July. <a href="https://doi.org/10.48550/arXiv.2507.00432">https://doi.org/10.48550/arXiv.2507.00432</a>.
</div>
<div id="ref-jablonka2024leveraging" class="csl-entry" role="listitem">
Jablonka, Kevin Maik, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. 2024. <span>“<span class="nocase">Leveraging large language models for predictive chemistry</span>.”</span> <em>Nature Machine Intelligence</em> 6 (2): 161–69. <a href="https://doi.org/10.1038/s42256-023-00788-1">https://doi.org/10.1038/s42256-023-00788-1</a>.
</div>
<div id="ref-Jha_2018" class="csl-entry" role="listitem">
Jha, Dipendra, Logan Ward, Arindam Paul, Wei-keng Liao, Alok Choudhary, Chris Wolverton, and Ankit Agrawal. 2018. <span>“<span class="nocase">ElemNet: Deep Learning the Chemistry of Materials From Only Elemental Composition</span>.”</span> <em>Scientific Reports</em> 8 (1). <a href="https://doi.org/10.1038/s41598-018-35934-y">https://doi.org/10.1038/s41598-018-35934-y</a>.
</div>
<div id="ref-joshi2025transformers" class="csl-entry" role="listitem">
Joshi, Chaitanya K. 2025. <span>“Transformers Are Graph Neural Networks.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2506.22084">https://doi.org/10.48550/arXiv.2506.22084</a>.
</div>
<div id="ref-krenn2020self" class="csl-entry" role="listitem">
Krenn, Mario, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. 2020. <span>“<span class="nocase">Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation</span>.”</span> <em>Machine Learning: Science and Technology</em> 1 (4): 045024. <a href="https://doi.org/10.1088/2632-2153/aba947">https://doi.org/10.1088/2632-2153/aba947</a>.
</div>
<div id="ref-Langer_2022" class="csl-entry" role="listitem">
Langer, Marcel F., Alex Goeßmann, and Matthias Rupp. 2022. <span>“<span class="nocase">Representations of molecules and materials for interpolation of quantum-mechanical simulations via machine learning</span>.”</span> <em>Npj Computational Materials</em> 8 (1). <a href="https://doi.org/10.1038/s41524-022-00721-x">https://doi.org/10.1038/s41524-022-00721-x</a>.
</div>
<div id="ref-lazaridou2020emergent" class="csl-entry" role="listitem">
Lazaridou, Angeliki, and Marco Baroni. 2020. <span>“Emergent Multi-Agent Communication in the Deep Learning Era.”</span> <em>arXiv Preprint arXiv:2006.02419</em>. <a href="https://doi.org/10.48550/arXiv.2006.02419">https://doi.org/10.48550/arXiv.2006.02419</a>.
</div>
<div id="ref-lee2024long1context" class="csl-entry" role="listitem">
Lee, Jinhyuk, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, et al. 2024. <span>“Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2406.13121">https://doi.org/10.48550/arXiv.2406.13121</a>.
</div>
<div id="ref-lewis2020retrieval" class="csl-entry" role="listitem">
Lewis, Patrick, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, et al. 2020. <span>“Retrieval-Augmented Generation for Knowledge-Intensive Nlp Tasks.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 9459–74. <a href="https://doi.org/10.48550/arXiv.2005.11401">https://doi.org/10.48550/arXiv.2005.11401</a>.
</div>
<div id="ref-li2024seeing" class="csl-entry" role="listitem">
Li, Junxian, Di Zhang, Xunzhi Wang, Zeying Hao, Jingdi Lei, Qian Tan, Cai Zhou, et al. 2024. <span>“Seeing and Understanding: Bridging Vision with Chemical Knowledge via ChemVLM.”</span> <em>arXiv Preprint arXiv: 2408.07246</em>. <a href="https://doi.org/10.48550/arXiv.2408.07246">https://doi.org/10.48550/arXiv.2408.07246</a>.
</div>
<div id="ref-liang2024encouragingdivergentthinkinglarge" class="csl-entry" role="listitem">
Liang, Tian, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2024. <span>“Encouraging Divergent Thinking in Large Language Models Through Multi-Agent Debate.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2305.19118">https://doi.org/10.48550/arXiv.2305.19118</a>.
</div>
<div id="ref-liu2025integrating" class="csl-entry" role="listitem">
Liu, Hongxuan, Haoyu Yin, Zhiyao Luo, and Xiaonan Wang. 2025. <span>“Integrating Chemistry Knowledge in Large Language Models via Prompt Engineering.”</span> <em>Synthetic and Systems Biotechnology</em> 10 (1): 23–38. <a href="https://doi.org/10.1016/j.synbio.2024.07.004">https://doi.org/10.1016/j.synbio.2024.07.004</a>.
</div>
<div id="ref-Liu2023multi0modal" class="csl-entry" role="listitem">
Liu, Shengchao, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang na, Chaowei Xiao na, and Animashree Anandkumar. 2023. <span>“Multi-Modal Molecule Structure-Text Model for Text-Based Retrieval and Editing.”</span> <em>Nature Machine Intelligence</em>. <a href="https://doi.org/10.1038/s42256-023-00759-6">https://doi.org/10.1038/s42256-023-00759-6</a>.
</div>
<div id="ref-liu2023molxpt0" class="csl-entry" role="listitem">
Liu, Zequn, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, and Tie-Yan Liu. 2023. <span>“MolXPT: Wrapping Molecules with Text for Generative Pre-Training.”</span> <em>arXiv Preprint arXiv: 2305.10688</em>. <a href="https://doi.org/10.48550/arXiv.2305.10688">https://doi.org/10.48550/arXiv.2305.10688</a>.
</div>
<div id="ref-liu2024wisdom" class="csl-entry" role="listitem">
Liu, Zichang, Qingyun Liu, Yuening Li, Liang Liu, Anshumali Shrivastava, Shuchao Bi, Lichan Hong, Ed H Chi, and Zhe Zhao. 2024. <span>“Wisdom of Committee: Distilling from Foundation Model to Specialized Application Model.”</span> <em>arXiv Preprint arXiv:2402.14035</em>. <a href="https://doi.org/10.48550/arXiv.2402.14035">https://doi.org/10.48550/arXiv.2402.14035</a>.
</div>
<div id="ref-mahmood2021masked" class="csl-entry" role="listitem">
Mahmood, Omar, Elman Mansimov, Richard Bonneau, and Kyunghyun Cho. 2021. <span>“Masked Graph Modeling for Molecule Generation.”</span> <em>Nature Communications</em> 12 (1): 3156. <a href="https://doi.org/10.1038/s41467-021-23415-2">https://doi.org/10.1038/s41467-021-23415-2</a>.
</div>
<div id="ref-marcus2020next" class="csl-entry" role="listitem">
Marcus, Gary. 2020. <span>“The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence.”</span> <em>arXiv Preprint arXiv:2002.06177</em>. <a href="https://doi.org/10.48550/arXiv.2002.06177">https://doi.org/10.48550/arXiv.2002.06177</a>.
</div>
<div id="ref-micikevicius2017mixed" class="csl-entry" role="listitem">
Micikevicius, Paulius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, et al. 2017. <span>“Mixed Precision Training.”</span> <em>arXiv Preprint arXiv:1710.03740</em>. <a href="https://doi.org/10.48550/arXiv.1710.03740">https://doi.org/10.48550/arXiv.1710.03740</a>.
</div>
<div id="ref-mikolov2013efficient" class="csl-entry" role="listitem">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“<span class="nocase">Efficient Estimation of Word Representations in Vector Space</span>.”</span> <em>arXiv Preprint arXiv: 1301.3781</em>. <a href="https://doi.org/10.48550/arXiv.1301.3781">https://doi.org/10.48550/arXiv.1301.3781</a>.
</div>
<div id="ref-mikolov2013distributed" class="csl-entry" role="listitem">
Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“Distributed Representations of Words and Phrases and Their Compositionality.”</span> <em>Neurips</em>. <a href="https://doi.org/10.48550/arXiv.1310.4546">https://doi.org/10.48550/arXiv.1310.4546</a>.
</div>
<div id="ref-mirza2024large" class="csl-entry" role="listitem">
Mirza, Adrian, Nawaf Alampara, Sreekanth Kunchapu, Martiño Rı́os-Garcı́a, Benedict Emoekabu, Aswanth Krishnan, Tanya Gupta, et al. 2025. <span>“A Framework for Evaluating the Chemical Knowledge and Reasoning Abilities of Large Language Models Against the Expertise of Chemists.”</span> <em>Nature Chemistry</em>, 1–8. <a href="https://doi.org/10.1038/s41557-025-01815-x">https://doi.org/10.1038/s41557-025-01815-x</a>.
</div>
<div id="ref-mirza2024elucidating" class="csl-entry" role="listitem">
Mirza, A., and K. M. Jablonka. 2024. <span>“<span class="nocase">Elucidating Structures from Spectra Using Multimodal Embeddings and Discrete Optimization</span>.”</span> <em>ChemRxiv Preprint</em>. <a href="https://doi.org/10.26434/chemrxiv-2024-f3b18-v2">https://doi.org/10.26434/chemrxiv-2024-f3b18-v2</a>.
</div>
<div id="ref-Musil_2021" class="csl-entry" role="listitem">
Musil, Felix, Andrea Grisafi, Albert P. Bartók, Christoph Ortner, Gábor Csányi, and Michele Ceriotti. 2021. <span>“<span class="nocase">Physics-Inspired Structural Representations for Molecules and Materials</span>.”</span> <em>Chemical Reviews</em> 121 (16): 9759–9815. <a href="https://doi.org/10.1021/acs.chemrev.1c00021">https://doi.org/10.1021/acs.chemrev.1c00021</a>.
</div>
<div id="ref-narayanan2025training" class="csl-entry" role="listitem">
Narayanan, Siddharth M., James D. Braza, Ryan-Rhys Griffiths, Albert Bou, Geemi Wellawatte, Mayk Caldas Ramos, Ludovico Mitchener, Samuel G. Rodriques, and Andrew D. White. 2025. <span>“Training a Scientific Reasoning Model for Chemistry.”</span> <em>arXiv Preprint arXiv: 2506.17238</em>. <a href="https://doi.org/10.48550/arXiv.2506.17238">https://doi.org/10.48550/arXiv.2506.17238</a>.
</div>
<div id="ref-ni2024pre" class="csl-entry" role="listitem">
Ni, Yuyan, Shikun Feng, Xin Hong, Yuancheng Sun, Wei-Ying Ma, Zhi-Ming Ma, Qiwei Ye, and Yanyan Lan. 2024. <span>“Pre-Training with Fractional Denoising to Enhance Molecular Property Prediction.”</span> <em>Nature Machine Intelligence</em> 6 (10): 1169–78. <a href="https://doi.org/10.1038/s42256-024-00900-z">https://doi.org/10.1038/s42256-024-00900-z</a>.
</div>
<div id="ref-oord2018representation" class="csl-entry" role="listitem">
Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. 2018. <span>“<span class="nocase">Representation Learning with Contrastive Predictive Coding</span>.”</span> <em>arXiv Preprint arXiv: 1807.03748</em>. <a href="https://doi.org/10.48550/arXiv.1807.03748">https://doi.org/10.48550/arXiv.1807.03748</a>.
</div>
<div id="ref-openai2023gpt04" class="csl-entry" role="listitem">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, et al. 2023. <span>“<span>GPT-4 Technical Report</span>.”</span> <em>arXiv Preprint arXiv: 2303.08774</em>. <a href="https://doi.org/10.48550/arXiv.2303.08774">https://doi.org/10.48550/arXiv.2303.08774</a>.
</div>
<div id="ref-ouyang2022training" class="csl-entry" role="listitem">
Ouyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. <span>“Training Language Models to Follow Instructions with Human Feedback.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2203.02155">https://doi.org/10.48550/arXiv.2203.02155</a>.
</div>
<div id="ref-parisi2022talm" class="csl-entry" role="listitem">
Parisi, Aaron, Yao Zhao, and Noah Fiedel. 2022. <span>“Talm: Tool Augmented Language Models.”</span> <em>arXiv Preprint arXiv:2205.12255</em>. <a href="https://doi.org/10.48550/arXiv.2205.12255">https://doi.org/10.48550/arXiv.2205.12255</a>.
</div>
<div id="ref-qian2024chatdevcommunicativeagentssoftware" class="csl-entry" role="listitem">
Qian, Chen, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, et al. 2024. <span>“ChatDev: Communicative Agents for Software Development.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2307.07924">https://doi.org/10.48550/arXiv.2307.07924</a>.
</div>
<div id="ref-radford2019language" class="csl-entry" role="listitem">
Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. <span>“Language Models Are Unsupervised Multitask Learners.”</span> Technical Report TR-2019-1. San Francisco, CA: OpenAI. <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a>.
</div>
<div id="ref-reiser2022graph" class="csl-entry" role="listitem">
Reiser, Patrick, Marlen Neubert, André Eberhard, Luca Torresi, Chen Zhou, Chen Shao, Houssam Metni, et al. 2022. <span>“Graph Neural Networks for Materials Science and Chemistry.”</span> <em>Communications Materials</em> 3 (1): 93. <a href="https://doi.org/10.48550/arXiv.2208.09481">https://doi.org/10.48550/arXiv.2208.09481</a>.
</div>
<div id="ref-Rives_2021" class="csl-entry" role="listitem">
Rives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. <span>“<span class="nocase">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</span>.”</span> <em>Proceedings of the National Academy of Sciences</em> 118 (15). <a href="https://doi.org/10.1073/pnas.2016239118">https://doi.org/10.1073/pnas.2016239118</a>.
</div>
<div id="ref-Ruffolo_2024" class="csl-entry" role="listitem">
Ruffolo, Jeffrey A., and Ali Madani. 2024. <span>“<span class="nocase">Designing proteins with language models</span>.”</span> <em>Nature Biotechnology</em> 42 (2): 200–202. <a href="https://doi.org/10.1038/s41587-024-02123-4">https://doi.org/10.1038/s41587-024-02123-4</a>.
</div>
<div id="ref-sanchez2023cloome" class="csl-entry" role="listitem">
Sanchez-Fernandez, Ana, Elisabeth Rumetshofer, Sepp Hochreiter, and Günter Klambauer. 2023. <span>“CLOOME: Contrastive Learning Unlocks Bioimaging Databases for Queries with Chemical Structures.”</span> <em>Nature Communications</em> 14 (1): 7339. <a href="https://doi.org/10.1038/s41467-023-42328-w">https://doi.org/10.1038/s41467-023-42328-w</a>.
</div>
<div id="ref-sanh2019distilbert" class="csl-entry" role="listitem">
Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. <span>“DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter.”</span> <em>arXiv Preprint arXiv:1910.01108</em>. <a href="https://doi.org/10.48550/arXiv.1910.01108">https://doi.org/10.48550/arXiv.1910.01108</a>.
</div>
<div id="ref-satorras2021n" class="csl-entry" role="listitem">
Satorras, Vıctor Garcia, Emiel Hoogeboom, and Max Welling. 2021. <span>“<span class="nocase">E (n) equivariant graph neural networks</span>.”</span> <em>International Conference on Machine Learning</em>, 9323–32. <a href="https://doi.org/10.48550/arXiv.2102.09844">https://doi.org/10.48550/arXiv.2102.09844</a>.
</div>
<div id="ref-schick2023toolformer" class="csl-entry" role="listitem">
Schick, Timo, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. <span>“Toolformer: Language Models Can Teach Themselves to Use Tools.”</span> <em>Advances in Neural Information Processing Systems</em> 36: 68539–51. <a href="https://doi.org/10.48550/arXiv.2302.04761">https://doi.org/10.48550/arXiv.2302.04761</a>.
</div>
<div id="ref-SchmidingerSSSH25" class="csl-entry" role="listitem">
Schmidinger, Niklas, Lisa Schneckenreiter, Philipp Seidl, Johannes Schimunek, Pieter-Jan Hoedt, Johannes Brandstetter, Andreas Mayr, Sohvi Luukkonen, Sepp Hochreiter, and Günter Klambauer. 2025. <span>“Bio-xLSTM: Generative Modeling, Representation and in-Context Learning of Biological and Chemical Sequences.”</span> <em>The Thirteenth International Conference on Learning Representations, <span>ICLR</span></em>. <a href="https://doi.org/10.48550/arXiv.2411.04165">https://doi.org/10.48550/arXiv.2411.04165</a>.
</div>
<div id="ref-schulman2017proximal" class="csl-entry" role="listitem">
Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. <span>“Proximal Policy Optimization Algorithms.”</span> <em>arXiv Preprint arXiv: 1707.06347</em>. <a href="https://doi.org/10.48550/arXiv.1707.06347">https://doi.org/10.48550/arXiv.1707.06347</a>.
</div>
<div id="ref-schwaller2019molecular" class="csl-entry" role="listitem">
Schwaller, Philippe, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. 2019. <span>“Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction.”</span> <em>ACS Central Science</em> 5 (9): 1572–83. <a href="https://doi.org/10.1021/acscentsci.9b00576">https://doi.org/10.1021/acscentsci.9b00576</a>.
</div>
<div id="ref-shazeer2017outrageously" class="csl-entry" role="listitem">
Shazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. <span>“Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.”</span> <em>arXiv Preprint arXiv:1701.06538</em>. <a href="https://doi.org/10.48550/arXiv.1701.06538">https://doi.org/10.48550/arXiv.1701.06538</a>.
</div>
<div id="ref-skarlinski2024language" class="csl-entry" role="listitem">
Skarlinski, Michael D, Sam Cox, Jon M Laurent, James D Braza, Michaela Hinks, Michael J Hammerling, Manvitha Ponnapati, Samuel G Rodriques, and Andrew D White. 2024. <span>“Language Agents Achieve Superhuman Synthesis of Scientific Knowledge.”</span> <em>arXiv Preprint arXiv:2409.13740</em>. <a href="https://doi.org/10.48550/arXiv.2409.13740">https://doi.org/10.48550/arXiv.2409.13740</a>.
</div>
<div id="ref-shirasuna2024multi" class="csl-entry" role="listitem">
Soares, Eduardo, Victor Yukio Shirasuna, Emilio Vital Brazil, Indra Priyadarsini, and Seiji Takeda. 2025. <span>“Multi-View Mixture-of-Experts for Predicting Molecular Properties Using SMILES, SELFIES, and Graph-Based Representations.”</span> <em>Machine Learning: Science and Technology</em> 6 (June): 025070. <a href="https://doi.org/10.1088/2632-2153/ade4ef">https://doi.org/10.1088/2632-2153/ade4ef</a>.
</div>
<div id="ref-soares2025mamba-based" class="csl-entry" role="listitem">
Soares, Eduardo, Emilio Vital Brazil, Victor Shirasuna, Dmitry Zubarev, Renato Cerqueira, and Kristin Schmidt. 2025. <span>“A Mamba-Based Foundation Model for Materials.”</span> <em>Npj Artificial Intelligence</em> 1 (1): 1–8. <a href="https://doi.org/10.1038/s44387-025-00009-7">https://doi.org/10.1038/s44387-025-00009-7</a>.
</div>
<div id="ref-sun2024scidfm" class="csl-entry" role="listitem">
Sun, Liangtai, Danyu Luo, Da Ma, Zihan Zhao, Baocai Chen, Zhennan Shen, Su Zhu, Lu Chen, Xin Chen, and Kai Yu. 2024. <span>“SciDFM: A Large Language Model with Mixture-of-Experts for Science.”</span> <em>arXiv Preprint arXiv:2409.18412</em>. <a href="https://doi.org/10.48550/arXiv.2409.18412">https://doi.org/10.48550/arXiv.2409.18412</a>.
</div>
<div id="ref-takeda2023multi" class="csl-entry" role="listitem">
Takeda, Seiji, Indra Priyadarsini, Akihiro Kishimoto, Hajime Shinohara, Lisa Hamada, Hirose Masataka, Junta Fuchiwaki, and Daiju Nakano. 2023. <span>“Multi-Modal Foundation Model for Material Design.”</span> <em>AI for Accelerated Materials Design-NeurIPS 2023 Workshop</em>. <a href="https://openreview.net/forum?id=EiT2bLsfM9">https://openreview.net/forum?id=EiT2bLsfM9</a>.
</div>
<div id="ref-taylor2022galactica" class="csl-entry" role="listitem">
Taylor, Ross, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. <span>“Galactica: A Large Language Model for Science.”</span> <em>arXiv Preprint arXiv:2211.09085</em>. <a href="https://doi.org/10.48550/arXiv.2211.09085">https://doi.org/10.48550/arXiv.2211.09085</a>.
</div>
<div id="ref-tian2022information" class="csl-entry" role="listitem">
Tian, Siyu Isaac Parker, Aron Walsh, Zekun Ren, Qianxiao Li, and Tonio Buonassisi. 2022. <span>“<span class="nocase">What Information is Necessary and Sufficient to Predict Materials Properties using Machine Learning?</span>”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2206.04968">https://doi.org/10.48550/arXiv.2206.04968</a>.
</div>
<div id="ref-Tshitoyan_2019" class="csl-entry" role="listitem">
Tshitoyan, Vahe, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin A. Persson, Gerbrand Ceder, and Anubhav Jain. 2019. <span>“Unsupervised Word Embeddings Capture Latent Knowledge from Materials Science Literature.”</span> <em>Nature</em> 571 (7763): 95–98. <a href="https://doi.org/10.1038/s41586-019-1335-8">https://doi.org/10.1038/s41586-019-1335-8</a>.
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>NEURIPS</em>. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
<div id="ref-velivckovic2023everything" class="csl-entry" role="listitem">
Veličković, Petar. 2023. <span>“Everything Is Connected: Graph Neural Networks.”</span> <em>Current Opinion in Structural Biology</em> 79: 102538. <a href="https://doi.org/10.1016/j.sbi.2023.102538">https://doi.org/10.1016/j.sbi.2023.102538</a>.
</div>
<div id="ref-vincent2008extracting" class="csl-entry" role="listitem">
Vincent, Pascal, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. <span>“Extracting and Composing Robust Features with Denoising Autoencoders.”</span> <em>Proceedings of the 25th International Conference on Machine Learning</em>, 1096–1103. <a href="https://doi.org/10.1145/1390156.1390294">https://doi.org/10.1145/1390156.1390294</a>.
</div>
<div id="ref-vincent2010stacked" class="csl-entry" role="listitem">
Vincent, Pascal, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and Léon Bottou. 2010. <span>“<span class="nocase">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</span>.”</span> <em>Journal of Machine Learning Research</em> 11 (12). <a href="https://jmlr.org/papers/v11/vincent10a.html">https://jmlr.org/papers/v11/vincent10a.html</a>.
</div>
<div id="ref-von2023transformers" class="csl-entry" role="listitem">
Von Oswald, Johannes, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2023. <span>“Transformers Learn in-Context by Gradient Descent.”</span> <em>International Conference on Machine Learning</em>, 35151–74. <a href="https://doi.org/10.48550/arXiv.2212.07677">https://doi.org/10.48550/arXiv.2212.07677</a>.
</div>
<div id="ref-Wang_2021" class="csl-entry" role="listitem">
Wang, Anthony Yu-Tung, Steven K. Kauwe, Ryan J. Murdock, and Taylor D. Sparks. 2021. <span>“<span class="nocase">Compositionally restricted attention-based network for materials property predictions</span>.”</span> <em>Npj Computational Materials</em> 7 (1). <a href="https://doi.org/10.1038/s41524-021-00545-1">https://doi.org/10.1038/s41524-021-00545-1</a>.
</div>
<div id="ref-wang2023cmolgpt" class="csl-entry" role="listitem">
Wang, Ye, Honggang Zhao, Simone Sciabola, and Wenlu Wang. 2023. <span>“cMolGPT: A Conditional Generative Pre-Trained Transformer for Target-Specific de Novo Molecular Generation.”</span> <em>Molecules</em> 28 (11): 4430. <a href="https://doi.org/10.3390/molecules28114430">https://doi.org/10.3390/molecules28114430</a>.
</div>
<div id="ref-wang2022molecular" class="csl-entry" role="listitem">
Wang, Yuyang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. 2022. <span>“Molecular Contrastive Learning of Representations via Graph Neural Networks.”</span> <em>Nature Machine Intelligence</em> 4 (3): 279–87. <a href="https://doi.org/10.1038/s42256-022-00447-x">https://doi.org/10.1038/s42256-022-00447-x</a>.
</div>
<div id="ref-wang2023denoise" class="csl-entry" role="listitem">
Wang, Yuyang, Changwen Xu, Zijie Li, and Amir Barati Farimani. 2023. <span>“Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable Neural Potentials.”</span> <em>Journal of Chemical Theory and Computation</em> 19 (15): 5077–87. <a href="https://doi.org/10.1021/acs.jctc.3c00289">https://doi.org/10.1021/acs.jctc.3c00289</a>.
</div>
<div id="ref-wei2025browsecomp" class="csl-entry" role="listitem">
Wei, Jason, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025. <span>“Browsecomp: A Simple yet Challenging Benchmark for Browsing Agents.”</span> <em>arXiv Preprint arXiv:2504.12516</em>. <a href="https://doi.org/10.48550/arXiv.2504.12516">https://doi.org/10.48550/arXiv.2504.12516</a>.
</div>
<div id="ref-wei2022chain" class="csl-entry" role="listitem">
Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. <span>“Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.”</span> <em>Advances in Neural Information Processing Systems</em> 35: 24824–37. <a href="https://doi.org/10.48550/arXiv.2201.11903">https://doi.org/10.48550/arXiv.2201.11903</a>.
</div>
<div id="ref-weininger1988smiles" class="csl-entry" role="listitem">
Weininger, David. 1988. <span>“<span>SMILES</span>, a Chemical Language and Information System. 1. <span>Introduction</span> to Methodology and Encoding Rules.”</span> <em>Journal of Chemical Information and Computer Sciences</em> 28 (1). <a href="https://doi.org/10.1021/ci00057a005">https://doi.org/10.1021/ci00057a005</a>.
</div>
<div id="ref-weng2022vlm" class="csl-entry" role="listitem">
Weng, Lilian. 2022. <span>“Generalized Visual Language Models.”</span> <em>Lil’Log</em>, June. <a href="https://lilianweng.github.io/posts/2022-06-09-vlm/">https://lilianweng.github.io/posts/2022-06-09-vlm/</a>.
</div>
<div id="ref-wood2025uma0" class="csl-entry" role="listitem">
Wood, Brandon M., Misko Dzamba, Xiang Fu, Meng Gao, Muhammed Shuaibi, Luis Barroso-Luque, Kareem Abdelmaqsoud, et al. 2025. <span>“UMA: A Family of Universal Models for Atoms.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2506.23971">https://doi.org/10.48550/arXiv.2506.23971</a>.
</div>
<div id="ref-wu2023autogen" class="csl-entry" role="listitem">
Wu, Qingyun, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, et al. 2023. <span>“Autogen: Enabling Next-Gen Llm Applications via Multi-Agent Conversation.”</span> <em>arXiv Preprint arXiv:2308.08155</em>. <a href="https://doi.org/10.48550/arXiv.2308.08155">https://doi.org/10.48550/arXiv.2308.08155</a>.
</div>
<div id="ref-Xiao_2023" class="csl-entry" role="listitem">
Xiao, Hang, Rong Li, Xiaoyang Shi, Yan Chen, Liangliang Zhu, Xi Chen, and Lei Wang. 2023. <span>“<span class="nocase">An invertible, invariant crystal representation for inverse design of solid-state materials using generative deep learning</span>.”</span> <em>Nature Communications</em> 14 (1). <a href="https://doi.org/10.1038/s41467-023-42870-7">https://doi.org/10.1038/s41467-023-42870-7</a>.
</div>
<div id="ref-xu2025towards" class="csl-entry" role="listitem">
Xu, Fengli, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, et al. 2025. <span>“Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2501.09686">https://doi.org/10.48550/arXiv.2501.09686</a>.
</div>
<div id="ref-yao2023react" class="csl-entry" role="listitem">
Yao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. <span>“React: Synergizing Reasoning and Acting in Language Models.”</span> <em>International Conference on Learning Representations (ICLR)</em>. <a href="https://doi.org/10.48550/arXiv.2210.03629">https://doi.org/10.48550/arXiv.2210.03629</a>.
</div>
<div id="ref-zhang2025scientific" class="csl-entry" role="listitem">
Zhang, Qiang, Keyan Ding, Tianwen Lv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, et al. 2025. <span>“Scientific Large Language Models: A Survey on Biological &amp; Chemical Domains.”</span> <em>ACM Computing Surveys</em> 57 (6): 1–38. <a href="https://doi.org/10.1145/3715318">https://doi.org/10.1145/3715318</a>.
</div>
<div id="ref-zheng2023chatgpt" class="csl-entry" role="listitem">
Zheng, Zhiling, Oufan Zhang, C. Borgs, J. Chayes, and O. Yaghi. 2023. <span>“ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis.”</span> <em>Journal of the American Chemical Society</em>. <a href="https://doi.org/10.1021/jacs.3c05819">https://doi.org/10.1021/jacs.3c05819</a>.
</div>
<div id="ref-zhou2023algorithms" class="csl-entry" role="listitem">
Zhou, Hattie, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. 2023. <span>“What Algorithms Can Transformers Learn? A Study in Length Generalization.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2310.16028">https://doi.org/10.48550/arXiv.2310.16028</a>.
</div>
<div id="ref-Zou2025ElAgente" class="csl-entry" role="listitem">
Zou, Yunheng, Austin H. Cheng, Abdulrahman Aldossary, Jiaru Bai, Shi Xuan Leong, Jorge Arturo Campos-Gonzalez-Angulo, Changhyeok Choi, et al. 2025. <span>“El Agente: An Autonomous Agent for Quantum Chemistry.”</span> <em>Matter</em> 8 (7): 102263. <a href="https://doi.org/10.1016/j.matt.2025.102263">https://doi.org/10.1016/j.matt.2025.102263</a>.
</div>
</div>
</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02-data_taxonomy.html" class="pagination-link" aria-label="The Shape and Structure of Chemical Data">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Shape and Structure of Chemical Data</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04-evals.html" class="pagination-link" aria-label="Evaluations">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Evaluations</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>