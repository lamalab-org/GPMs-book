<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Evaluations – General Purpose Models for the Chemical Sciences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05-applications.html" rel="next">
<link href="./03-architectures.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-fffb2cfd06bc0bcd22fa5e79382abad9.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./04-evals.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Evaluations</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">General Purpose Models for the Chemical Sciences</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">General purpose models for the chemical sciences</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-data_taxonomy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Shape and Structure of Chemical Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Building Principles of GPMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-evals.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Evaluations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-accelerating_applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Accelerating Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-safety.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Implications of GPMs: Education, Safety, and Ethics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-outlook_conclusions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Outlook and Conclusions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <img src="https://raw.githubusercontent.com/lamalab-org/lamalab.github.io/main/static/png-file.png" alt="LamaLab logo">
    <h2 class="anchored">LamaLab - Lab for AI in Materials Science</h2>
    <ul>
      <li>
        <a href="https://lamalab.org/" class="nav-link active" data-scroll-target="https\://lamalab.org/"><strong>🌐 Visit our website</strong></a>
      </li>
      <li>
        <a href="https://x.com/jablonkagroup" class="nav-link" data-scroll-target="https\://x.com/jablonkagroup"><strong>🐦 Follow us on X (Twitter)</strong></a>
      </li>
      <li>
        <a href="" class="nav-link" data-scroll-target=""><strong>📄 Read our publications</strong></a>
      </li>
      <li>
        <a href="mailto:mail@kjablonka.com" class="nav-link" data-scroll-target="mailto\:mail@kjablonka.com"><strong>✉️ Contact us</strong></a>
      </li>
    </ul>
    <hr>
        <h2 id="toc-title">On this page</h2>
        <ul>
        <li><a href="#sec:evals" id="toc-sec:evals" class="nav-link" data-scroll-target="#sec\:evals"><span class="header-section-number">6</span> Evaluations</a>
        <ul>
        <li><a href="#the-evolution-of-model-evaluation" id="toc-the-evolution-of-model-evaluation" class="nav-link" data-scroll-target="#the-evolution-of-model-evaluation"><span class="header-section-number">6.1</span> The Evolution of Model Evaluation</a></li>
        <li><a href="#sec:eval_design" id="toc-sec:eval_design" class="nav-link" data-scroll-target="#sec\:eval_design"><span class="header-section-number">6.2</span> Design of Evaluations</a>
        <ul>
        <li><a href="#desired-properties-for-evaluations" id="toc-desired-properties-for-evaluations" class="nav-link" data-scroll-target="#desired-properties-for-evaluations"><span class="header-section-number">6.2.1</span> Desired Properties for Evaluations</a>
        <ul class="collapse">
        <li><a href="#data-and-biases" id="toc-data-and-biases" class="nav-link" data-scroll-target="#data-and-biases"><span class="header-section-number">6.2.1.1</span> Data and Biases</a></li>
        <li><a href="#scoring-mechanism" id="toc-scoring-mechanism" class="nav-link" data-scroll-target="#scoring-mechanism"><span class="header-section-number">6.2.1.2</span> Scoring Mechanism</a></li>
        <li><a href="#statistical-significance-and-uncertainty-estimation" id="toc-statistical-significance-and-uncertainty-estimation" class="nav-link" data-scroll-target="#statistical-significance-and-uncertainty-estimation"><span class="header-section-number">6.2.1.3</span> Statistical significance and uncertainty estimation</a></li>
        <li><a href="#reproducibility-and-reporting" id="toc-reproducibility-and-reporting" class="nav-link" data-scroll-target="#reproducibility-and-reporting"><span class="header-section-number">6.2.1.4</span> Reproducibility and Reporting</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#sec:eval_methods" id="toc-sec:eval_methods" class="nav-link" data-scroll-target="#sec\:eval_methods"><span class="header-section-number">6.3</span> Evaluation Methodologies</a>
        <ul>
        <li><a href="#representational-vs.-pragmatic-evaluations" id="toc-representational-vs.-pragmatic-evaluations" class="nav-link" data-scroll-target="#representational-vs.-pragmatic-evaluations"><span class="header-section-number">6.3.1</span> Representational vs.&nbsp;Pragmatic Evaluations</a>
        <ul class="collapse">
        <li><a href="#estimator-types" id="toc-estimator-types" class="nav-link" data-scroll-target="#estimator-types"><span class="header-section-number">6.3.1.1</span> Estimator Types</a></li>
        <li><a href="#para:trad_benchmarks" id="toc-para:trad_benchmarks" class="nav-link" data-scroll-target="#para\:trad_benchmarks"><span class="header-section-number">6.3.1.2</span> Traditional Benchmarks</a></li>
        <li><a href="#challenges-and-competitions" id="toc-challenges-and-competitions" class="nav-link" data-scroll-target="#challenges-and-competitions"><span class="header-section-number">6.3.1.3</span> Challenges and Competitions</a></li>
        <li><a href="#para:red_teaming" id="toc-para:red_teaming" class="nav-link" data-scroll-target="#para\:red_teaming"><span class="header-section-number">6.3.1.4</span> Red Teaming and Capability Discovery</a></li>
        <li><a href="#real-world-deployment-studies" id="toc-real-world-deployment-studies" class="nav-link" data-scroll-target="#real-world-deployment-studies"><span class="header-section-number">6.3.1.5</span> Real-World Deployment Studies</a></li>
        <li><a href="#ablation-studies-and-systematic-testing" id="toc-ablation-studies-and-systematic-testing" class="nav-link" data-scroll-target="#ablation-studies-and-systematic-testing"><span class="header-section-number">6.3.1.6</span> Ablation Studies and Systematic Testing</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#future-directions" id="toc-future-directions" class="nav-link" data-scroll-target="#future-directions"><span class="header-section-number">6.4</span> Future Directions</a>
        <ul>
        <li><a href="#emerging-evaluations-needs" id="toc-emerging-evaluations-needs" class="nav-link" data-scroll-target="#emerging-evaluations-needs"><span class="header-section-number">6.4.1</span> Emerging Evaluations Needs</a>
        <ul class="collapse">
        <li><a href="#standardization-efforts" id="toc-standardization-efforts" class="nav-link" data-scroll-target="#standardization-efforts"><span class="header-section-number">6.4.1.1</span> Standardization Efforts</a></li>
        </ul></li>
        </ul></li>
        </ul></li>
        </ul>
  </nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Evaluations</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>




  <meta charset="utf-8">
  <title>LamaLab - Lab for AI in Materials Science</title>
  <style>
    /* Simple styling to keep it clean and readable */
    nav ul { list-style-type: none; padding-left: 0; }
    nav li { margin: 0.5rem 0; }
    nav a { text-decoration: none; }
    nav img { max-width: 200px; height: auto; display: block; margin: 0 auto 1rem; }
  </style>


  


<section id="sec:evals" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Evaluations</h1>
<section id="the-evolution-of-model-evaluation" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="the-evolution-of-model-evaluation"><span class="header-section-number">6.1</span> The Evolution of Model Evaluation</h2>
<p>Assessing modern <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> is challenging due to their broad applicability across diverse domains. Unlike traditional models, which are designed for specific tasks and can be directly tested on well-defined objectives [<span class="citation" data-cites="raschka2018model">Raschka (<a href="09-references.html#ref-raschka2018model" role="doc-biblioref">2018</a>)</span>], it is impractical to evaluate <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> on every possible capability. As a result, many evaluations rely on structured benchmarks that measure proficiency in key areas such as mathematics, chemistry, and language understanding [<span class="citation" data-cites="tikhonov2023post">Tikhonov and Yamshchikov (<a href="09-references.html#ref-tikhonov2023post" role="doc-biblioref">2023</a>)</span>]. However, such benchmarks often fall short in capturing open-ended problem-solving or emergent abilities that arise without explicit training for them and are sensitive to factors such as prompt phrasing and task framing [<span class="citation" data-cites="Siska2024">Siska et al. (<a href="09-references.html#ref-Siska2024" role="doc-biblioref">2024</a>)</span>].</p>
<p>Early benchmarks primarily focused on evaluating specialized models based on their ability to predict molecular properties from molecular structures. [<span class="citation" data-cites="wu2018moleculenet">Wu et al. (<a href="09-references.html#ref-wu2018moleculenet" role="doc-biblioref">2018</a>)</span>] While useful, these evaluations largely emphasized numerical accuracy on the isolated tasks the models were fine-tuned on, without probing the more complex reasoning or generative capabilities that <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> aim to capture. Over time, this evolution expanded to exam-like problem-solving, assessing structured tasks similar to those found in academic chemistry courses. [<span class="citation" data-cites="zaki2023mascqa0">Zaki et al. (<a href="09-references.html#ref-zaki2023mascqa0" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="li2023camel">Li et al. (<a href="09-references.html#ref-li2023camel" role="doc-biblioref">2023</a>)</span>] More recent efforts aim to evaluate a broader range of skills, including knowledge retrieval, logical reasoning, and even the ability to mimic human intuition when solving complex chemical problems.[<span class="citation" data-cites="feng2024sciknoweval0">Feng et al. (<a href="09-references.html#ref-feng2024sciknoweval0" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="mirza2024large">Mirza et al. (<a href="09-references.html#ref-mirza2024large" role="doc-biblioref">2025</a>)</span>] This shift highlights the need for more flexible evaluation methods that consider the specific context and nature of each task. Rather than relying solely on static benchmarks, there is a growing demand for assessments that dynamically account for the diversity of chemical tasks and the specific capabilities required to solve them—mirroring the multifaceted potential of <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> in chemistry. <a href="#tab:chemistry_benchmarks,fig:benchmarks" data-reference-type="ref+Label" data-reference="tab:chemistry_benchmarks,fig:benchmarks">[tab:chemistry_benchmarks,fig:benchmarks]</a> gives an overview of some benchmarks that have been used in the chemical sciences.</p>
<div id="tab:chemistry_benchmarks">
<table class="caption-top table">
<caption><strong>Non-comprehensive overview of chemistry benchmarks</strong>. Overview of chemistry benchmarks including the topics covered, the curation method (automated, using <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, manual), and the number of questions. We limit our scope here to benchmarks (and exclude other evaluation methods), since they constitute the most actively used and publicly available resources in the field at present.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Benchmark name</strong></th>
<th style="text-align: left;"><strong>Overall topic</strong></th>
<th style="text-align: left;"><strong>Curation method</strong></th>
<th style="text-align: left;"><strong>Count</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">CAMEL - Chemistry [<span class="citation" data-cites="li2023camel">Li et al. (<a href="09-references.html#ref-li2023camel" role="doc-biblioref">2023</a>)</span>]</td>
<td style="text-align: left;">General Chemistry <span data-acronym-label="mcq" data-acronym-form="singular+short">mcq</span></td>
<td style="text-align: left;">A, L</td>
<td style="text-align: left;">20 K</td>
</tr>
<tr class="even">
<td style="text-align: left;">ChemBench [<span class="citation" data-cites="mirza2024large">Mirza et al. (<a href="09-references.html#ref-mirza2024large" role="doc-biblioref">2025</a>)</span>]</td>
<td style="text-align: left;">General Chemistry <span data-acronym-label="mcq" data-acronym-form="singular+short">mcq</span>, Reasoning</td>
<td style="text-align: left;">M</td>
<td style="text-align: left;">2.7 K</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ChemIQ [<span class="citation" data-cites="runcie2025assessing">Runcie, Deane, and Imrie (<a href="09-references.html#ref-runcie2025assessing" role="doc-biblioref">2025</a>)</span>]</td>
<td style="text-align: left;">Molecule Naming, Reasoning, Reaction Generation, Spectrum Interpretation</td>
<td style="text-align: left;">A</td>
<td style="text-align: left;">796</td>
</tr>
<tr class="even">
<td style="text-align: left;">ChemLLM [<span class="citation" data-cites="zhang2024chemllm">Zhang et al. (<a href="09-references.html#ref-zhang2024chemllm" role="doc-biblioref">2024</a>)</span>]</td>
<td style="text-align: left;">Molecule Naming, Property Prediction, Reaction Prediction, Reaction Conditions Prediction, Molecule &amp; Reaction Generation, Molecule Description</td>
<td style="text-align: left;">A, L</td>
<td style="text-align: left;">4.1 K</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ChemLLMBench [<span class="citation" data-cites="guo2023large">T. Guo et al. (<a href="09-references.html#ref-guo2023large" role="doc-biblioref">2023</a>)</span>]</td>
<td style="text-align: left;">Molecule Naming, Property Prediction, Reaction Prediction, Reaction Conditions Prediction, Molecule &amp; Reaction Generation</td>
<td style="text-align: left;">A, L</td>
<td style="text-align: left;">800</td>
</tr>
<tr class="even">
<td style="text-align: left;">LAB-Bench [<span class="citation" data-cites="laurent2024lab0bench0">Laurent et al. (<a href="09-references.html#ref-laurent2024lab0bench0" role="doc-biblioref">2024</a>)</span>]</td>
<td style="text-align: left;">Information Extraction, Reasoning, Molecule &amp; Reaction Generation</td>
<td style="text-align: left;">A, M</td>
<td style="text-align: left;">2.5 K</td>
</tr>
<tr class="odd">
<td style="text-align: left;">LabSafety Bench [<span class="citation" data-cites="zhou2024labsafety">Zhou et al. (<a href="09-references.html#ref-zhou2024labsafety" role="doc-biblioref">2024</a>)</span>]</td>
<td style="text-align: left;">Lab Safety, Experimental Chemistry</td>
<td style="text-align: left;">M, L</td>
<td style="text-align: left;">765</td>
</tr>
<tr class="even">
<td style="text-align: left;">LlaSMol [<span class="citation" data-cites="yu2024llasmol">Yu et al. (<a href="09-references.html#ref-yu2024llasmol" role="doc-biblioref">2024</a>)</span>]</td>
<td style="text-align: left;">Molecule Naming, Property Prediction, Reaction Prediction, Reaction Conditions Prediction, Molecule &amp; Reaction Generation, Molecule Description</td>
<td style="text-align: left;">A, L</td>
<td style="text-align: left;">3.3 M</td>
</tr>
<tr class="odd">
<td style="text-align: left;">MaCBench [<span class="citation" data-cites="alampara2024probing">Alampara et al. (<a href="09-references.html#ref-alampara2024probing" role="doc-biblioref">2024</a>)</span>]</td>
<td style="text-align: left;">Multimodal Chemistry, Information Extraction, Experimental Chemistry, Material Properties &amp; Characterization</td>
<td style="text-align: left;">M</td>
<td style="text-align: left;">1.2 K</td>
</tr>
<tr class="even">
<td style="text-align: left;">MaScQA [<span class="citation" data-cites="zaki2023mascqa0">Zaki et al. (<a href="09-references.html#ref-zaki2023mascqa0" role="doc-biblioref">2023</a>)</span>]</td>
<td style="text-align: left;">Material Properties &amp; Characterization, Reasoning, Experimental Chemistry</td>
<td style="text-align: left;">A</td>
<td style="text-align: left;">650</td>
</tr>
<tr class="odd">
<td style="text-align: left;">MolLangBench [<span class="citation" data-cites="cai2025mollangbench">F. Cai et al. (<a href="09-references.html#ref-cai2025mollangbench" role="doc-biblioref">2025</a>)</span>]</td>
<td style="text-align: left;">Molecule Structure Understanding, Molecule Generation</td>
<td style="text-align: left;">A, M</td>
<td style="text-align: left;">4 K</td>
</tr>
<tr class="even">
<td style="text-align: left;">MolPuzzle [<span class="citation" data-cites="guocan">K. Guo et al. (<a href="09-references.html#ref-guocan" role="doc-biblioref">2024</a>)</span>]</td>
<td style="text-align: left;">Molecule Understanding, Spectrum Interpretation, Molecule construction</td>
<td style="text-align: left;">A, L, M</td>
<td style="text-align: left;">23 K</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SciAssess [<span class="citation" data-cites="cai2024sciassess0">H. Cai et al. (<a href="09-references.html#ref-cai2024sciassess0" role="doc-biblioref">2024</a>)</span>]</td>
<td style="text-align: left;">General Chemistry MCQ, Information Extraction, Reasoning</td>
<td style="text-align: left;">A, M</td>
<td style="text-align: left;">2 K</td>
</tr>
<tr class="even">
<td style="text-align: left;">SciKnowEval [<span class="citation" data-cites="feng2024sciknoweval0">Feng et al. (<a href="09-references.html#ref-feng2024sciknoweval0" role="doc-biblioref">2024</a>)</span>]</td>
<td style="text-align: left;">General Chemistry MCQ, Information Extraction, Reasoning, Lab Safety, Experimental Chemistry</td>
<td style="text-align: left;">A, L</td>
<td style="text-align: left;">18.3 K</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><strong>Abbreviations:</strong> A: Automated methods, L: Usage of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, M: Manual curation.</p>
</div>
<figure id="fig:benchmarks" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure8.png" alt="" width="100%" class="figure-img">
<figcaption>
<strong>Comparison of measurement specificity and application relevance of chemistry benchmarks.</strong> This figure presents a subjective, qualitative positioning of selected chemistry-related benchmarks along the axes of application relevance and measurement specificity (the extent to which a benchmark evaluates well-defined and objectively measurable outputs—such as physical or chemical properties—rather than more ambiguous tasks like answering general chemistry trivia or <span data-acronym-label="mcq" data-acronym-form="singular+short">mcq</span>). The icons indicate whether a benchmark incorporates multimodal inputs (e.g., images or spectra), when it is actively maintained (based on GitHub activity within the past 6 months), and if it supports end-to-end evaluation via a clearly described pipeline with code provided by the authors.
</figcaption>
</figure>
</section>
<section id="sec:eval_design" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="sec:eval_design"><span class="header-section-number">6.2</span> Design of Evaluations</h2>
<section id="desired-properties-for-evaluations" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="desired-properties-for-evaluations"><span class="header-section-number">6.2.1</span> Desired Properties for Evaluations</h3>
<p>To meaningfully evaluate <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>, we must first consider what makes a good evaluation. Ideally, an evaluation should provide insights that translate into real-world impact, allowing comparisons between models. Thus, evaluation results must be stable over time and reproducible across different environments—assuming access to the same model weights or version, which is not always guaranteed when using proprietary <span data-acronym-label="api" data-acronym-form="plural+short">apis</span>. [<span class="citation" data-cites="Ollion2024dangers">Ollion et al. (<a href="09-references.html#ref-Ollion2024dangers" role="doc-biblioref">2024</a>)</span>] A key challenge is so-called construct validity—ensuring that evaluations measure what truly matters rather than what is easiest to quantify. For example, asking a model to generate valid <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> strings may test surface-level structure learning but fails to assess whether the model understands chemical reactivity or synthesis planning. Many methods fall into the trap of assessing proxy tasks instead of meaningful competencies, which leads to misleading progress. However, it is important to note that proxy tasks are often chosen because measurements at higher fidelity are more expensive or time-consuming to construct.</p>
<section id="data-and-biases" class="level4" data-number="6.2.1.1">
<h4 data-number="6.2.1.1" class="anchored" data-anchor-id="data-and-biases"><span class="header-section-number">6.2.1.1</span> Data and Biases</h4>
<p>The choice of what and how to measure is highly impacted by the data. Datasets in chemistry often differ in subtle but impactful ways: biases in chemical space coverage (e.g., overrepresented reaction types) [<span class="citation" data-cites="Jia_2019anthropogenic">Jia et al. (<a href="09-references.html#ref-Jia_2019anthropogenic" role="doc-biblioref">2019</a>)</span>; <span class="citation" data-cites="Fujinuma_2022why">Fujinuma et al. (<a href="09-references.html#ref-Fujinuma_2022why" role="doc-biblioref">2022</a>)</span>], variations in data fidelity (e.g., <span data-acronym-label="dft" data-acronym-form="singular+short">dft</span> vs.&nbsp;experimental measurements), inconsistent underlying assumptions (e.g., simulation level or experimental conditions), and differences in task difficulty. These differences can distort what evaluations actually measure, making comparisons across models or tasks unreliable. [<span class="citation" data-cites="peng2024survey">Peng et al. (<a href="09-references.html#ref-peng2024survey" role="doc-biblioref">2024</a>)</span>] Moreover, the process of collecting or curating data itself introduces further variability, introduced by incomplete or biased coverage of the chemical space, computational constraints, or design decisions in the construction of tasks. As such, evaluations must be built using transparent, well-documented construction protocols, with clearly stated scope and limitations.</p>
</section>
<section id="scoring-mechanism" class="level4" data-number="6.2.1.2">
<h4 data-number="6.2.1.2" class="anchored" data-anchor-id="scoring-mechanism"><span class="header-section-number">6.2.1.2</span> Scoring Mechanism</h4>
<figure id="fig:scoring" class="figure">
<img src="media/figures/final_figures/chembench_scoring.png" alt="" width="100%" class="figure-img">
<figcaption>
<strong><code>ChemBench</code> <span class="citation" data-cites="mirza2024large">(<a href="09-references.html#ref-mirza2024large" role="doc-biblioref">Mirza et al. 2025</a>)</span> rankings based on different scoring metrics</strong>: All metrics are a sum, weighted sum, or maximum values over all <span data-acronym-label="mcq" data-acronym-form="plural+short">mcqs</span>. The weighted sums are calculated by taking the manually rated difficulty (basic, immediate, advanced) of the question into account. For equal weighting, all categories are weighted equally, regardless of the number of questions. The metric “all correct” is a binary metric indicating if a given answer is completely correct. For normalized Hamming (max), the normalized maximum value of the Hamming loss of each model was taken. We find that the ranking of models changes if we change the metric, or even just the aggregation—showcasing the importance of proper and transparent evaluation design.
</figcaption>
</figure>
<p>The way model performance is scored has a direct impact on how results are interpreted and compared. Leaderboards and summary statistics often shape which models are considered state-of-the-art, making even small design choices in scoring, such as metric selection, aggregation, or treatment of uncertainty, highly consequential (see <a href="#fig:scoring" data-reference-type="ref+Label" data-reference="fig:scoring">2</a>). Inconsistent or poorly designed scoring can lead to misleading conclusions or unfair comparisons. Metric selection and aggregation critically shape evaluation outcomes. For example, in tasks where multiple correct answers are possible different evaluation strategies yield different insights: a permissive metric may assign partial credit for each correct option selected, while a stricter “all-or-nothing” metric only gives credit if the full set of correct choices is identified without any mistakes. The former captures varying levels of performance, while the latter enforces a binary pass or fail threshold. Aggregation strategies, such as task averaging or difficulty-based weighting, further influence the overall score and can substantially shift model rankings.</p>
</section>
<section id="statistical-significance-and-uncertainty-estimation" class="level4" data-number="6.2.1.3">
<h4 data-number="6.2.1.3" class="anchored" data-anchor-id="statistical-significance-and-uncertainty-estimation"><span class="header-section-number">6.2.1.3</span> Statistical significance and uncertainty estimation</h4>
<p>Statistical significance and uncertainty estimation are essential for drawing robust conclusions. Evaluations must include a sufficient number of questions per task type to ensure statistical power, and repeated runs (e.g., different seeds or sampling variations) are needed to report confidence intervals or error bars. [<span class="citation" data-cites="tikhonov2023post">Tikhonov and Yamshchikov (<a href="09-references.html#ref-tikhonov2023post" role="doc-biblioref">2023</a>)</span>] While this is feasible for automated, large-scale benchmarks, it becomes significantly more challenging in resource-intensive settings such as real-world deployment studies (e.g., testing a model in a wet-lab setting), where replicability and scale are limited.</p>
</section>
<section id="reproducibility-and-reporting" class="level4" data-number="6.2.1.4">
<h4 data-number="6.2.1.4" class="anchored" data-anchor-id="reproducibility-and-reporting"><span class="header-section-number">6.2.1.4</span> Reproducibility and Reporting</h4>
<p>Without clear and consistent documentation, even well-designed evaluations risk being misunderstood or unreproducible. To ensure that results are interpretable, verifiable, and extensible, every step of the evaluation process should be clearly specified, from prompt formulation and data pre-processing to metric selection and aggregation. Standardized evaluation protocols, careful tracking of environmental variables (e.g., hardware, model version, and sampling settings such as the inference temperature for <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>), and consistent version control (documenting the exact version of the used model) are essential to avoid unintentional variation across runs. Additionally, communicating limitations and design decisions openly can help the broader community understand the scope and reliability of the reported results. <span class="citation" data-cites="alampara2025lessons">Alampara, Schilling-Wilhelmi, and Jablonka (<a href="09-references.html#ref-alampara2025lessons" role="doc-biblioref">2025</a>)</span> proposed evaluation cards as a structured format to transparently report all relevant details of an evaluation, including design choices, assumptions, and known limitations, making it easier for others to interpret, reproduce, and build upon the results.</p>
</section>
</section>
</section>
<section id="sec:eval_methods" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="sec:eval_methods"><span class="header-section-number">6.3</span> Evaluation Methodologies</h2>
<section id="representational-vs.-pragmatic-evaluations" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="representational-vs.-pragmatic-evaluations"><span class="header-section-number">6.3.1</span> Representational vs.&nbsp;Pragmatic Evaluations</h3>
<p>It is useful to think of evaluations in <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> as living on a spectrum from representational to pragmatic. Representational evaluations focus on measuring concepts that exist in the real world. For instance, one might ask how well a model predicts a physically significant quantity like the band gap of a material or the yield of a chemical reaction. In contrast, in pragmatic evaluations, the concept we measure is defined by the evaluation procedure itself. A well-known example of this is IQ tests. The IQ is not a physical property that exists in the world independent of the test. It is rather defined by the measurement procedure. In the practice of evaluating <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> models, this includes tasks like answering <span data-acronym-label="mcq" data-acronym-form="singular+short">mcq</span> or completing structured benchmarks, where meaning emerges primarily through performance comparison. Such evaluations are essential for standardization and progress tracking; however, they risk creating feedback loops, as models may end up optimized for benchmark success rather than real-world usefulness, thereby reinforcing narrow objectives or biases.[<span class="citation" data-cites="alampara2025lessons">Alampara, Schilling-Wilhelmi, and Jablonka (<a href="09-references.html#ref-alampara2025lessons" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="skalse2022defining">Skalse et al. (<a href="09-references.html#ref-skalse2022defining" role="doc-biblioref">2022</a>)</span>]</p>
<section id="estimator-types" class="level4" data-number="6.3.1.1">
<h4 data-number="6.3.1.1" class="anchored" data-anchor-id="estimator-types"><span class="header-section-number">6.3.1.1</span> Estimator Types</h4>
<p>To systematically evaluate <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>, various methods (so-called estimators) have been developed, yet their application in chemistry remains limited. Broadly, these approaches summarized in <a href="#fig:estimators" data-reference-type="ref+Label" data-reference="fig:estimators">3</a> can be categorized into traditional benchmarks, challenges and competitions, red teaming and capability discovery, real-world deployment studies and ablation studies and systematic testing. Each evaluation method has its own strengths and limitations, and no single approach can comprehensively capture a model’s performance across all potential application scenarios.</p>
<figure id="fig:estimators" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure10.png" alt="" width="100%" class="figure-img">
<figcaption>
<strong>Comparison of Evaluation Methodologies</strong>: This figure compares five common evaluation methodologies for <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> across the dimensions of resource intensity (required human and computational effort), scalability (ease of applying the method across tasks and models), automation potential (need for manual intervention), real-world applicability (alignment with practical use cases), and numerical reducibility (ability to express results quantitatively). Checkmarks indicate a strength in the respective dimension, crosses denote a limitation, and dashes represent a neutral position.
</figcaption>
</figure>
</section>
<section id="para:trad_benchmarks" class="level4" data-number="6.3.1.2">
<h4 data-number="6.3.1.2" class="anchored" data-anchor-id="para:trad_benchmarks"><span class="header-section-number">6.3.1.2</span> Traditional Benchmarks</h4>
<p>Traditional benchmarks can provide a fast and scalable evaluation of the models. In the context of <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span>, a benchmark typically refers to a curated collection of tasks or questions alongside a defined evaluation protocol, which allows different models to be compared under the same conditions. Despite their advantages, summarized in <a href="#fig:estimators" data-reference-type="ref+Label" data-reference="fig:estimators">3</a>, benchmarks come with several limitations. They often struggle to capture real-world impact, as they evaluate models in controlled environments that may not reflect the complexity and open-endedness of real-world applications. Current chemistry benchmarks like ChemBench[<span class="citation" data-cites="mirza2024large">Mirza et al. (<a href="09-references.html#ref-mirza2024large" role="doc-biblioref">2025</a>)</span>] and CAMEL - Chemistry[<span class="citation" data-cites="li2023camel">Li et al. (<a href="09-references.html#ref-li2023camel" role="doc-biblioref">2023</a>)</span>] mostly fall on the pragmatic side of the measurement spectrum, as they are designed for comparison and decision-making rather than assessing inherent model properties. Classical benchmarks such as <code>MoleculeNet</code> [<span class="citation" data-cites="wu2018moleculenet">Wu et al. (<a href="09-references.html#ref-wu2018moleculenet" role="doc-biblioref">2018</a>)</span>] typically target molecular properties—such as solubility or binding affinity—that are experimentally measurable and representationally grounded. However, their evaluation setup, which relies on static and narrowly defined datasets, often fails to capture real-world applicability.</p>
<p>An additional disadvantage of traditional benchmarks is ease of overfitting, where models are optimized for high scores rather than genuine improvements. This problem is closely linked to data leakage—also known as test set pollution—which refers to the unintentional incorporation of test set information into the training process. Such leakage can distort performance estimates, especially when models are trained on publicly available benchmarks.[<span class="citation" data-cites="thompson2025chatbots">Thompson (<a href="09-references.html#ref-thompson2025chatbots" role="doc-biblioref">2025</a>)</span>] This exposure increases the risk that models learn to perform well on specific benchmark questions rather than developing a deeper, more generalizable understanding of the underlying concepts, leading to an overestimation of their true capabilities.</p>
<p>Several strategies have been proposed to mitigate these issues. One approach is to keep a portion of the benchmark private, preventing models from being exposed to evaluation data during training—as implemented in <code>LAB-Bench</code>, where <span class="math inline">\(20\%\)</span> of the questions are held out to safeguard against data leakage and overfitting.[<span class="citation" data-cites="laurent2024lab0bench0">Laurent et al. (<a href="09-references.html#ref-laurent2024lab0bench0" role="doc-biblioref">2024</a>)</span>] Alternatively, some initiatives explore privacy-preserving methods or the use of trusted third parties to evaluate models on held-out data without releasing it publicly. [<span class="citation" data-cites="eleutherai2024thirdparty">EleutherAI (<a href="09-references.html#ref-eleutherai2024thirdparty" role="doc-biblioref">2024</a>)</span>] Another strategy is to regularly update benchmarks by introducing more difficult or diverse tasks over time. [<span class="citation" data-cites="jimenez2023swe">Jimenez et al. (<a href="09-references.html#ref-jimenez2023swe" role="doc-biblioref">2023</a>)</span>] While this helps reduce overfitting by continually challenging models, it complicates long-term comparisons across versions and can undermine stability in performance tracking. [<span class="citation" data-cites="alampara2025lessons">Alampara, Schilling-Wilhelmi, and Jablonka (<a href="09-references.html#ref-alampara2025lessons" role="doc-biblioref">2025</a>)</span>]</p>
<p>Another limitation is that most benchmarks force models to respond to every question, even when uncertain, preventing evaluation of their ability to recognize what they do not know—an essential skill in real-world settings. <code>LAB-Bench</code> addresses this by allowing models to abstain via an “insufficient information” option, enabling a more nuanced assessment that distinguishes between confident knowledge and uncertainty.</p>
</section>
<section id="challenges-and-competitions" class="level4" data-number="6.3.1.3">
<h4 data-number="6.3.1.3" class="anchored" data-anchor-id="challenges-and-competitions"><span class="header-section-number">6.3.1.3</span> Challenges and Competitions</h4>
<p>Competitions and challenges offer a structured way to evaluate models under realistic conditions, emphasizing prospective prediction and reducing overfitting risks.[<span class="citation" data-cites="Moult2005">Moult (<a href="09-references.html#ref-Moult2005" role="doc-biblioref">2005</a>)</span>] Participants typically must submit predictions for a hidden test set, enabling blind, fair evaluation that more closely mirrors real-world deployment. Unlike standard benchmarking setups, these challenges often involve tasks with temporal, external, or domain-specific novelty, making them more resistant to subtle data leakage or overfitting through test set familiarity. While such challenges allow for systematic and rigorous assessment of model capabilities, they also require substantial coordination and oversight by a trusted third party.</p>
<p>Such challenges have been rare in chemistry to date, though recent examples in areas like polymer property prediction [<span class="citation" data-cites="gang2025neurips">Liu et al. (<a href="09-references.html#ref-gang2025neurips" role="doc-biblioref">2025</a>)</span>] suggest this is beginning to change. More successful examples exist in related domains—most notably the <span data-acronym-label="caspr" data-acronym-form="singular+short">caspr</span> competition[<span class="citation" data-cites="Moult2005">Moult (<a href="09-references.html#ref-Moult2005" role="doc-biblioref">2005</a>)</span>] for protein structure prediction in biology, and the crystal structure prediction blind test challenge[<span class="citation" data-cites="Lommerse2000">Lommerse et al. (<a href="09-references.html#ref-Lommerse2000" role="doc-biblioref">2000</a>)</span>] in materials science.</p>
</section>
<section id="para:red_teaming" class="level4" data-number="6.3.1.4">
<h4 data-number="6.3.1.4" class="anchored" data-anchor-id="para:red_teaming"><span class="header-section-number">6.3.1.4</span> Red Teaming and Capability Discovery</h4>
<p>Red teaming focuses on testing models in ways that they were not explicitly designed for, often probing their weaknesses, as well as unintended and unknown behaviors. [<span class="citation" data-cites="perez2022red">Perez et al. (<a href="09-references.html#ref-perez2022red" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="ganguli2022red">Ganguli et al. (<a href="09-references.html#ref-ganguli2022red" role="doc-biblioref">2022</a>)</span>] This group of evaluations includes attempts to bypass alignment mechanisms through adversarial prompting—deliberate attempts to elicit harmful, unsafe, or hidden model outputs by manipulating the input in subtle ways—or to reveal unintended capabilities. [<span class="citation" data-cites="zhu2023prompt">Zhu et al. (<a href="09-references.html#ref-zhu2023prompt" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="kumar2023computation">Kumar et al. (<a href="09-references.html#ref-kumar2023computation" role="doc-biblioref">2023</a>)</span>] Unlike standardized benchmarks, red teaming can reveal model abilities that remain undetected in evaluations with predefined tasks. However, a major challenge is the lack of systematic comparability. Results often depend on specific test strategies and are harder to quantify across models. Currently, most red teaming is conducted by human experts, making it a time-consuming process. Automated approaches are emerging to scale these evaluations, [<span class="citation" data-cites="ge2023mart0">Ge et al. (<a href="09-references.html#ref-ge2023mart0" role="doc-biblioref">2023</a>)</span>], but in domains like chemistry, effective automation requires models to possess deep scientific knowledge, which remains a significant challenge.</p>
<p>A concrete example of red teaming in chemistry was presented in the <code>GPT-4</code> Technical Report [<span class="citation" data-cites="openai2023gpt04">OpenAI et al. (<a href="09-references.html#ref-openai2023gpt04" role="doc-biblioref">2023</a>)</span>]. By augmenting <code>GPT-4</code> with tools like molecule search, synthesis planning, literature retrieval, and purchasability checks, red-teamers were able to identify purchasable chemical analogs of a given compound, and even managed to have one delivered to a home address. While the demonstration used a benign leukemia drug, the same approach could, in principle, be applied to identify alternatives to harmful substances.[<span class="citation" data-cites="urbina2022dual">Urbina et al. (<a href="09-references.html#ref-urbina2022dual" role="doc-biblioref">2022</a>)</span>]</p>
</section>
<section id="real-world-deployment-studies" class="level4" data-number="6.3.1.5">
<h4 data-number="6.3.1.5" class="anchored" data-anchor-id="real-world-deployment-studies"><span class="header-section-number">6.3.1.5</span> Real-World Deployment Studies</h4>
<p>Real-world deployment studies evaluate models in practical use settings, such as testing a <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> in a laboratory environment. [<span class="citation" data-cites="he2020deployment">He et al. (<a href="09-references.html#ref-he2020deployment" role="doc-biblioref">2020</a>)</span>] Unlike controlled benchmarks, these studies provide insights into how models perform in dynamic, real-world conditions, capturing challenges that predefined evaluations may overlook. For example, a generative model might be used to suggest synthesis routes that are then tested experimentally, revealing failures due to overlooked side reactions or missing reaction feasibility. However, they come with significant drawbacks: they are highly time-consuming, and systematic comparisons between models are difficult, as real-world environments introduce variability that is hard to control.</p>
<p>To date, such evaluations remain rare in the chemical sciences.</p>
</section>
<section id="ablation-studies-and-systematic-testing" class="level4" data-number="6.3.1.6">
<h4 data-number="6.3.1.6" class="anchored" data-anchor-id="ablation-studies-and-systematic-testing"><span class="header-section-number">6.3.1.6</span> Ablation Studies and Systematic Testing</h4>
<p>Ablation studies analyze models by systematically isolating and testing individual components or capabilities. By removing or modifying specific parts of the models, the impact on performance can be evaluated, providing information on the model’s functionality and potential weaknesses. This approach can be relatively scalable and structured, allowing for thorough and reproducible assessments. Ablation studies reveal limitations and improve overall reliability by ensuring a deeper understanding of how different elements contribute to the model’s behavior.</p>
<p><span class="citation" data-cites="alampara2024probing">Alampara et al. (<a href="09-references.html#ref-alampara2024probing" role="doc-biblioref">2024</a>)</span> conducted ablation studies to isolate the effects of scientific terminology, task complexity, and prompt guidance on model performance in multimodal chemistry tasks. In <code>MaCBench</code>, they showed that removing scientific terms or adding explicit guidance substantially improved model accuracy, suggesting that current models often rely on shallow heuristics rather than deep understanding. These structured ablations highlight specific failure modes and inform targeted improvements in prompt design and training strategies.</p>
</section>
</section>
</section>
<section id="future-directions" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="future-directions"><span class="header-section-number">6.4</span> Future Directions</h2>
<section id="emerging-evaluations-needs" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="emerging-evaluations-needs"><span class="header-section-number">6.4.1</span> Emerging Evaluations Needs</h3>
<p>To evaluate <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> in real-world conditions, more open-ended, multimodal, and robust approaches are needed. This is particularly evident in chemistry, where meaningful tasks often extend beyond text and require interpreting molecular structures, reaction schemes, or lab settings. Here, vision plays a fundamental role, enabling perception and reasoning in complex environments—such as reading labels, observing color changes, or manipulating an apparatus. [<span class="citation" data-cites="Eppel2020computer">Eppel et al. (<a href="09-references.html#ref-Eppel2020computer" role="doc-biblioref">2020</a>)</span>] In addition to visual cues, auditory signals—such as timer alerts or mechanical noise—can play a critical role in ensuring safety and coordination in lab environments. Sensorimotor input may also be relevant for simulating or guiding physical manipulation tasks, such as pipetting, adjusting equipment, or following multistep experimental procedures.</p>
<p>Beyond multimodality, another crucial challenge lies in evaluating open-ended scientific capabilities. Unlike well-defined benchmarks with fixed answers, real-world scientific inquiry is inherently open-ended.[<span class="citation" data-cites="mitchener2025bixbench0">Mitchener et al. (<a href="09-references.html#ref-mitchener2025bixbench0" role="doc-biblioref">2025</a>)</span>] This not only demands flexible and adaptive evaluation schemes but also raises deeper questions about what constitutes scientific understanding in generative models. This becomes even more important as agent-based systems (<a href="03-architectures.html#sec:agents" data-reference-type="ref+Label" data-reference="sec:agents">[sec:agents]</a> gain traction—models that do not simply respond to prompts but autonomously plan, reason, and execute multistep tasks in interaction with tools, databases, and lab environments. [<span class="citation" data-cites="cao2024agents">Cao et al. (<a href="09-references.html#ref-cao2024agents" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="mandal2024autonomous">Mandal et al. (<a href="09-references.html#ref-mandal2024autonomous" role="doc-biblioref">2024</a>)</span>] Simple input-output benchmarks are insufficient; instead, we need frameworks that can track progress in dynamic, goal-driven settings, where multiple valid solutions may exist.</p>
<p>In parallel to capability assessments, the evaluation of safety (see <a href="07-safety.html#sec:safety" data-reference-type="ref+Label" data-reference="sec:safety">[sec:safety]</a>) is becoming increasingly important—especially as <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> gain access to sensitive scientific knowledge and tools.[<span class="citation" data-cites="bran2024augmenting">Bran et al. (<a href="09-references.html#ref-bran2024augmenting" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="boiko2023autonomous">Boiko et al. (<a href="09-references.html#ref-boiko2023autonomous" role="doc-biblioref">2023</a>)</span>] Current safety evaluations often rely on manual red teaming <a href="#sec:eval_methods" data-reference-type="ref+Label" data-reference="sec:eval_methods">1.3</a>, which is neither scalable nor systematic. Future evaluation frameworks must therefore include robust, automated, and scalable safety testing pipelines, capable of detecting misuse potential and risky behaviors across modalities and contexts.[<span class="citation" data-cites="goldstein2023generative">Goldstein et al. (<a href="09-references.html#ref-goldstein2023generative" role="doc-biblioref">2023</a>)</span>]</p>
<p>Moreover, evaluations should not be limited to static benchmarks. One promising avenue could be the organization of recurring community-wide challenges, similar to established competitions in other fields (e.g., <span data-acronym-label="caspr" data-acronym-form="singular+short">caspr</span>[<span class="citation" data-cites="Moult2005">Moult (<a href="09-references.html#ref-Moult2005" role="doc-biblioref">2005</a>)</span>]). These challenges—ideally coordinated by major research consortia or national labs—can serve as shared reference points, drive innovation in evaluation design.</p>
<section id="standardization-efforts" class="level4" data-number="6.4.1.1">
<h4 data-number="6.4.1.1" class="anchored" data-anchor-id="standardization-efforts"><span class="header-section-number">6.4.1.1</span> Standardization Efforts</h4>
<p>One persistent challenge in evaluating <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> is the lack of common standards—whether in benchmark design, metric selection, or reporting protocols. This fragmentation makes it challenging to compare results or ensure reproducibility. While some degree of standardization can support transparency and cumulative progress, rigid frameworks risk constraining innovation and may conflict with the need in scientific discovery for more open-ended, adaptive evaluations.</p>
<p>A more feasible path may lie in promoting transparent documentation of evaluation choices and developing meta-evaluation tools that assess the validity, coverage, and robustness of different approaches. Emerging frameworks such as <span data-acronym-label="irt" data-acronym-form="singular+short">irt</span> offer promising directions for such reflective evaluations, enabling nuanced insights beyond surface-level metrics. [<span class="citation" data-cites="schilling2025lifting">Schilling-Wilhelmi, Alampara, and Jablonka (<a href="09-references.html#ref-schilling2025lifting" role="doc-biblioref">2025</a>)</span>]</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-alampara2025lessons" class="csl-entry" role="listitem">
Alampara, Nawaf, Mara Schilling-Wilhelmi, and Kevin Maik Jablonka. 2025. <span>“<span class="nocase">Lessons from the trenches on evaluating machine-learning systems in materials science</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2503.10837">https://doi.org/10.48550/arXiv.2503.10837</a>.
</div>
<div id="ref-alampara2024probing" class="csl-entry" role="listitem">
Alampara, Nawaf, Mara Schilling-Wilhelmi, Martiño Rı́os-Garcı́a, Indrajeet Mandal, Pranav Khetarpal, Hargun Singh Grover, NM Krishnan, and Kevin Maik Jablonka. 2024. <span>“<span class="nocase">Probing the limitations of multimodal language models for chemistry and materials research</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2411.16955">https://doi.org/10.48550/arXiv.2411.16955</a>.
</div>
<div id="ref-boiko2023autonomous" class="csl-entry" role="listitem">
Boiko, Daniil A, Robert MacKnight, Ben Kline, and Gabe Gomes. 2023. <span>“<span class="nocase">Autonomous chemical research with large language models</span>.”</span> <em>Nature</em> 624 (7992): 570–78. <a href="https://doi.org/10.1038/s41586-023-06792-0">https://doi.org/10.1038/s41586-023-06792-0</a>.
</div>
<div id="ref-bran2024augmenting" class="csl-entry" role="listitem">
Bran, Andres M., Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and Philippe Schwaller. 2024. <span>“Augmenting Large Language Models with Chemistry Tools.”</span> <em>Nature Machine Intelligence</em> 6 (5). <a href="https://doi.org/10.1038/s42256-024-00832-8">https://doi.org/10.1038/s42256-024-00832-8</a>.
</div>
<div id="ref-cai2025mollangbench" class="csl-entry" role="listitem">
Cai, Feiyang, Jiahui Bai, Tao Tang, Joshua Luo, Tianyu Zhu, Ling Liu, and Feng Luo. 2025. <span>“MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arxiv.2505.15054">https://doi.org/10.48550/arxiv.2505.15054</a>.
</div>
<div id="ref-cai2024sciassess0" class="csl-entry" role="listitem">
Cai, Hengxing, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, et al. 2024. <span>“<span class="nocase">SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis</span>.”</span> <em>arXiv Preprint arXiv: 2403.01976</em>. <a href="https://doi.org/10.48550/arXiv.2403.01976">https://doi.org/10.48550/arXiv.2403.01976</a>.
</div>
<div id="ref-cao2024agents" class="csl-entry" role="listitem">
Cao, Shuxiang, Zijian Zhang, Mohammed Alghadeer, Simone D Fasciati, Michele Piscitelli, Mustafa Bakr, Peter Leek, and Alán Aspuru-Guzik. 2024. <span>“<span class="nocase">Agents for self-driving laboratories applied to quantum computing</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2412.07978">https://doi.org/10.48550/arXiv.2412.07978</a>.
</div>
<div id="ref-eleutherai2024thirdparty" class="csl-entry" role="listitem">
EleutherAI. 2024. <span>“Third Party Model Evaluations.”</span> <a href="https://blog.eleuther.ai/third-party-evals/">https://blog.eleuther.ai/third-party-evals/</a>.
</div>
<div id="ref-Eppel2020computer" class="csl-entry" role="listitem">
Eppel, Sagi, Haoping Xu, Mor Bismuth, and Alan Aspuru-Guzik. 2020. <span>“Computer Vision for Recognition of Materials and Vessels in Chemistry Lab Settings and the Vector-LabPics Data Set.”</span> <em>ACS Central Science</em> 6 (10): 1743–52. <a href="https://doi.org/10.1021/acscentsci.0c00460">https://doi.org/10.1021/acscentsci.0c00460</a>.
</div>
<div id="ref-feng2024sciknoweval0" class="csl-entry" role="listitem">
Feng, Kehua, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, and Huajun Chen. 2024. <span>“<span class="nocase">SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large Language Models</span>.”</span> <em>arXiv Preprint arXiv: 2406.09098</em>. <a href="https://doi.org/10.48550/arXiv.2406.09098">https://doi.org/10.48550/arXiv.2406.09098</a>.
</div>
<div id="ref-Fujinuma_2022why" class="csl-entry" role="listitem">
Fujinuma, Naohiro, Brian DeCost, Jason Hattrick-Simpers, and Samuel E. Lofland. 2022. <span>“Why Big Data and Compute Are Not Necessarily the Path to Big Materials Science.”</span> <em>Communications Materials</em> 3 (1). <a href="https://doi.org/10.1038/s43246-022-00283-x">https://doi.org/10.1038/s43246-022-00283-x</a>.
</div>
<div id="ref-ganguli2022red" class="csl-entry" role="listitem">
Ganguli, Deep, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, et al. 2022. <span>“<span class="nocase">Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned</span>.”</span> <em>arXiv Preprint arXiv: 2209.07858</em>. <a href="https://doi.org/10.48550/arXiv.2209.07858">https://doi.org/10.48550/arXiv.2209.07858</a>.
</div>
<div id="ref-ge2023mart0" class="csl-entry" role="listitem">
Ge, Suyu, Chunting Zhou, Rui Hou, Madian Khabsa, Yi-Chia Wang, Qifan Wang, Jiawei Han, and Yuning Mao. 2023. <span>“<span class="nocase">MART: Improving LLM Safety with Multi-round Automatic Red-Teaming</span>.”</span> <em>arXiv Preprint arXiv: 2311.07689</em>. <a href="https://doi.org/10.48550/arXiv.2311.07689">https://doi.org/10.48550/arXiv.2311.07689</a>.
</div>
<div id="ref-goldstein2023generative" class="csl-entry" role="listitem">
Goldstein, Josh A., Girish Sastry, Micah Musser, Renee DiResta, Matthew Gentzel, and Katerina Sedova. 2023. <span>“Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arxiv.2301.04246">https://doi.org/10.48550/arxiv.2301.04246</a>.
</div>
<div id="ref-guocan" class="csl-entry" role="listitem">
Guo, Kehan, Bozhao Nan, Yujun Zhou, Taicheng Guo, Zhichun Guo, Mihir Surve, Zhenwen Liang, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. <span>“<span class="nocase">Can <span>LLM</span>s Solve Molecule Puzzles? A Multimodal Benchmark for Molecular Structure Elucidation</span>.”</span> <em>The Thirty-Eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>. <a href="https://openreview.net/forum?id=t1mAXb4Cop">https://openreview.net/forum?id=t1mAXb4Cop</a>.
</div>
<div id="ref-guo2023large" class="csl-entry" role="listitem">
Guo, Taicheng, Kehan Guo, B. Nan, Zhengwen Liang, Zhichun Guo, N. Chawla, O. Wiest, and Xiangliang Zhang. 2023. <span>“<span class="nocase">What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks</span>.”</span> <em>Neural Information Processing Systems</em>. <a href="https://doi.org/10.48550/arXiv.2305.18365">https://doi.org/10.48550/arXiv.2305.18365</a>.
</div>
<div id="ref-he2020deployment" class="csl-entry" role="listitem">
He, Mingguang, Zhixi Li, Chi Liu, Danli Shi, and Zachary Tan. 2020. <span>“Deployment of Artificial Intelligence in Real-World Practice: Opportunity and Challenge.”</span> <em>Asia-Pacific Journal of Ophthalmology</em> 9 (4): 299–307. <a href="https://doi.org/10.1097/apo.0000000000000301">https://doi.org/10.1097/apo.0000000000000301</a>.
</div>
<div id="ref-Jia_2019anthropogenic" class="csl-entry" role="listitem">
Jia, Xiwen, Allyson Lynch, Yuheng Huang, Matthew Danielson, Immaculate Lang’at, Alexander Milder, Aaron E. Ruby, et al. 2019. <span>“Anthropogenic Biases in Chemical Reaction Data Hinder Exploratory Inorganic Synthesis.”</span> <em>Nature</em> 573 (7773): 251–55. <a href="https://doi.org/10.1038/s41586-019-1540-5">https://doi.org/10.1038/s41586-019-1540-5</a>.
</div>
<div id="ref-jimenez2023swe" class="csl-entry" role="listitem">
Jimenez, Carlos E., John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. <span>“SWE-Bench: Can Language Models Resolve Real-World GitHub Issues?”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arxiv.2310.06770">https://doi.org/10.48550/arxiv.2310.06770</a>.
</div>
<div id="ref-kumar2023computation" class="csl-entry" role="listitem">
Kumar, Aounon, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun Li, Soheil Feizi, and Himabindu Lakkaraju. 2023. <span>“Certifying LLM Safety Against Adversarial Prompting.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arxiv.2309.02705">https://doi.org/10.48550/arxiv.2309.02705</a>.
</div>
<div id="ref-laurent2024lab0bench0" class="csl-entry" role="listitem">
Laurent, Jon M., Joseph D. Janizek, Michael Ruzo, Michaela M. Hinks, Michael J. Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D. White, and Samuel G. Rodriques. 2024. <span>“<span class="nocase">LAB-Bench: Measuring Capabilities of Language Models for Biology Research</span>.”</span> <em>arXiv Preprint arXiv: 2407.10362</em>. <a href="https://doi.org/10.48550/arXiv.2407.10362">https://doi.org/10.48550/arXiv.2407.10362</a>.
</div>
<div id="ref-li2023camel" class="csl-entry" role="listitem">
Li, Guohao, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. <span>“<span class="nocase">CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society</span>.”</span> <em>arXiv Preprint arXiv: 2303.17760</em>. <a href="https://doi.org/10.48550/arXiv.2303.17760">https://doi.org/10.48550/arXiv.2303.17760</a>.
</div>
<div id="ref-gang2025neurips" class="csl-entry" role="listitem">
Liu, Gang, Jiaxin Xu, Eric Inae, Yihan Zhu, Ying Li, Tengfei Luo, Meng Jiang, et al. 2025. <span>“NeurIPS - Open Polymer Prediction 2025.”</span> <a href="https://kaggle.com/competitions/neurips-open-polymer-prediction-2025">https://kaggle.com/competitions/neurips-open-polymer-prediction-2025</a>.
</div>
<div id="ref-Lommerse2000" class="csl-entry" role="listitem">
Lommerse, Jos P. M., W. D. Sam Motherwell, Herman L. Ammon, Jack D. Dunitz, Angelo Gavezzotti, Detlef W. M. Hofmann, Frank J. J. Leusen, et al. 2000. <span>“<span class="nocase">A test of crystal structure prediction of small organic molecules</span>.”</span> <em>Acta Crystallographica Section B Structural Science</em> 56 (4): 697–714. <a href="https://doi.org/10.1107/s0108768100004584">https://doi.org/10.1107/s0108768100004584</a>.
</div>
<div id="ref-mandal2024autonomous" class="csl-entry" role="listitem">
Mandal, Indrajeet, Jitendra Soni, Mohd Zaki, Morten M. Smedskjaer, Katrin Wondraczek, Lothar Wondraczek, Nitya Nand Gosvami, and N. M. Anoop Krishnan. 2024. <span>“<span class="nocase">Autonomous Microscopy Experiments through Large Language Model Agents</span>.”</span> <em>arXiv Preprint arXiv: 2501.10385</em>. <a href="https://doi.org/10.48550/arXiv.2501.10385">https://doi.org/10.48550/arXiv.2501.10385</a>.
</div>
<div id="ref-mirza2024large" class="csl-entry" role="listitem">
Mirza, Adrian, Nawaf Alampara, Sreekanth Kunchapu, Martiño Rı́os-Garcı́a, Benedict Emoekabu, Aswanth Krishnan, Tanya Gupta, et al. 2025. <span>“A Framework for Evaluating the Chemical Knowledge and Reasoning Abilities of Large Language Models Against the Expertise of Chemists.”</span> <em>Nature Chemistry</em>, 1–8. <a href="https://doi.org/10.1038/s41557-025-01815-x">https://doi.org/10.1038/s41557-025-01815-x</a>.
</div>
<div id="ref-mitchener2025bixbench0" class="csl-entry" role="listitem">
Mitchener, Ludovico, Jon M Laurent, Benjamin Tenmann, Siddharth Narayanan, Geemi P Wellawatte, Andrew White, Lorenzo Sani, and Samuel G Rodriques. 2025. <span>“<span class="nocase">BixBench: a Comprehensive Benchmark for LLM-based Agents in Computational Biology</span>.”</span> <em>arXiv Preprint arXiv: 2503.00096</em>. <a href="https://doi.org/10.48550/arXiv.2503.00096">https://doi.org/10.48550/arXiv.2503.00096</a>.
</div>
<div id="ref-Moult2005" class="csl-entry" role="listitem">
Moult, John. 2005. <span>“<span class="nocase">A decade of CASP: progress, bottlenecks and prognosis in protein structure prediction</span>.”</span> <em>Current Opinion in Structural Biology</em> 15 (3): 285–89. <a href="https://doi.org/10.1016/j.sbi.2005.05.011">https://doi.org/10.1016/j.sbi.2005.05.011</a>.
</div>
<div id="ref-Ollion2024dangers" class="csl-entry" role="listitem">
Ollion, Étienne, Rubing Shen, Ana Macanovic, and Arnault Chatelain. 2024. <span>“The Dangers of Using Proprietary LLMs for Research.”</span> <em>Nature Machine Intelligence</em> 6 (1): 4–5. <a href="https://doi.org/10.1038/s42256-023-00783-6">https://doi.org/10.1038/s42256-023-00783-6</a>.
</div>
<div id="ref-openai2023gpt04" class="csl-entry" role="listitem">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, et al. 2023. <span>“<span>GPT-4 Technical Report</span>.”</span> <em>arXiv Preprint arXiv: 2303.08774</em>. <a href="https://doi.org/10.48550/arXiv.2303.08774">https://doi.org/10.48550/arXiv.2303.08774</a>.
</div>
<div id="ref-peng2024survey" class="csl-entry" role="listitem">
Peng, Ji-Lun, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, and Yun-Nung Chen. 2024. <span>“<span class="nocase">A Survey of Useful LLM Evaluation</span>.”</span> <em>arXiv Preprint arXiv: 2406.00936</em>. <a href="https://doi.org/10.48550/arXiv.2406.00936">https://doi.org/10.48550/arXiv.2406.00936</a>.
</div>
<div id="ref-perez2022red" class="csl-entry" role="listitem">
Perez, Ethan, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022. <span>“<span class="nocase">Red Teaming Language Models with Language Models</span>.”</span> <em>arXiv Preprint arXiv: 2202.03286</em>. <a href="https://doi.org/10.48550/arXiv.2202.03286">https://doi.org/10.48550/arXiv.2202.03286</a>.
</div>
<div id="ref-raschka2018model" class="csl-entry" role="listitem">
Raschka, Sebastian. 2018. <span>“<span class="nocase">Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning</span>.”</span> <em>arXiv Preprint arXiv: 1811.12808</em>. <a href="https://doi.org/10.48550/arXiv.1811.12808">https://doi.org/10.48550/arXiv.1811.12808</a>.
</div>
<div id="ref-runcie2025assessing" class="csl-entry" role="listitem">
Runcie, Nicholas T., Charlotte M. Deane, and Fergus Imrie. 2025. <span>“Assessing the Chemical Intelligence of Large Language Models.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arxiv.2505.07735">https://doi.org/10.48550/arxiv.2505.07735</a>.
</div>
<div id="ref-schilling2025lifting" class="csl-entry" role="listitem">
Schilling-Wilhelmi, Mara, Nawaf Alampara, and Kevin Maik Jablonka. 2025. <span>“Lifting the Benchmark Iceberg with Item-Response Theory.”</span> <em>OpenReview</em>. <a href="https://openreview.net/forum?id=ZyVQqK7mcP">https://openreview.net/forum?id=ZyVQqK7mcP</a>.
</div>
<div id="ref-Siska2024" class="csl-entry" role="listitem">
Siska, Charlotte, Katerina Marazopoulou, Melissa Ailem, and James Bono. 2024. <span>“<span class="nocase">Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks</span>.”</span> <em>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 10406–21. <a href="https://doi.org/10.18653/v1/2024.acl-long.560">https://doi.org/10.18653/v1/2024.acl-long.560</a>.
</div>
<div id="ref-skalse2022defining" class="csl-entry" role="listitem">
Skalse, Joar, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. 2022. <span>“Defining and Characterizing Reward Hacking.”</span> <em>Advances in Neural Information Processing Systems</em> 35. <a href="https://doi.org/10.48550/arXiv.2209.13085">https://doi.org/10.48550/arXiv.2209.13085</a>.
</div>
<div id="ref-thompson2025chatbots" class="csl-entry" role="listitem">
Thompson, Derek. 2025. <span>“<span class="nocase">Why Chatbots Keep Beating the Tests</span>.”</span> <em>The Atlantic</em>, March. <a href="https://www.theatlantic.com/technology/archive/2025/03/chatbots-benchmark-tests/681929/">https://www.theatlantic.com/technology/archive/2025/03/chatbots-benchmark-tests/681929/</a>.
</div>
<div id="ref-tikhonov2023post" class="csl-entry" role="listitem">
Tikhonov, Alexey, and Ivan P. Yamshchikov. 2023. <span>“<span class="nocase">Post Turing: Mapping the landscape of LLM Evaluation</span>.”</span> <em>arXiv Preprint arXiv: 2311.02049</em>. <a href="https://doi.org/10.48550/arXiv.2311.02049">https://doi.org/10.48550/arXiv.2311.02049</a>.
</div>
<div id="ref-urbina2022dual" class="csl-entry" role="listitem">
Urbina, Fabio, Filippa Lentzos, Cedric Invernizzi, and Sean Ekins. 2022. <span>“<span class="nocase">Dual use of artificial-intelligence-powered drug discovery</span>.”</span> <em>Nature Machine Intelligence</em> 4 (3): 189–91. <a href="https://doi.org/10.1038/s42256-022-00465-9">https://doi.org/10.1038/s42256-022-00465-9</a>.
</div>
<div id="ref-wu2018moleculenet" class="csl-entry" role="listitem">
Wu, Zhenqin, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay Pande. 2018. <span>“<span class="nocase">MoleculeNet: a benchmark for molecular machine learning</span>.”</span> <em>Chemical Science</em> 9 (2): 513–30. <a href="https://doi.org/10.1039/c7sc02664a">https://doi.org/10.1039/c7sc02664a</a>.
</div>
<div id="ref-yu2024llasmol" class="csl-entry" role="listitem">
Yu, Botao, Frazier N. Baker, Ziqi Chen, Xia Ning, and Huan Sun. 2024. <span>“<span class="nocase">LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset</span>.”</span> <em>arXiv Preprint arXiv: 2402.09391</em>. <a href="https://doi.org/10.48550/arXiv.2402.09391">https://doi.org/10.48550/arXiv.2402.09391</a>.
</div>
<div id="ref-zaki2023mascqa0" class="csl-entry" role="listitem">
Zaki, Mohd, Jayadeva, Mausam, and N. M. Anoop Krishnan. 2023. <span>“<span class="nocase">MaScQA: A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models</span>.”</span> <em>arXiv Preprint arXiv: 2308.09115</em>. <a href="https://doi.org/10.48550/arXiv.2308.09115">https://doi.org/10.48550/arXiv.2308.09115</a>.
</div>
<div id="ref-zhang2024chemllm" class="csl-entry" role="listitem">
Zhang, Di, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, et al. 2024. <span>“<span class="nocase">Chemllm: A chemical large language model</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2402.06852">https://doi.org/10.48550/arXiv.2402.06852</a>.
</div>
<div id="ref-zhou2024labsafety" class="csl-entry" role="listitem">
Zhou, Yujun, Jingdong Yang, Yue Huang, Kehan Guo, Zoe Emory, Bikram Ghosh, Amita Bedar, et al. 2024. <span>“<span class="nocase">LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2410.14182">https://doi.org/10.48550/arXiv.2410.14182</a>.
</div>
<div id="ref-zhu2023prompt" class="csl-entry" role="listitem">
Zhu, Kaijie, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, et al. 2023. <span>“PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts.”</span> <a href="https://doi.org/10.48550/arxiv.2306.04528">https://doi.org/10.48550/arxiv.2306.04528</a>.
</div>
</div>
</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03-architectures.html" class="pagination-link" aria-label="Building Principles of GPMs">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Building Principles of GPMs</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05-applications.html" class="pagination-link" aria-label="Applications">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Applications</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><img src="https://raw.githubusercontent.com/lamalab-org/lamalab.github.io/main/static/png-file.png" alt="Lab for AI in Materials Science logo" style="height:5.0rem;vertical-align:middle;margin-right:0.4rem;"></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Copyright © 2025 Lab for AI in Materials Science</p>
</div>
  </div>
</footer>




</body></html>