<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Applications – General Purpose Models for the Chemical Sciences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./06-safety.html" rel="next">
<link href="./04-evals.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b95ce3833fcbbb97f3687e6914a03b85.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./05-applications.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Applications</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">General Purpose Models for the Chemical Sciences</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">General purpose models for the chemical sciences</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-data_taxonomy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Shape and Structure of Chemical Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Building Principles of GPMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-evals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Evaluations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-applications.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-safety.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implications of GPMs: Education, Safety, and Ethics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-outlook_conclusions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Outlook and Conclusions</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#applications" id="toc-applications" class="nav-link active" data-scroll-target="#applications"><span class="header-section-number">7</span> Applications</a>
  <ul class="collapse">
  <li><a href="#sec:ai-scientists" id="toc-sec:ai-scientists" class="nav-link" data-scroll-target="#sec\:ai-scientists"><span class="header-section-number">7.1</span> Automating the Scientific Workflow</a>
  <ul class="collapse">
  <li><a href="#coding-and-ml-applications-of-ai-scientists" id="toc-coding-and-ml-applications-of-ai-scientists" class="nav-link" data-scroll-target="#coding-and-ml-applications-of-ai-scientists"><span class="header-section-number">7.1.1</span> Coding and ML Applications of AI Scientists</a></li>
  <li><a href="#chemistry-and-related-fields" id="toc-chemistry-and-related-fields" class="nav-link" data-scroll-target="#chemistry-and-related-fields"><span class="header-section-number">7.1.2</span> Chemistry and Related Fields</a></li>
  <li><a href="#are-these-systems-capable-of-real-autonomous-research" id="toc-are-these-systems-capable-of-real-autonomous-research" class="nav-link" data-scroll-target="#are-these-systems-capable-of-real-autonomous-research"><span class="header-section-number">7.1.3</span> Are these Systems Capable of Real Autonomous Research?</a></li>
  </ul></li>
  <li><a href="#existing-gpms-for-chemical-science" id="toc-existing-gpms-for-chemical-science" class="nav-link" data-scroll-target="#existing-gpms-for-chemical-science"><span class="header-section-number">7.2</span> Existing GPMs for Chemical Science</a></li>
  <li><a href="#sec:information_gathering" id="toc-sec:information_gathering" class="nav-link" data-scroll-target="#sec\:information_gathering"><span class="header-section-number">7.3</span> Knowledge Gathering</a>
  <ul class="collapse">
  <li><a href="#structured-data-extraction" id="toc-structured-data-extraction" class="nav-link" data-scroll-target="#structured-data-extraction"><span class="header-section-number">7.3.1</span> Structured Data Extraction</a></li>
  <li><a href="#question-answering" id="toc-question-answering" class="nav-link" data-scroll-target="#question-answering"><span class="header-section-number">7.3.2</span> Question Answering</a></li>
  </ul></li>
  <li><a href="#sec:hypothesis-gen" id="toc-sec:hypothesis-gen" class="nav-link" data-scroll-target="#sec\:hypothesis-gen"><span class="header-section-number">7.4</span> Hypothesis Generation</a>
  <ul class="collapse">
  <li><a href="#initial-sparks" id="toc-initial-sparks" class="nav-link" data-scroll-target="#initial-sparks"><span class="header-section-number">7.4.1</span> Initial Sparks</a></li>
  <li><a href="#chemistry-focused-hypotheses" id="toc-chemistry-focused-hypotheses" class="nav-link" data-scroll-target="#chemistry-focused-hypotheses"><span class="header-section-number">7.4.2</span> Chemistry-Focused Hypotheses</a></li>
  <li><a href="#are-llms-actually-capable-of-novel-hypothesis-generation" id="toc-are-llms-actually-capable-of-novel-hypothesis-generation" class="nav-link" data-scroll-target="#are-llms-actually-capable-of-novel-hypothesis-generation"><span class="header-section-number">7.4.3</span> Are LLMs Actually Capable of Novel Hypothesis Generation?</a></li>
  </ul></li>
  <li><a href="#sec:planning" id="toc-sec:planning" class="nav-link" data-scroll-target="#sec\:planning"><span class="header-section-number">7.5</span> Experiment Planning</a>
  <ul class="collapse">
  <li><a href="#conventional-planning" id="toc-conventional-planning" class="nav-link" data-scroll-target="#conventional-planning"><span class="header-section-number">7.5.1</span> Conventional Planning</a></li>
  <li><a href="#llms-to-decompose-problems-into-plans" id="toc-llms-to-decompose-problems-into-plans" class="nav-link" data-scroll-target="#llms-to-decompose-problems-into-plans"><span class="header-section-number">7.5.2</span> LLMs to Decompose Problems into Plans</a></li>
  <li><a href="#pruning-of-search-spaces" id="toc-pruning-of-search-spaces" class="nav-link" data-scroll-target="#pruning-of-search-spaces"><span class="header-section-number">7.5.3</span> Pruning of Search Spaces</a></li>
  <li><a href="#evaluation" id="toc-evaluation" class="nav-link" data-scroll-target="#evaluation"><span class="header-section-number">7.5.4</span> Evaluation</a></li>
  </ul></li>
  <li><a href="#experiment-execution" id="toc-experiment-execution" class="nav-link" data-scroll-target="#experiment-execution"><span class="header-section-number">7.6</span> Experiment Execution</a>
  <ul class="collapse">
  <li><a href="#compiled-automation" id="toc-compiled-automation" class="nav-link" data-scroll-target="#compiled-automation"><span class="header-section-number">7.6.1</span> Compiled Automation</a></li>
  <li><a href="#interpreted-automation" id="toc-interpreted-automation" class="nav-link" data-scroll-target="#interpreted-automation"><span class="header-section-number">7.6.2</span> Interpreted Automation</a></li>
  <li><a href="#hybrid-approaches" id="toc-hybrid-approaches" class="nav-link" data-scroll-target="#hybrid-approaches"><span class="header-section-number">7.6.3</span> Hybrid Approaches</a></li>
  <li><a href="#comparison-and-outlook" id="toc-comparison-and-outlook" class="nav-link" data-scroll-target="#comparison-and-outlook"><span class="header-section-number">7.6.4</span> Comparison and Outlook</a></li>
  </ul></li>
  <li><a href="#data-analysis" id="toc-data-analysis" class="nav-link" data-scroll-target="#data-analysis"><span class="header-section-number">7.7</span> Data Analysis</a>
  <ul class="collapse">
  <li><a href="#prompting" id="toc-prompting" class="nav-link" data-scroll-target="#prompting"><span class="header-section-number">7.7.1</span> Prompting</a></li>
  <li><a href="#agentic-systems" id="toc-agentic-systems" class="nav-link" data-scroll-target="#agentic-systems"><span class="header-section-number">7.7.2</span> Agentic Systems</a></li>
  <li><a href="#current-limitations" id="toc-current-limitations" class="nav-link" data-scroll-target="#current-limitations"><span class="header-section-number">7.7.3</span> Current Limitations</a></li>
  </ul></li>
  <li><a href="#reporting" id="toc-reporting" class="nav-link" data-scroll-target="#reporting"><span class="header-section-number">7.8</span> Reporting</a>
  <ul class="collapse">
  <li><a href="#from-data-to-explanation" id="toc-from-data-to-explanation" class="nav-link" data-scroll-target="#from-data-to-explanation"><span class="header-section-number">7.8.1</span> From Data to Explanation</a></li>
  <li><a href="#writing-assistance" id="toc-writing-assistance" class="nav-link" data-scroll-target="#writing-assistance"><span class="header-section-number">7.8.2</span> Writing Assistance</a></li>
  <li><a href="#vision" id="toc-vision" class="nav-link" data-scroll-target="#vision"><span class="header-section-number">7.8.3</span> Vision</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#accelerating-applications" id="toc-accelerating-applications" class="nav-link" data-scroll-target="#accelerating-applications"><span class="header-section-number">8</span> Accelerating Applications</a>
  <ul class="collapse">
  <li><a href="#sec:prediction" id="toc-sec:prediction" class="nav-link" data-scroll-target="#sec\:prediction"><span class="header-section-number">8.1</span> Property Prediction</a>
  <ul class="collapse">
  <li><a href="#prompting-1" id="toc-prompting-1" class="nav-link" data-scroll-target="#prompting-1"><span class="header-section-number">8.1.1</span> Prompting</a></li>
  <li><a href="#sec:prediction_FT" id="toc-sec:prediction_FT" class="nav-link" data-scroll-target="#sec\:prediction_FT"><span class="header-section-number">8.1.2</span> Fine-Tuning</a></li>
  <li><a href="#agents" id="toc-agents" class="nav-link" data-scroll-target="#agents"><span class="header-section-number">8.1.3</span> Agents</a></li>
  <li><a href="#sec:property_core_limits" id="toc-sec:property_core_limits" class="nav-link" data-scroll-target="#sec\:property_core_limits"><span class="header-section-number">8.1.4</span> Core Limitations</a></li>
  </ul></li>
  <li><a href="#sec:mol_generation" id="toc-sec:mol_generation" class="nav-link" data-scroll-target="#sec\:mol_generation"><span class="header-section-number">8.2</span> Molecular and Material Generation</a>
  <ul class="collapse">
  <li><a href="#sec:generation" id="toc-sec:generation" class="nav-link" data-scroll-target="#sec\:generation"><span class="header-section-number">8.2.1</span> Generation</a></li>
  <li><a href="#validation" id="toc-validation" class="nav-link" data-scroll-target="#validation"><span class="header-section-number">8.2.2</span> Validation</a></li>
  </ul></li>
  <li><a href="#sec:retrosynthesis" id="toc-sec:retrosynthesis" class="nav-link" data-scroll-target="#sec\:retrosynthesis"><span class="header-section-number">8.3</span> Retrosynthesis</a></li>
  <li><a href="#sec:llm-optimizers" id="toc-sec:llm-optimizers" class="nav-link" data-scroll-target="#sec\:llm-optimizers"><span class="header-section-number">8.4</span> LLMs as Optimizers</a>
  <ul class="collapse">
  <li><a href="#llms-as-surrogate-models" id="toc-llms-as-surrogate-models" class="nav-link" data-scroll-target="#llms-as-surrogate-models"><span class="header-section-number">8.4.1</span> LLMs as Surrogate Models</a></li>
  <li><a href="#llms-as-next-candidate-generators" id="toc-llms-as-next-candidate-generators" class="nav-link" data-scroll-target="#llms-as-next-candidate-generators"><span class="header-section-number">8.4.2</span> LLMs as Next Candidate Generators</a></li>
  <li><a href="#sec:opt-llm-know-source" id="toc-sec:opt-llm-know-source" class="nav-link" data-scroll-target="#sec\:opt-llm-know-source"><span class="header-section-number">8.4.3</span> LLMs as Prior Knowledge Sources</a></li>
  <li><a href="#how-to-face-optimization-problems" id="toc-how-to-face-optimization-problems" class="nav-link" data-scroll-target="#how-to-face-optimization-problems"><span class="header-section-number">8.4.4</span> How to Face Optimization Problems?</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Applications</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="applications" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Applications</h1>
<section id="sec:ai-scientists" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="sec:ai-scientists"><span class="header-section-number">7.1</span> Automating the Scientific Workflow</h2>
<p>Recent advances in <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>, particularly <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, have enabled initial demonstrations of fully autonomous <span data-acronym-label="ai" data-acronym-form="singular+short">ai</span> scientists [<span class="citation" data-cites="schmidgall2025agent">Schmidgall et al. (<a href="#ref-schmidgall2025agent" role="doc-biblioref">2025</a>)</span>]. We define these as <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-powered architectures capable of executing end-to-end research workflows based solely on the final objectives, e.g., “<em>Unexplained rise of antimicrobial resistance in Pseudomonas. Formulate hypotheses, design confirmatory in vitro assays, and suggest repurposing candidates for liver-fibrosis drugs</em>”. Such systems navigate partially or entirely through all components of the scientific process outlined in <a href="#fig:applications" data-reference-type="ref+Label" data-reference="fig:applications">1</a>, and detailed in the subsequent sections.</p>
<p>While significant applications emerge in <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> and programming, scientific implementations remain less explored.</p>
<section id="coding-and-ml-applications-of-ai-scientists" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="coding-and-ml-applications-of-ai-scientists"><span class="header-section-number">7.1.1</span> Coding and ML Applications of AI Scientists</h3>
<p>Notable frameworks, including [<span class="citation" data-cites="gottweis2025towards">Gottweis et al. (<a href="#ref-gottweis2025towards" role="doc-biblioref">2025</a>)</span>], and [<span class="citation" data-cites="yamada2025ai">Yamada et al. (<a href="#ref-yamada2025ai" role="doc-biblioref">2025</a>)</span>], aim to automate the entire <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> research pipeline, typically employing multi-agent architectures (described in detail in <a href="03-architectures.html#sec:multi-agent" data-reference-type="ref+Label" data-reference="sec:multi-agent">[sec:multi-agent]</a>) where specialized agents manage distinct phases of the scientific method [<span class="citation" data-cites="schmidgall2025agentrxiv">Schmidgall and Moor (<a href="#ref-schmidgall2025agentrxiv" role="doc-biblioref">2025</a>)</span>]. Critical to these systems is self-reflection [<span class="citation" data-cites="renze2024self0reflection">Renze and Guven (<a href="#ref-renze2024self0reflection" role="doc-biblioref">2024</a>)</span>]—iterative evaluation and criticism of results within validation loops. However, comparative analyses reveal that <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-based evaluations frequently overscore outputs relative to human assessments [<span class="citation" data-cites="huang2023mlagentbench0">Q. Huang et al. (<a href="#ref-huang2023mlagentbench0" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="chan2024mle">Chan et al. (<a href="#ref-chan2024mle" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="starace2025paperbench0">Starace et al. (<a href="#ref-starace2025paperbench0" role="doc-biblioref">2025</a>)</span>]. From an engineering perspective, alternative approaches focus specifically on iterative code optimization, enabling systems to refine their codebases [<span class="citation" data-cites="zhang2025darwin">J. Zhang et al. (<a href="#ref-zhang2025darwin" role="doc-biblioref">2025</a>)</span>] or generate improved agents autonomously [<span class="citation" data-cites="hu2024automated">Hu, Lu, and Clune (<a href="#ref-hu2024automated" role="doc-biblioref">2024</a>)</span>]. In another work, [<span class="citation" data-cites="novikov2025alphaevolve">Novikov et al. (<a href="#ref-novikov2025alphaevolve" role="doc-biblioref">2025</a>)</span>], which is an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> operating within a <span data-acronym-label="ga" data-acronym-form="singular+short">ga</span> environment, found novel algorithms for matrix multiplication (which had seen no innovation in fifty years) and sorting.</p>
</section>
<section id="chemistry-and-related-fields" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="chemistry-and-related-fields"><span class="header-section-number">7.1.2</span> Chemistry and Related Fields</h3>
<p>In chemistry, proposed systems show promising results. identified ripasudil as a treatment for <span data-acronym-label="damd" data-acronym-form="singular+short">damd</span> [<span class="citation" data-cites="ghareeb2025robin0">Ghareeb et al. (<a href="#ref-ghareeb2025robin0" role="doc-biblioref">2025</a>)</span>]—despite pending clinical trials and the general debate for these systems about novelty of their findings[<span class="citation" data-cites="Listgarten2024perpetual">Listgarten (<a href="#ref-Listgarten2024perpetual" role="doc-biblioref">2024</a>)</span>]. However, automation of experiment execution poses a major constraint for the chemistry-focused <span data-acronym-label="ai" data-acronym-form="singular+short">ai</span>-scientists due to hardware requirements, making computational chemistry the most feasible subfield in which agents have successfully run simple quantum simulations [<span class="citation" data-cites="Zou2025ElAgente">Zou et al. (<a href="#ref-Zou2025ElAgente" role="doc-biblioref">2025</a>)</span>]. Further, the <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> powering these systems exhibit limited chemical knowledge [<span class="citation" data-cites="mirza2024large">Mirza et al. (<a href="#ref-mirza2024large" role="doc-biblioref">2025</a>)</span>]. Despite this, [<span class="citation" data-cites="narayanan2025training">Narayanan et al. (<a href="#ref-narayanan2025training" role="doc-biblioref">2025</a>)</span>]—the first chemistry-specialized reasoning <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> (see <a href="03-architectures.html#sec:rl" data-reference-type="ref+Label" data-reference="sec:rl">[sec:rl]</a> for a deeper discussion on reasoning models)—demonstrated strong capabilities in molecular design and accurate reaction prediction, positioning it as a promising foundation for chemistry-focused <span data-acronym-label="ai" data-acronym-form="singular+short">ai</span> scientists.</p>
</section>
<section id="are-these-systems-capable-of-real-autonomous-research" class="level3" data-number="7.1.3">
<h3 data-number="7.1.3" class="anchored" data-anchor-id="are-these-systems-capable-of-real-autonomous-research"><span class="header-section-number">7.1.3</span> Are these Systems Capable of Real Autonomous Research?</h3>
<p>Although agents like [<span class="citation" data-cites="intologyai2025zochi">Intology.ai (<a href="#ref-intologyai2025zochi" role="doc-biblioref">2025</a>)</span>] achieved peer-reviewed publication in top-tier venues (<span data-acronym-label="acl" data-acronym-form="singular+short">acl</span> 2025), their capacity for truly autonomous end-to-end research remains debatable [<span class="citation" data-cites="son2025ai">Son et al. (<a href="#ref-son2025ai" role="doc-biblioref">2025</a>)</span>]. Even when generating hypotheses that appear novel and impactful, their execution and reporting of these ideas, as demonstrated by <span class="citation" data-cites="si2025ideation1execution">Si, Hashimoto, and Yang (<a href="#ref-si2025ideation1execution" role="doc-biblioref">2025</a>)</span>, yield results deemed less attractive than those produced by humans. Additionally, this autonomy raises a critical question: <em>What should the role of <span data-acronym-label="ai" data-acronym-form="singular+short">ai</span> in science be?</em> While these systems can generate hypotheses, conduct experiments, and produce publication-ready manuscripts, their integration demands careful consideration (refer to <a href="06-safety.html#sec:ethics" data-reference-type="ref+Label" data-reference="sec:ethics">[sec:ethics]</a> for further discussion about moral concerns around these systems). Beyond the vision of fully autonomous scientists, <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>—primarily <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>—are already utilized across most scientific workflow components, for which <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> have proven useful for some. These elements are shown in <a href="#fig:applications" data-reference-type="ref+Label" data-reference="fig:applications">1</a>, and we discuss next.</p>
<figure id="fig:applications" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure11.png" width="50%" class="figure-img">
<figcaption>
<strong>Overview of the scientific process</strong>. The outer elements represent the typical scientific research process: from gathering information and generating hypotheses based on the observations, to executing experiments and analyzing the results. The terms that are in the center represent data-driven “shortcuts” that “accelerate” the typical scientific method. All stages represented in the figure are discussed in detail in the following sections.
</figcaption>
</figure>
</section>
</section>
<section id="existing-gpms-for-chemical-science" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="existing-gpms-for-chemical-science"><span class="header-section-number">7.2</span> Existing GPMs for Chemical Science</h2>
<p>The development of <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> for chemical science represents a departure from traditional single-task approaches. Rather than fine-tuning pre-trained models for specific applications such as property prediction or molecular generation, these chemistry-aware language models are intentionally designed to perform multiple chemical tasks simultaneously. This multitask paradigm offers several advantages: shared representations across related chemical problems[<span class="citation" data-cites="dimitrios2023unifying">Christofidellis et al. (<a href="#ref-dimitrios2023unifying" role="doc-biblioref">2023</a>)</span>], improved data efficiency through transfer learning between tasks[<span class="citation" data-cites="lim2021predicting">Lim and Lee (<a href="#ref-lim2021predicting" role="doc-biblioref">2020</a>)</span>], and the potential for emergent capabilities that arise from joint training across diverse chemical domains[<span class="citation" data-cites="livne2024nach0">Livne et al. (<a href="#ref-livne2024nach0" role="doc-biblioref">2024</a>)</span>].</p>
<section id="multitask-learning-frameworks" class="level4" data-number="7.2.0.1">
<h4 data-number="7.2.0.1" class="anchored" data-anchor-id="multitask-learning-frameworks"><span class="header-section-number">7.2.0.1</span> Multitask Learning Frameworks</h4>
<p>pioneered the multitask approach by fine-tuning through a two-stage process [<span class="citation" data-cites="xie2025darwin">Xie et al. (<a href="#ref-xie2025darwin" role="doc-biblioref">2025</a>)</span>]. Initially trained on 332k scientific question-answer pairs to establish foundational scientific reasoning, the model subsequently underwent multitask learning to perform property prediction-related regression and classification tasks concurrently.</p>
<p>Building on similar principles, introduced a unified encoder-decoder transformer architecture for cross-domain chemical tasks [<span class="citation" data-cites="livne2024nach0">Livne et al. (<a href="#ref-livne2024nach0" role="doc-biblioref">2024</a>)</span>]. Pre-trained using <span data-acronym-label="ssl" data-acronym-form="singular+short">ssl</span> on both natural language and chemical data, tackles diverse downstream applications including molecular structure generation, chemical property prediction, and reaction prediction. Notably, the authors found that combining chemistry-specific tasks outperformed models trained on distinct task groups, suggesting that chemical reasoning benefits from focused domain integration.</p>
<p>In the materials domain, <span class="citation" data-cites="qu2023leveraging">Qu et al. (<a href="#ref-qu2023leveraging" role="doc-biblioref">2023</a>)</span> developed a language-driven materials discovery framework that uses transformer-based embeddings (e.g., [<span class="citation" data-cites="wan2024tokens">Wan et al. (<a href="#ref-wan2024tokens" role="doc-biblioref">2024</a>)</span>]) to represent and generate novel crystal structures. Candidates are first recalled via similarity in embedding space, then ranked using a multitask multi-gate <span data-acronym-label="moe" data-acronym-form="singular+short">moe</span> model that predicts the desired properties jointly. Their method successfully identifies novel high-performance materials (e.g., halide perovskites) and demonstrates that language representations encode latent knowledge for task-agnostic materials design.</p>
</section>
<section id="domain-specific-pre-training-strategies" class="level4" data-number="7.2.0.2">
<h4 data-number="7.2.0.2" class="anchored" data-anchor-id="domain-specific-pre-training-strategies"><span class="header-section-number">7.2.0.2</span> Domain-Specific Pre-Training Strategies</h4>
<p>A second category of <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> emphasizes deep domain knowledge through specialized pre-training. employed <span data-acronym-label="peft" data-acronym-form="singular+short">peft</span> specifically on crystal structure data in <span data-acronym-label="cif" data-acronym-form="singular+short">cif</span> format, enabling the generation of thermodynamically stable structures [<span class="citation" data-cites="mishra2024foundational">Mishra et al. (<a href="#ref-mishra2024foundational" role="doc-biblioref">2024</a>)</span>].</p>
<p>scales this concept significantly, implementing domain pre-training on over 34 billion tokens from chemical textbooks and research articles [<span class="citation" data-cites="zhao2024chemdfm">Zhao et al. (<a href="#ref-zhao2024chemdfm" role="doc-biblioref">2024</a>)</span>]. Through comprehensive instruction tuning, familiarizes itself with chemical notation and patterns, distinguishing it from more materials-focused approaches like through its broader chemical knowledge base.</p>
<p>further refined this approach by introducing template-based instruction tuning (ChemData) to optimize property-guided molecular generation [<span class="citation" data-cites="zhang2024chemllm">D. Zhang et al. (<a href="#ref-zhang2024chemllm" role="doc-biblioref">2024</a>)</span>].</p>
</section>
<section id="reasoning-based-approaches" class="level4" data-number="7.2.0.3">
<h4 data-number="7.2.0.3" class="anchored" data-anchor-id="reasoning-based-approaches"><span class="header-section-number">7.2.0.3</span> Reasoning-Based Approaches</h4>
<p>A recent development in chemical <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> incorporates explicit reasoning capabilities. demonstrates this approach as a 24 billion parameter reasoning model trained on over 640k experimentally-grounded chemistry problems across diverse tasks, including retrosynthesis, solubility editing, and property prediction [<span class="citation" data-cites="narayanan2025training">Narayanan et al. (<a href="#ref-narayanan2025training" role="doc-biblioref">2025</a>)</span>]. Unlike previous models, uses <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> (see <a href="03-architectures.html#sec:rl" data-reference-type="ref+Label" data-reference="sec:rl">[sec:rl]</a>) to develop reasoning behaviors like verification and backtracking, demonstrating that structured problem-solving approaches can significantly improve performance on complex chemical tasks while maintaining grounding in experimental data.</p>
<p>These diverse approaches illustrate the evolving landscape of chemical <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>, each balancing broad applicability with domain-specific precision. Still, most applications of <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> focus on using these models for one specific application and we will review those in the following.</p>
</section>
</section>
<section id="sec:information_gathering" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="sec:information_gathering"><span class="header-section-number">7.3</span> Knowledge Gathering</h2>
<p>The rate of publishing keeps growing, and as a result, it is increasingly challenging to manually collect all relevant knowledge, potentially stymying scientific progress.[<span class="citation" data-cites="schilling2025text">Schilling-Wilhelmi et al. (<a href="#ref-schilling2025text" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="Chu_2021">Chu and Evans (<a href="#ref-Chu_2021" role="doc-biblioref">2021</a>)</span>] Even though knowledge collection might seem like a simple task, it often involves multiple different steps, visually described in <a href="#fig:knowledge_gathering" data-reference-type="ref+Label" data-reference="fig:knowledge_gathering">2</a>. We split the discussion in this section in two: structured data extraction and question answering. Example queries for both sections are shown at the bottom of <a href="#fig:knowledge_gathering" data-reference-type="ref+Label" data-reference="fig:knowledge_gathering">2</a>.</p>
<figure id="fig:knowledge_gathering" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure12.png" width="100%" class="figure-img">
<figcaption>
<strong>A. a representation of a typical agent for scientific queries.</strong> The <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> is the central piece of the system, surrounded by typical tools that improve its question-answering capabilities, together forming an agentic system. The tools represented in this figure are semantic search, citation traversal, evidence gathering, and question answering. Semantic search finds relevant documents. Evidence gathering ranks and filters chunks of text using <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>. The citation traversal tool provides model access to citation graphs, enabling accurate referencing of each chunk and facilitating the discovery of additional sources. Finally, the question-answering tool (an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>) collects all the information found by other tools and generates a final response to a user’s query. This part the figure is inspired by the agent.<span class="citation" data-cites="skarlinski2024language">(<a href="#ref-skarlinski2024language" role="doc-biblioref">Skarlinski et al. 2024</a>)</span> <strong>B.</strong> two examples of applications discussed in this section.
</figcaption>
</figure>
<section id="semantic-search" class="level4" data-number="7.3.0.1">
<h4 data-number="7.3.0.1" class="anchored" data-anchor-id="semantic-search"><span class="header-section-number">7.3.0.1</span> Semantic Search</h4>
<p>A step that is key to most, if not all, knowledge-gathering tasks is <span data-acronym-label="rag" data-acronym-form="singular+short">rag</span>, discussed in more detail in <a href="03-architectures.html#sec:rag" data-reference-type="ref+Label" data-reference="sec:rag">[sec:rag]</a>. Most commonly, this involves semantic search, intended to identify chunks of text with similar meaning. The difference between semantic search and conventional search lies in how each approach interprets queries. The latter operates through lexical matching—whether exact or fuzzy—focusing on the literal words and their variations. Semantic search, however, goes deeper by interpreting the underlying meaning and contextual relationships within the content.</p>
<p>To enable semantic search, documents are stored in vector databases using embedding vectors (see <a href="03-architectures.html#sec:embeddings" data-reference-type="ref+Label" data-reference="sec:embeddings">[sec:embeddings]</a>).[<span class="citation" data-cites="bojanowski2017enriching">Bojanowski et al. (<a href="#ref-bojanowski2017enriching" role="doc-biblioref">2017</a>)</span>] They represent the content of a document as a vector in a learned vector space and hence allow for similarity search by vector comparison (e.g., using cosine similarity for small databases or more sophisticated algorithms like <span data-acronym-label="hnsw" data-acronym-form="singular+short">hnsw</span> for large databases[<span class="citation" data-cites="malkov2018efficient">Malkov and Yashunin (<a href="#ref-malkov2018efficient" role="doc-biblioref">2018</a>)</span>]).</p>
<p>In chemistry, semantic search has been used extensively to classify and identify chemical text.[<span class="citation" data-cites="Guo2021">J. Guo et al. (<a href="#ref-Guo2021" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="beltagy2019scibert0">Beltagy, Lo, and Cohan (<a href="#ref-beltagy2019scibert0" role="doc-biblioref">2019</a>)</span>; <span class="citation" data-cites="trewartha2022quantifying">Trewartha et al. (<a href="#ref-trewartha2022quantifying" role="doc-biblioref">2022</a>)</span>]</p>
</section>
<section id="structured-data-extraction" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="structured-data-extraction"><span class="header-section-number">7.3.1</span> Structured Data Extraction</h3>
<p>Semantic search can help us find relevant resources. However, for many applications it can be useful to collect data in a structured form, e.g., tables with fixed columns. Obtaining such a dataset based on extracting data from the literature using <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> is currently one of the most practical avenues for the use of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> in the chemical sciences [<span class="citation" data-cites="schilling2025text">Schilling-Wilhelmi et al. (<a href="#ref-schilling2025text" role="doc-biblioref">2025</a>)</span>]. Currently, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> are used in various forms for this application.</p>
<section id="data-extraction-using-prompting" class="level4" data-number="7.3.1.1">
<h4 data-number="7.3.1.1" class="anchored" data-anchor-id="data-extraction-using-prompting"><span class="header-section-number">7.3.1.1</span> Data Extraction Using Prompting</h4>
<p>For most applications, zero-shot prompting should be the starting point. In this context, zero-shot prompting has been used to extract data about organic reactions[<span class="citation" data-cites="rios2025llm">Rı́os-Garcı́a and Jablonka (<a href="#ref-rios2025llm" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="vangala2024suitability">Vangala et al. (<a href="#ref-vangala2024suitability" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="Patiny2023automatic">Patiny and Godin (<a href="#ref-Patiny2023automatic" role="doc-biblioref">2023</a>)</span>], synthesis procedures for metal-organic frameworks[<span class="citation" data-cites="zheng2023chatgpt">Z. Zheng et al. (<a href="#ref-zheng2023chatgpt" role="doc-biblioref">2023</a>)</span>], polymers[<span class="citation" data-cites="schilling2024using">Schilling-Wilhelmi and Jablonka (<a href="#ref-schilling2024using" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="gupta2024data">Gupta et al. (<a href="#ref-gupta2024data" role="doc-biblioref">2024</a>)</span>], solar cells[<span class="citation" data-cites="shabih2025automated">Shabih et al. (<a href="#ref-shabih2025automated" role="doc-biblioref">2025</a>)</span>], or materials data[<span class="citation" data-cites="polak2024extracting">Polak and Morgan (<a href="#ref-polak2024extracting" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="hira2024reconstructing">Hira et al. (<a href="#ref-hira2024reconstructing" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="kumar2025mechbert">Kumar, Kabra, and Cole (<a href="#ref-kumar2025mechbert" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="wu2025large">T. Wu et al. (<a href="#ref-wu2025large" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="huang2022batterybert">S. Huang and Cole (<a href="#ref-huang2022batterybert" role="doc-biblioref">2022</a>)</span>].</p>
</section>
<section id="fine-tuning-based-data-extraction" class="level4" data-number="7.3.1.2">
<h4 data-number="7.3.1.2" class="anchored" data-anchor-id="fine-tuning-based-data-extraction"><span class="header-section-number">7.3.1.2</span> Fine-tuning Based Data Extraction</h4>
<p>If a commercial model needs to be run very often, it can be more cost-efficient to fine-tune a smaller, open-source model compared to prompting a large model. In addition, models might lack specialized knowledge and might not follow certain style guides, which can be introduced with fine-tuning. <span class="citation" data-cites="ai2024extracting">Ai et al. (<a href="#ref-ai2024extracting" role="doc-biblioref">2024</a>)</span> fine-tuned the model to extract procedural chemical reaction data from <span data-acronym-label="uspto" data-acronym-form="singular+short">uspto</span>, and converted it to a <span data-acronym-label="json" data-acronym-form="singular+short">json</span> format compatible with the schema of <span data-acronym-label="ord" data-acronym-form="singular+short">ord</span>[<span class="citation" data-cites="Kearnes_2021">Kearnes et al. (<a href="#ref-Kearnes_2021" role="doc-biblioref">2021</a>)</span>], achieving an overall accuracy of more than <span class="math inline">\(90\%\)</span>. In a different work, <span class="citation" data-cites="zhang2024fine">W. Zhang et al. (<a href="#ref-zhang2024fine" role="doc-biblioref">2024</a>)</span> fine-tuned to recognize and extract chemical entities from <span data-acronym-label="uspto" data-acronym-form="singular+short">uspto</span>. Fine-tuning improved the performance of the base model on the same task by more than <span class="math inline">\(15\%\)</span>. Similarly, <span class="citation" data-cites="dagdelen2024structured">Dagdelen et al. (<a href="#ref-dagdelen2024structured" role="doc-biblioref">2024</a>)</span> proposed a human-in-the-loop data annotation process, where humans correct the outputs from an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> extraction instead of extracting data from scratch.</p>
</section>
<section id="agents-for-data-extraction" class="level4" data-number="7.3.1.3">
<h4 data-number="7.3.1.3" class="anchored" data-anchor-id="agents-for-data-extraction"><span class="header-section-number">7.3.1.3</span> Agents for Data Extraction</h4>
<p>Agents (<a href="03-architectures.html#sec:agents" data-reference-type="ref+Label" data-reference="sec:agents">[sec:agents]</a>) have shown their potential in data extraction, though to a limited extent.[<span class="citation" data-cites="chen2024autonomous">K. Chen et al. (<a href="#ref-chen2024autonomous" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="kang2024chatmof">Kang and Kim (<a href="#ref-kang2024chatmof" role="doc-biblioref">2024</a>)</span>] For example, <span class="citation" data-cites="ansari2024agent">Ansari and Moosavi (<a href="#ref-ansari2024agent" role="doc-biblioref">2024</a>)</span> introduced , an agent that autonomously extracts structured materials science data from scientific literature without requiring fine-tuning, demonstrating performance comparable to or better than fine-tuned methods. Their agent is an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> with access to tools such as chemical databases (e.g., the database) and research papers from various sources.</p>
<p>While the authors claim this approach simplifies dataset creation for materials discovery, the evaluation is limited to a narrow set of materials science tasks (mostly focusing on <span data-acronym-label="mof" data-acronym-form="plural+short">mofs</span>), indicating the need for the creation of agent evaluation tools.</p>
</section>
<section id="limitations" class="level4" data-number="7.3.1.4">
<h4 data-number="7.3.1.4" class="anchored" data-anchor-id="limitations"><span class="header-section-number">7.3.1.4</span> Limitations</h4>
<p>The ability to extract data from sources other than text is important since a large amount of data is only stored in plots, tables, and figures. Despite some initial simple proofs of concept [<span class="citation" data-cites="Zheng2024image">Z. Zheng et al. (<a href="#ref-Zheng2024image" role="doc-biblioref">2024</a>)</span>], the main bottleneck presently is the limited understanding of image data compared to text data in multimodal models.[<span class="citation" data-cites="alampara2024probing">Alampara et al. (<a href="#ref-alampara2024probing" role="doc-biblioref">2024</a>)</span>] The promise of agents lies in their ability to interact with tools (that can also interpret multimodal data). Moreover, their ability to self-reflect could automatically improve wrong results.[<span class="citation" data-cites="du2023improving">Du et al. (<a href="#ref-du2023improving" role="doc-biblioref">2023</a>)</span>]</p>
</section>
</section>
<section id="question-answering" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="question-answering"><span class="header-section-number">7.3.2</span> Question Answering</h3>
<p>Besides extracting information from documents in a structured format, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can also be used to answer questions—such as “Has X been tried before” by synthesizing knowledge from a corpus of documents (and potentially automatically retrieving additional documents).</p>
<p>An example of a system that can do that is . This agentic system contains tools for search, evidence-gathering, and question answering as well as for traversing citation graphs, which are shown in <a href="#fig:knowledge_gathering" data-reference-type="ref+Label" data-reference="fig:knowledge_gathering">2</a>. The evidence-gathering tool collects the most relevant chunks of information via the semantic search and performs <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-based re-ranking of these chunks (i.e. the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> changes the order of the chunks depending on what is needed to answer the query). Subsequently, only the top-<span class="math inline">\(n\)</span> most relevant chunks are kept. To further ground the responses, citation traversal tools (e.g., Semantic Scholar[<span class="citation" data-cites="kinney2023semantic">Kinney et al. (<a href="#ref-kinney2023semantic" role="doc-biblioref">2023</a>)</span>]) are used. These leverage the citation graph as a means of discovering supplementary literature references. Ultimately, to address the user’s query, a question-answering tool is employed. It initially augments the query with all the collected information before providing a definitive answer. The knowledge aggregated by these systems could be used to generate new hypotheses or challenge existing ones. Thus, in the next section, we focus on this aspect.</p>
</section>
</section>
<section id="sec:hypothesis-gen" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="sec:hypothesis-gen"><span class="header-section-number">7.4</span> Hypothesis Generation</h2>
<p>Coming up with new hypotheses represents a cornerstone of the scientific process [<span class="citation" data-cites="rock2018hypothesis">Rock (<a href="#ref-rock2018hypothesis" role="doc-biblioref">2018</a>)</span>]. Historically, hypotheses have emerged from systematic observation of natural phenomena, exemplified by Isaac Newton’s formulation of the law of universal gravitation [<span class="citation" data-cites="newton1999principia">Newton (<a href="#ref-newton1999principia" role="doc-biblioref">1999</a>)</span>], which was inspired by the seemingly mundane observation of a falling apple [<span class="citation" data-cites="kosso2017whatgoesup">Kosso (<a href="#ref-kosso2017whatgoesup" role="doc-biblioref">2017</a>)</span>].</p>
<p>In modern research, hypothesis generation increasingly relies on data-driven tools. For example, clinical research employs frameworks such as <span data-acronym-label="viads" data-acronym-form="singular+short">viads</span> to derive testable hypotheses from well-curated datasets [<span class="citation" data-cites="Jing2022roles">Jing et al. (<a href="#ref-Jing2022roles" role="doc-biblioref">2022</a>)</span>]. Similarly, advances in <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> are now being explored for their potential to automate and enhance idea generation across scientific domains [<span class="citation" data-cites="oneill2025sparks">O’Neill et al. (<a href="#ref-oneill2025sparks" role="doc-biblioref">2025</a>)</span>]. However, such approaches face significant challenges due to the inherently open-ended nature of scientific discovery [<span class="citation" data-cites="stanley2017openendedness">Stanley, Lehman, and Soros (<a href="#ref-stanley2017openendedness" role="doc-biblioref">2017</a>)</span>]. Open-ended domains, as discussed in <a href="02-data_taxonomy.html#sec:data-section" data-reference-type="ref+Label" data-reference="sec:data-section">[sec:data-section]</a>, risk intractability, as an unbounded combinatorial space of potential variables, interactions, and experimental parameters complicates systematic exploration [<span class="citation" data-cites="clune2019ai0gas0">Clune (<a href="#ref-clune2019ai0gas0" role="doc-biblioref">2019</a>)</span>]. Moreover, the quantitative evaluation of the novelty and impact of generated hypotheses remains non-trivial. As Karl Popper argued, scientific discovery defies rigid logical frameworks [<span class="citation" data-cites="popper1959logic">Popper (<a href="#ref-popper1959logic" role="doc-biblioref">1959</a>)</span>], and objective metrics for “greatness” of ideas are elusive [<span class="citation" data-cites="stanley2015greatness">Stanley and Lehman (<a href="#ref-stanley2015greatness" role="doc-biblioref">2015</a>)</span>]. These challenges underscore the complexity of automating or systematizing the creative core of scientific inquiry.</p>
<section id="initial-sparks" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="initial-sparks"><span class="header-section-number">7.4.1</span> Initial Sparks</h3>
<p>Recent efforts in the <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> community have sought to simulate the hypothesis formulation process [<span class="citation" data-cites="Gu2025forecasting">Gu and Krenn (<a href="#ref-Gu2025forecasting" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="arlt2024meta0designing">Arlt et al. (<a href="#ref-arlt2024meta0designing" role="doc-biblioref">2024</a>)</span>], primarily leveraging multi-agent systems [<span class="citation" data-cites="jansen2025codescientist0">Jansen et al. (<a href="#ref-jansen2025codescientist0" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="kumbhar2025hypothesis">Kumbhar et al. (<a href="#ref-kumbhar2025hypothesis" role="doc-biblioref">2025</a>)</span>]. In such frameworks, agents typically retrieve prior knowledge to contextualize previous related work—grounding hypothesis generation in existing literature [<span class="citation" data-cites="naumov2025dora">Naumov et al. (<a href="#ref-naumov2025dora" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="ghareeb2025robin0">Ghareeb et al. (<a href="#ref-ghareeb2025robin0" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="gu2024interesting">Gu and Krenn (<a href="#ref-gu2024interesting" role="doc-biblioref">2024</a>)</span>]. A key challenge, however, lies in evaluating the generated hypotheses. While some studies leverage <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> to evaluate novelty or interestingness [<span class="citation" data-cites="zhang2024omni0">J. Zhang et al. (<a href="#ref-zhang2024omni0" role="doc-biblioref">2024</a>)</span>], recent work has introduced critic agents—specialized components designed to monitor and iteratively correct outputs from other agents—into multi-agent frameworks (see <a href="03-architectures.html#sec:multi-agent" data-reference-type="ref+Label" data-reference="sec:multi-agent">[sec:multi-agent]</a>). For instance, <span class="citation" data-cites="Ghafarollahi2024">Ghafarollahi and Buehler (<a href="#ref-Ghafarollahi2024" role="doc-biblioref">2024</a>)</span> demonstrated how integrating such critics enables systematic hypothesis refinement through continuous feedback mechanisms.</p>
<p>However, the reliability of purely model-based evaluation remains contentious. <span class="citation" data-cites="si2025llms">Si, Yang, and Hashimoto (<a href="#ref-si2025llms" role="doc-biblioref">2025</a>)</span> argued that relying on a <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> to evaluate hypotheses lacks robustness, advocating instead for human assessment. This approach was adopted in their work, where human evaluators validated hypotheses produced by their system, finding more novel <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-produced hypotheses compared to the ones proposed by humans. Notably, <span class="citation" data-cites="yamada2025ai">Yamada et al. (<a href="#ref-yamada2025ai" role="doc-biblioref">2025</a>)</span> advanced the scope of such systems by automating the entire research <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> process, from hypothesis generation to article writing. Their system’s outputs were submitted to workshops at the <span data-acronym-label="iclr" data-acronym-form="singular+short">iclr</span> 2025, with one contribution ultimately accepted. However, the advancements made by such works are currently incremental instead of unveiling new, paradigm-shifting research (see <a href="#fig:hypothesis-generation" data-reference-type="ref+Label" data-reference="fig:hypothesis-generation">3</a>).</p>
</section>
<section id="chemistry-focused-hypotheses" class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="chemistry-focused-hypotheses"><span class="header-section-number">7.4.2</span> Chemistry-Focused Hypotheses</h3>
<p>In scientific fields such as chemistry and materials science, hypothesis generation requires domain intuition, mastery of specialized terminology, and the ability to reason through foundational concepts [<span class="citation" data-cites="miret2024llms">Miret and Krishnan (<a href="#ref-miret2024llms" role="doc-biblioref">2024</a>)</span>]. To address potential knowledge gaps in <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, <span class="citation" data-cites="wang2023scimon0">Q. Wang et al. (<a href="#ref-wang2023scimon0" role="doc-biblioref">2023</a>)</span> proposed a few-shot learning approach (see <a href="03-architectures.html#sec:prompting" data-reference-type="ref+Label" data-reference="sec:prompting">[sec:prompting]</a>) for hypothesis generation and compared it with model fine-tuning for the same task. Their method strategically incorporates in-context examples to supplement domain knowledge while discouraging over-reliance on existing literature. For fine-tuning, they designed a loss function that penalizes possible biases—e.g., given the context “hierarchical tables challenge numerical reasoning”, the model would be penalized if it generated an overly generic prediction like “table analysis” instead of a task-specific one—when trained on such examples. Human evaluations of ablation studies revealed that , augmented with a knowledge graph of prior research, outperformed fine-tuned models in generating hypotheses with greater technical specificity and iterative refinement of such hypotheses.</p>
<p>Complementing this work, <span class="citation" data-cites="yang2025moose">Z. Yang, Liu, Gao, Xie, et al. (<a href="#ref-yang2025moose" role="doc-biblioref">2025</a>)</span> introduced the framework to evaluate the novelty of <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-generated hypotheses. To avoid data contamination, their benchmark exclusively uses papers published after the knowledge cutoff date of the evaluated model, . Ground-truth hypotheses were derived from articles in high-impact journals (e.g., Nature, Science) and validated by domain-specialized PhD researchers. By iteratively providing the model with context from prior studies, achieved coverage of over <span class="math inline">\(80\%\)</span> of the evaluation set’s hypotheses while accessing only <span class="math inline">\(4\%\)</span> of the retrieval corpus, demonstrating efficient synthesis of ideas presumably not present in its training corpus.</p>
<figure id="fig:hypothesis-generation" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure13.png" width="100%" class="figure-img">
<figcaption>
<strong>Overview of <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-based hypothesis generation</strong>. Current methods are based on <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-sampling methods in which an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> proposes new hypotheses. The generated hypotheses are evaluated in terms of novelty and impact either by another <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> or by a human. Then, through experimentation, the hypotheses are transformed into results which showcase that current <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> cannot produce groundbreaking ideas, limited to their training corpus, resulting in the best cases, in incremental work. This is shown metaphorically with the puzzle. The “pieces of chemical knowledge” based on the hypothesis produced by <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> are already present in the “chemistry puzzle”, not unveiling new parts of it.
</figcaption>
</figure>
</section>
<section id="are-llms-actually-capable-of-novel-hypothesis-generation" class="level3" data-number="7.4.3">
<h3 data-number="7.4.3" class="anchored" data-anchor-id="are-llms-actually-capable-of-novel-hypothesis-generation"><span class="header-section-number">7.4.3</span> Are LLMs Actually Capable of Novel Hypothesis Generation?</h3>
<p>Automatic hypothesis generation is often regarded as the Holy Grail of automating the scientific process [<span class="citation" data-cites="coley2020autonomous">Coley, Eyke, and Jensen (<a href="#ref-coley2020autonomous" role="doc-biblioref">2020</a>)</span>]. However, achieving this milestone remains challenging, as generating novel and impactful ideas requires questioning current scientific paradigms [<span class="citation" data-cites="Kuhn1962Structure">Kuhn (<a href="#ref-Kuhn1962Structure" role="doc-biblioref">1962</a>)</span>]—a skill typically refined through years of experience—which is currently impossible for most <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> systems.</p>
<p>Current progress in <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> illustrates these limitations [<span class="citation" data-cites="kon2025exp0bench0">Kon et al. (<a href="#ref-kon2025exp0bench0" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="gu2024interesting">Gu and Krenn (<a href="#ref-gu2024interesting" role="doc-biblioref">2024</a>)</span>]. Although some studies claim success in <span data-acronym-label="ai" data-acronym-form="singular+short">ai</span>-generated ideas accepted at workshops in <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> conferences via double-blind review [<span class="citation" data-cites="zhou2025tempest0">A. Zhou and Arel (<a href="#ref-zhou2025tempest0" role="doc-biblioref">2025</a>)</span>], these achievements are limited. First, accepted submissions often focus on coding tasks, one of the strongest domains for <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>. Second, workshop acceptances are less competitive than main conferences, as they prioritize early-stage ideas over rigorously validated contributions. In chemistry, despite some works showing promise on these systems [<span class="citation" data-cites="yang2025moose0chem20">Z. Yang, Liu, Gao, Liu, et al. (<a href="#ref-yang2025moose0chem20" role="doc-biblioref">2025</a>)</span>], <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> struggle to propose functional hypotheses [<span class="citation" data-cites="si2025ideation1execution">Si, Hashimoto, and Yang (<a href="#ref-si2025ideation1execution" role="doc-biblioref">2025</a>)</span>]. Their apparent success often hinges on extensive sampling and iterative refinement, rather than genuine conceptual innovation.</p>
<p>As <span class="citation" data-cites="Kuhn1962Structure">Kuhn (<a href="#ref-Kuhn1962Structure" role="doc-biblioref">1962</a>)</span> argued, generating groundbreaking ideas demands challenging prevailing paradigms—a capability missing in current <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> models (they are trained to make the existing paradigm more likely in training rather than questioning their training data), as shown in <a href="#fig:hypothesis-generation" data-reference-type="ref+Label" data-reference="fig:hypothesis-generation">3</a>. Thus, while accidental discoveries can arise from non-programmed events (e.g., Fleming’s identification of penicillin [<span class="citation" data-cites="Fleming1929antibacterial">Fleming (<a href="#ref-Fleming1929antibacterial" role="doc-biblioref">1929</a>)</span>; <span class="citation" data-cites="Fleming1945penicillin">Fleming (<a href="#ref-Fleming1945penicillin" role="doc-biblioref">1964</a>)</span>]), transformative scientific advances typically originate from deliberate critique of existing knowledge [<span class="citation" data-cites="popper1959logic">Popper (<a href="#ref-popper1959logic" role="doc-biblioref">1959</a>)</span>; <span class="citation" data-cites="Lakatos1970falsification">Lakatos (<a href="#ref-Lakatos1970falsification" role="doc-biblioref">1970</a>)</span>]. In addition, very often breakthroughs can also not be achieved by optimizing for a simple metric—as we often do not fully understand the problem and, hence, cannot design a metric.[<span class="citation" data-cites="stanley2015greatness">Stanley and Lehman (<a href="#ref-stanley2015greatness" role="doc-biblioref">2015</a>)</span>] Despite some publications suggesting that <span data-acronym-label="ai" data-acronym-form="singular+short">ai</span> scientists already exist, such claims are supported only by narrow evaluations that yield incremental progress [<span class="citation" data-cites="novikov2025alphaevolve">Novikov et al. (<a href="#ref-novikov2025alphaevolve" role="doc-biblioref">2025</a>)</span>], not paradigm-shifting insights. For <span data-acronym-label="ai" data-acronym-form="singular+short">ai</span> to evolve from research assistants into autonomous scientists, it must demonstrate efficacy in addressing societally consequential challenges, such as solving complex, open-ended problems at scale (e.g., “millennium” math problems [<span class="citation" data-cites="Carlson2006millennium">Carlson, Jaffe, and Wiles (<a href="#ref-Carlson2006millennium" role="doc-biblioref">2006</a>)</span>]).</p>
<p>Finally, ethical considerations become critical as hypothesis generation grows more data-driven and automated. Adherence to legal and ethical standards must guide these efforts (see <a href="06-safety.html#sec:safety" data-reference-type="ref+Label" data-reference="sec:safety">[sec:safety]</a>) [<span class="citation" data-cites="danish_gov2024hypothesis">The Danish National Committee on Health Research Ethics (<a href="#ref-danish_gov2024hypothesis" role="doc-biblioref">2024</a>)</span>].</p>
<p>With a hypothesis in hand, the next step is often to run an experiment to test it.</p>
</section>
</section>
<section id="sec:planning" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="sec:planning"><span class="header-section-number">7.5</span> Experiment Planning</h2>
<p>Before a human or robot can execute any experiments, a plan must be created. Planning can be formalized as the process of decomposing a high-level task into a structured sequence of actionable steps aimed at achieving a specific goal. The term planning is often confused with scheduling and <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span>, which are closely related but distinct concepts. Scheduling is a more specific process focused on the timing and sequence of tasks. It ensures that resources are efficiently allocated, experiments are conducted in an optimal order, and constraints (such as lab availability, time, and equipment) are respected.[<span class="citation" data-cites="kambhampati2023llmplanning">Kambhampati et al. (<a href="#ref-kambhampati2023llmplanning" role="doc-biblioref">2023</a>)</span>] <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> is about adapting and improving plans over time based on ongoing results.[<span class="citation" data-cites="chen2022deep">P. Chen et al. (<a href="#ref-chen2022deep" role="doc-biblioref">2022</a>)</span>]</p>
<section id="conventional-planning" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="conventional-planning"><span class="header-section-number">7.5.1</span> Conventional Planning</h3>
<p>Early experimental planning in chemistry relied on human intuition and domain expertise. One example of this is retrosynthesis. Since the 1960s, systems like <span data-acronym-label="lhasa" data-acronym-form="singular+short">lhasa</span> [<span class="citation" data-cites="corey1972computer">Corey, Cramer III, and Howe (<a href="#ref-corey1972computer" role="doc-biblioref">1972</a>)</span>] began automating retrosynthesis using hand-coded rules and heuristics[<span class="citation" data-cites="warr2014short">Warr (<a href="#ref-warr2014short" role="doc-biblioref">2014</a>)</span>]. Later tools, such as [<span class="citation" data-cites="grzybowski2018chematica">Grzybowski et al. (<a href="#ref-grzybowski2018chematica" role="doc-biblioref">2018</a>)</span>], expanded these efforts by integrating larger template libraries and optimization strategies. As reaction data grew in volume and complexity, manual rule encoding became unsustainable. Platforms like ASKCOS[<span class="citation" data-cites="tu2025askcos">Tu et al. (<a href="#ref-tu2025askcos" role="doc-biblioref">2025</a>)</span>] integrated <span data-acronym-label="gnn" data-acronym-form="plural+short">gnns</span> and neural classifiers to predict reactivity and suggest conditions, enabling actionable synthetic routes.</p>
<p>All applications, however, face the problem that planning is difficult because search spaces are combinatorially large and evaluating potential paths, in principle, requires a model that can perfectly predict the outcomes of different actions. Conventional approaches often rely on various forms of search algorithms such as <span data-acronym-label="bfs" data-acronym-form="singular+short">bfs</span>, <span data-acronym-label="dfs" data-acronym-form="singular+short">dfs</span>, <span data-acronym-label="mcts" data-acronym-form="singular+short">mcts</span> [<span class="citation" data-cites="segler2017towards">Segler, Preuß, and Waller (<a href="#ref-segler2017towards" role="doc-biblioref">2017</a>)</span>]. Those, however, are often still not efficient enough to tackle long-horizon planning for complex problems.</p>
</section>
<section id="llms-to-decompose-problems-into-plans" class="level3" data-number="7.5.2">
<h3 data-number="7.5.2" class="anchored" data-anchor-id="llms-to-decompose-problems-into-plans"><span class="header-section-number">7.5.2</span> LLMs to Decompose Problems into Plans</h3>
<p><span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>, in particular <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, can potentially assist in planning with two modes of thinking. Deliberate (system-2-like) thinking can be used to score potential options or to decompose problems into plans. Intuitive (system-1-like) thinking can be used to efficiently prune search spaces. These two modes align with psychological frameworks known as system-1 and system-2 thinking. [<span class="citation" data-cites="kahneman2011thinking">Kahneman (<a href="#ref-kahneman2011thinking" role="doc-biblioref">2011</a>)</span>] In the system-1 thinking, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> support rapid decision-making by leveraging heuristics and pattern recognition to quickly narrow down options. In contrast, system-2 thinking represents a slower, more analytical process, in which <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> solve complex tasks—such as logical reasoning and planning—by explicitly generating step-by-step reasoning. [<span class="citation" data-cites="ji2025test">Ji et al. (<a href="#ref-ji2025test" role="doc-biblioref">2025</a>)</span>]</p>
<p>Decomposing a goal into actionable milestones relies on this deliberate, system-2-style reasoning, enabling the model to evaluate alternatives and structure plans effectively. A variety of strategies have been proposed to improve the reasoning capabilities of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> during inference. Methods such as <span data-acronym-label="cot" data-acronym-form="singular+short">cot</span> and least-to-most prompting guide models to decompose problems into interpretable steps, improving transparency and interpretability. However, their effectiveness in planning is limited by error accumulation and linear thinking patterns.[<span class="citation" data-cites="stechly2024chain">Stechly, Valmeekam, and Kambhampati (<a href="#ref-stechly2024chain" role="doc-biblioref">2024</a>)</span>] To address these limitations, recent test-time strategies such as repeat sampling and tree search have been proposed to enhance planning capabilities in <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>. Repeated sampling allows the model to generate multiple candidate reasoning paths, encouraging diversity in thought and increasing the chances of discovering effective subgoal decompositions. [<span class="citation" data-cites="wang2024planning">E. Wang et al. (<a href="#ref-wang2024planning" role="doc-biblioref">2024</a>)</span>] Meanwhile, tree search methods like <span data-acronym-label="tot" data-acronym-form="singular+short">tot</span> and <span data-acronym-label="rap" data-acronym-form="singular+short">rap</span> treat reasoning as a structured search, also using algorithms like <span data-acronym-label="mcts" data-acronym-form="singular+short">mcts</span> to explore and evaluate multiple solution paths, facilitating more global and strategic decision-making. [<span class="citation" data-cites="hao2023reasoning">Hao et al. (<a href="#ref-hao2023reasoning" role="doc-biblioref">2023</a>)</span>]</p>
<p>Beyond purely linguistic reasoning, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> have also been used to interpret natural-language queries and to translate them into structured planning steps, as demonstrated by systems like <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>+P[<span class="citation" data-cites="liu2023llm">B. Liu et al. (<a href="#ref-liu2023llm" role="doc-biblioref">2023</a>)</span>] and <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-DP[<span class="citation" data-cites="dagan2023dynamic">Dagan, Keller, and Lascarides (<a href="#ref-dagan2023dynamic" role="doc-biblioref">2023</a>)</span>], which integrated <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> with classical planners to convert planning problems into <span data-acronym-label="pddl" data-acronym-form="singular+short">pddl</span>. <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> have also been applied to generate structured procedures from limited observations. For example, in quantum physics, a model was trained to infer reusable experimental templates from measurement data, producing Python code that generalized across system sizes. [<span class="citation" data-cites="arlt2024meta0designing">Arlt et al. (<a href="#ref-arlt2024meta0designing" role="doc-biblioref">2024</a>)</span>] This demonstrates how <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can support scientific planning by synthesizing high-level protocols from low-level evidence, moving beyond symbolic reasoning to executable plan generation.</p>
</section>
<section id="pruning-of-search-spaces" class="level3" data-number="7.5.3">
<h3 data-number="7.5.3" class="anchored" data-anchor-id="pruning-of-search-spaces"><span class="header-section-number">7.5.3</span> Pruning of Search Spaces</h3>
<p>Pruning refers to the process of eliminating unlikely or suboptimal options during the search to reduce the computational burden. Because the number of potential pathways can grow exponentially, exhaustive search may be computationally intensive. Classical planners employ heuristics, value functions, or logical filters to perform pruning[<span class="citation" data-cites="bonet2012action">Bonet and Geffner (<a href="#ref-bonet2012action" role="doc-biblioref">2012</a>)</span>]. <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can emulate pruning through learned heuristics, intuitive judgment, or context-driven evaluation, [<span class="citation" data-cites="gao2025synergizing">Y. Gao et al. (<a href="#ref-gao2025synergizing" role="doc-biblioref">2025</a>)</span>] reflecting system-1 thinking. <a href="#fig:planning" data-reference-type="ref+Label" data-reference="fig:planning">4</a> illustrates how <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can support experimental planning by selectively pruning options. Rule-based heuristics derived from domain knowledge can automatically discard routes involving unfavorable motifs, such as chemically strained rings or complex aromatic scaffolds. Meanwhile, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can emulate an expert chemist’s intuition by discarding synthetic routes that appear unnecessarily long, inefficient, or mechanistically implausible.</p>
<p>To further enhance planning efficacy, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can be augmented with external tools that estimate the feasibility or performance of candidate plans, enabling targeted pruning of the search space before costly execution. In , the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> collaborated with specialized chemical tools with knowledge about molecular and reaction properites. While does not explicitly generate and prune a large pool of candidate plans, these tools serve as real-time evaluators that help the model avoid unfeasible or inefficient directions during synthesis or reaction planning.</p>
<p>In addition to external tools, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can also engage in self-correction, a reflective strategy that identifies and prunes flawed reasoning steps within their own outputs. This introspective pruning supports more robust and coherent planning by discarding faulty intermediate steps before they affect final decisions. As such, self-correction offers a lightweight yet effective mechanism for narrowing the solution space in complex reasoning tasks. At the highest level of oversight, human-in-the-loop frameworks introduce expert feedback to guide pruning decisions. The system[<span class="citation" data-cites="darvish2025organa">Darvish et al. (<a href="#ref-darvish2025organa" role="doc-biblioref">2025</a>)</span>] integrated chemist feedback into the planning process, helping define goals, resolve ambiguities, and eliminate invalid directions.</p>
<figure id="fig:planning" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure14.png" width="100%" class="figure-img">
<figcaption>
<strong><span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span>-guided retrosynthesis route planning and pruning</strong>. <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> can systematically evaluate and prune retrosynthetic routes using multiple reasoning capabilities to discriminate between viable and problematic approaches. The partially overlapping arrows at the start of each route indicate multiple steps. <strong>Route A</strong>: This route was pruned by heuristic reasoning due to the unfavorable aromatic core construction. <strong>Route B</strong>: This route was selected as it successfully passes all <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> planning checks, demonstrating optimal synthetic feasibility. <strong>Route C</strong>: This pathway was pruned by external tools due to the poor region-selectivity of the oxidation step. <strong>Route D</strong>: This route was pruned based on learned intuition, as it represents an inefficient multistep pathway; the route could just start with phenol instead of synthesizing it.
</figcaption>
</figure>
</section>
<section id="evaluation" class="level3" data-number="7.5.4">
<h3 data-number="7.5.4" class="anchored" data-anchor-id="evaluation"><span class="header-section-number">7.5.4</span> Evaluation</h3>
<p>While pruning accelerates planning, its effectiveness depends on reliable evaluation—the ability to judge whether a candidate plan is valid or promising. However, evaluating planning quality is particularly challenging in scientific fields such as chemistry and biology. Many alternative plans may achieve the same goal, so evaluation is inherently ambiguous in the absence of a comprehensive world model. In open-ended domains, evaluation is often conducted manually. For example, [<span class="citation" data-cites="bran2024augmenting">Bran et al. (<a href="#ref-bran2024augmenting" role="doc-biblioref">2024</a>)</span>] relied on expert review to assess the correctness and plausibility of generated outputs. More dynamic evaluations can be performed in simulated or real embodied environments [<span class="citation" data-cites="song2023llm">Song et al. (<a href="#ref-song2023llm" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="choi2024lota">Choi et al. (<a href="#ref-choi2024lota" role="doc-biblioref">2024</a>)</span>], offering interactive feedback on feasibility. In parallel, automatic evaluation methods are emerging. For example, [<span class="citation" data-cites="o2023bioplanner">O’Donoghue et al. (<a href="#ref-o2023bioplanner" role="doc-biblioref">2023</a>)</span>] used pseudocode-based evaluation, comparing <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-generated protocols to expert-written pseudocode representations to assess plausibility and correctness without requiring manual review or physical execution.</p>
</section>
</section>
<section id="experiment-execution" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="experiment-execution"><span class="header-section-number">7.6</span> Experiment Execution</h2>
<p>Once an experimental plan is available, whether from a human scientist’s idea or a sophisticated AI model, the next step is to execute it. Regardless of its source, the plan must be translated into concrete, low-level actions for execution. One of the main challenges of lab automation is to convert the high-level and abstract experimental plan into real-world operations carried out by the experimental hardware (liquid-handing systems, robotic arms, instruments, etc.).</p>
<p>It is worth noting that, despite their methodological differences, executing experiments <em>in silico</em> (running simulations or code) and <em>in vitro</em> are not fundamentally different—both follow an essentially identical workflow: Plan <span class="math inline">\(\rightarrow\)</span> Instructions <span class="math inline">\(\rightarrow\)</span> Execution <span class="math inline">\(\rightarrow\)</span> Analysis. In a computer simulation, a researcher writes a program (plan), which is then compiled or interpreted into machine code (instructions) for the <span data-acronym-label="cpu" data-acronym-form="singular+short">cpu</span>, executed to produce data, and finally the outputs are analyzed. In an automated laboratory, the scientist specifies a protocol (plan), which must be translated into instrument commands (instructions), executed on a robotic platform, followed by the analysis of sensor data or assay results. Both scenarios require careful translation of abstract steps into concrete actions, as well as further decision-making based on the acquired results.</p>
<p>The execution of <em>in silico</em> experiments can be reduced to two essential steps: preparing input files and running the computational code; <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> can be used in both steps.[<span class="citation" data-cites="Liu2025ASA">Zhihan Liu, Chai, and Li (<a href="#ref-Liu2025ASA" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="Mendible-Barreto2025DynaMate">Mendible-Barreto et al. (<a href="#ref-Mendible-Barreto2025DynaMate" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="Zou2025ElAgente">Zou et al. (<a href="#ref-Zou2025ElAgente" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="Campbell2025MDCrow">Campbell et al. (<a href="#ref-Campbell2025MDCrow" role="doc-biblioref">2025</a>)</span>] <span class="citation" data-cites="Jacobs2025orca">Jacobs and Pollice (<a href="#ref-Jacobs2025orca" role="doc-biblioref">2025</a>)</span> found that using a combination of fine-tuning, <span data-acronym-label="cot" data-acronym-form="singular+short">cot</span> and <span data-acronym-label="rag" data-acronym-form="singular+short">rag</span> (see <a href="03-architectures.html#sec:model_adaptation" data-reference-type="ref+Label" data-reference="sec:model_adaptation">[sec:model_adaptation]</a>) can improve the performance of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> in generating executable input files for the quantum chemistry software <em>ORCA</em>[<span class="citation" data-cites="ORCA5">Neese (<a href="#ref-ORCA5" role="doc-biblioref">2022</a>)</span>], while <span class="citation" data-cites="Gadde2025chatbot">Gadde et al. (<a href="#ref-Gadde2025chatbot" role="doc-biblioref">2025</a>)</span> created , an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-based platform that assists users in preparing input files for <span data-acronym-label="qmmm" data-acronym-form="singular+short">qmmm</span> simulations of explicitly solvated molecules and running them on a remote computer. Examples of <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span>-based autonomous agents (see <a href="03-architectures.html#sec:agents" data-reference-type="ref+Label" data-reference="sec:agents">[sec:agents]</a>) capable of performing the entire computational workflow (i.e., preparing inputs, executing the code, and analyzing the results) are [<span class="citation" data-cites="Campbell2025MDCrow">Campbell et al. (<a href="#ref-Campbell2025MDCrow" role="doc-biblioref">2025</a>)</span>] (for molecular dynamics) and [<span class="citation" data-cites="Zou2025ElAgente">Zou et al. (<a href="#ref-Zou2025ElAgente" role="doc-biblioref">2025</a>)</span>] (for quantum chemistry).</p>
<p><span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> can also assist in automating <em>in vitro</em> experiments. We can draw parallels from programming language paradigms—compiled vs.&nbsp;interpreted (see <a href="#fig:exec" data-reference-type="ref+Label" data-reference="fig:exec">5</a>A)—to better understand how <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> can be useful in different approaches of experiment automation. In compiled languages (like or ), the entire code is converted ahead of time by another program called the “compiler” into binary machine code, which is directly executable by the hardware. In interpreted languages (like or ), a program called the “interpreter” reads the instructions line-by-line during runtime, translating and executing them on the fly. Compiled languages offer high performance and early error detection, making them ideal for performance-critical systems, but they require a separate compilation step and are less flexible during development. Interpreted languages are easier to use, debug, and modify on the fly, which makes them great for rapid development and scripting, but they generally run slower and catch errors only at runtime. Similarly, we can broadly categorize different approaches to experiment automation into two different groups: “compiled automation” and “interpreted automation” (see <a href="#fig:exec" data-reference-type="ref+Label" data-reference="fig:exec">5</a>B). In the compiled approach, the entire protocol is translated—either by a human or a <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span>—to low-level instructions before execution, while in interpreted automation, the <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> plays a central role, acting as the “interpreter” and executing the protocol step by step. As we show below, it can be instructive to use this perspective when discussing approaches to automate experiment execution with <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>.</p>
<figure id="fig:exec" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure15+16.png" width="\textwidth" class="figure-img">
<figcaption>
<strong>Programming languages vs.&nbsp;lab automation. A) programming paradigms</strong>: In compiled languages, the entire source code is translated ahead of time to machine code by the compiler. This stand-alone code is then given to the <span data-acronym-label="os" data-acronym-form="singular+short">os</span>, which is responsible for scheduling and distributing tasks to the hardware. In interpreted languages, the interpreter reads and translates each line of the source code to machine code and hands it to the <span data-acronym-label="os" data-acronym-form="singular+short">os</span> for execution. <strong>B) automation paradigms</strong>: In the compiled approach, a <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> formalizes the protocol, a compiler, such as the chempiler<span class="citation" data-cites="steiner2019organic">(<a href="#ref-steiner2019organic" role="doc-biblioref">Steiner et al. 2019</a>)</span>, translates the formalized protocol to hardware-specific low-level steps, which the controller then executes—a central hub tasked with scheduling and distributing commands to chemical hardware. In the interpreted approach, a <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span>, acting as the interpreter, first breaks down the protocol into specific steps, then sends them (via an <span data-acronym-label="api" data-acronym-form="singular+short">api</span>) for execution one by one. The strength of interpreted systems is dynamic feedback: after the execution of each step, the <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> receives a signal (e.g., data, errors), which can influence its behavior for the next steps.
</figcaption>
</figure>
<section id="compiled-automation" class="level3" data-number="7.6.1">
<h3 data-number="7.6.1" class="anchored" data-anchor-id="compiled-automation"><span class="header-section-number">7.6.1</span> Compiled Automation</h3>
<p>In the case of “compiled automation”, the experiment protocol needs to be formalized in a high-level or <span data-acronym-label="dsl" data-acronym-form="singular+short">dsl</span> that describes exactly what operations to perform in what order. A chemical compiler (or “chempiler” [<span class="citation" data-cites="steiner2019organic">Steiner et al. (<a href="#ref-steiner2019organic" role="doc-biblioref">2019</a>)</span>]) then converts this high-level protocol into low-level code for the specific lab hardware, which is then executed by robotic instruments, orchestrated by a controller (refer to the caption of <a href="#fig:exec" data-reference-type="ref+Label" data-reference="fig:exec">5</a>B).</p>
<section id="protocol-languages" class="level4" data-number="7.6.1.1">
<h4 data-number="7.6.1.1" class="anchored" data-anchor-id="protocol-languages"><span class="header-section-number">7.6.1.1</span> Protocol Languages</h4>
<p>While -based scripts are frequently used as the <em>de facto</em> protocol language due to ’s accessibility and flexibility,[<span class="citation" data-cites="pylabrobot">Wierenga et al. (<a href="#ref-pylabrobot" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="vriza2023polybot">Vriza et al. (<a href="#ref-vriza2023polybot" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="wang2025polybot">C. Wang et al. (<a href="#ref-wang2025polybot" role="doc-biblioref">2025</a>)</span>] specialized languages (<span data-acronym-label="dsl" data-acronym-form="plural+short">dsls</span>) have also been developed to provide more structured and semantically rich representations of experimental procedures.[<span class="citation" data-cites="wang2022ulsa">Z. Wang et al. (<a href="#ref-wang2022ulsa" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="ananthanarayanan2010biocoder">Ananthanarayanan and Thies (<a href="#ref-ananthanarayanan2010biocoder" role="doc-biblioref">2010</a>)</span>; <span class="citation" data-cites="autoprotocol2023">Strateos (<a href="#ref-autoprotocol2023" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="Park2023CMDL">Park et al. (<a href="#ref-Park2023CMDL" role="doc-biblioref">2023</a>)</span>] One of the prominent examples of such languages is <span data-acronym-label="chidl" data-acronym-form="singular+short">chidl</span>[<span class="citation" data-cites="xdl2023spec">Group (<a href="#ref-xdl2023spec" role="doc-biblioref">2023</a>)</span>], developed as part of the Chemputer architecture [<span class="citation" data-cites="steiner2019organic">Steiner et al. (<a href="#ref-steiner2019organic" role="doc-biblioref">2019</a>)</span>; <span class="citation" data-cites="mehr2020universal">Mehr et al. (<a href="#ref-mehr2020universal" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="hammer2021chemputation">Hammer et al. (<a href="#ref-hammer2021chemputation" role="doc-biblioref">2021</a>)</span>]. <span data-acronym-label="chidl" data-acronym-form="singular+short">chidl</span> uses a <span data-acronym-label="json" data-acronym-form="singular+short">json</span>-like format, and the experimental protocol is described by defining , , etc, and using abstract chemical commands such as , , , etc. In the next step, the software takes this <span data-acronym-label="chidl" data-acronym-form="singular+short">chidl</span> script and a description of the physical connectivity and composition of the automated platform as a graph and translates it into <span data-acronym-label="chasm" data-acronym-form="singular+short">chasm</span> which is specific to the platform (akin to machine code). In practice, <span data-acronym-label="chidl" data-acronym-form="singular+short">chidl</span> has been used to automate multi-step organic syntheses with yields comparable to manual experiments.[<span class="citation" data-cites="mehr2020universal">Mehr et al. (<a href="#ref-mehr2020universal" role="doc-biblioref">2020</a>)</span>]</p>
<p>Developing experimental protocols in a formal language is a non-trivial task, often requiring specialized coding expertise. Within the compiled approach, the role of the <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> is to translate natural-language protocols into their formalized, machine-readable counterparts.[<span class="citation" data-cites="Lamas2024DSLXpert">Sardiña, García-González, and Luaces (<a href="#ref-Lamas2024DSLXpert" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="jiang2024protocode">Jiang et al. (<a href="#ref-jiang2024protocode" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="conrad2025lowering">Conrad et al. (<a href="#ref-conrad2025lowering" role="doc-biblioref">2025</a>)</span>; <span class="citation" data-cites="inagaki2023robotic">Inagaki et al. (<a href="#ref-inagaki2023robotic" role="doc-biblioref">2023</a>)</span>] <span class="citation" data-cites="Vaucher2020AutoExtraction">Vaucher et al. (<a href="#ref-Vaucher2020AutoExtraction" role="doc-biblioref">2020</a>)</span> used an encoder-decoder transformer model to convert English experimental procedures to structured sequences of pre-defined synthesis actions (e.g., , , ). They pre-trained the model on <span class="math inline">\(2\)</span>M sentence-action pairs extracted by a rule-based <span data-acronym-label="nlp" data-acronym-form="singular+short">nlp</span> algorithm and then fine-tuned it on manually annotated samples to improve accuracy. The model achieved exact sentence-pair matching in <span class="math inline">\(61\%\)</span> of the test samples and had more than <span class="math inline">\(75\%\)</span> overlap in <span class="math inline">\(82\%\)</span> of them. Although this approach accelerates automated protocol extraction from chemical literature, the output format is not directly suitable for execution.</p>
<p><span class="citation" data-cites="Pagel2024LLMChemputer">Pagel, Jirásek, and Cronin (<a href="#ref-Pagel2024LLMChemputer" role="doc-biblioref">2024</a>)</span> introduced a multi-agent workflow (based on ) that can address this issue and convert unstructured chemistry papers into executable code. The first agent extracts all synthesis-relevant text, including supporting information; a procedure agent then sanitizes the data and tries to fill the gaps from chemical databases (using <span data-acronym-label="rag" data-acronym-form="singular+short">rag</span>); another agent translates procedures into <span data-acronym-label="chidl" data-acronym-form="singular+short">chidl</span> and simulates them on virtual hardware; finally, a critique agent cross-checks the translation and fixes errors.</p>
<p>The example above shows one of the strengths of the compiled approach: it allows for pre-validation. The protocol can be simulated or checked for any errors before running on the actual hardware, ensuring safety. Another example of <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-based validators for chemistry protocols is .[<span class="citation" data-cites="Yoshikawa2023CLAIRify">Yoshikawa et al. (<a href="#ref-Yoshikawa2023CLAIRify" role="doc-biblioref">2023</a>)</span>] Leveraging an iterative prompting strategy, it uses to first translate the natural-language protocol into <span data-acronym-label="chidl" data-acronym-form="singular+short">chidl</span> script, then automatically verifies its syntax and structure, identifies any errors, appends those errors to the prompt, and prompts the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> again—iterating this process until a valid <span data-acronym-label="chidl" data-acronym-form="singular+short">chidl</span> script is produced.</p>
<p>Similar to how compiled software can be recompiled for different platforms, compiled automation is hardware-agnostic: by using appropriate compilation, a well-defined protocol can—at least in principle—be run on different robotic systems as long as they have the required capabilities.[<span class="citation" data-cites="rauschen2024universal">Rauschen et al. (<a href="#ref-rauschen2024universal" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="strieth-kalthoff2024delocalized">Strieth-Kalthoff et al. (<a href="#ref-strieth-kalthoff2024delocalized" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="wilbraham2021chemPU">Wilbraham, Mehr, and Cronin (<a href="#ref-wilbraham2021chemPU" role="doc-biblioref">2021</a>)</span>] In practice, however, inconsistencies in hardware interfaces and software standards across the lab automation community make cross-platform execution challenging.</p>
<p>The main limitations of compiled approaches are the flip side of their strengths: low flexibility and adaptability. Any logic or decision-making must either be explicitly encoded within the protocol—necessitating meticulous scripting—or delegated to an external control layer.[<span class="citation" data-cites="mehr2023digitizing">M. Mehr, Caramelli, and Cronin (<a href="#ref-mehr2023digitizing" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="leonov2024integrated">Leonov et al. (<a href="#ref-leonov2024integrated" role="doc-biblioref">2024</a>)</span>] If something unexpected occurs (a pump clogging, a reaction taking longer than expected), the pre-compiled protocol cannot easily adjust in real-time, and human intervention or a complete recompile might be needed.</p>
</section>
</section>
<section id="interpreted-automation" class="level3" data-number="7.6.2">
<h3 data-number="7.6.2" class="anchored" data-anchor-id="interpreted-automation"><span class="header-section-number">7.6.2</span> Interpreted Automation</h3>
<p>Interpreted programming languages support higher levels of abstraction, enabling the use of more general and flexible command structures. Similarly, since <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> can translate high-level goals into concrete steps[<span class="citation" data-cites="ahn2022can">Ahn et al. (<a href="#ref-ahn2022can" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="huang2022language">W. Huang et al. (<a href="#ref-huang2022language" role="doc-biblioref">2022</a>)</span>], they can act as an “interpreter” between the experimental intent and lab hardware. For instance, given an instruction “titrate the solution until it turns purple”, a <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> agent (see <a href="03-architectures.html#sec:agents" data-reference-type="ref+Label" data-reference="sec:agents">[sec:agents]</a>) can break it down into smaller steps and convert each step to executable code, allowing it to perform incremental additions of titrant and read a color sensor, looping until the condition is met. This conversion of concrete steps to code happens at runtime; it is not pre-compiled. We refer to such systems as “interpreted automation” systems. In contrast to the deterministic, preplanned nature of compiled systems, interpreted architectures introduce real-time decision-making. As each action completes, the system collects sensor data (instrument readings, spectra, error messages, etc.) which the agent analyzes and decides on the next action. This allows for dynamic branching and conditional logic during the experiment execution.</p>
<p><em>Coscientist</em> [<span class="citation" data-cites="boiko2023autonomous">Boiko et al. (<a href="#ref-boiko2023autonomous" role="doc-biblioref">2023</a>)</span>] is an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-based chemistry assistant built around that can autonomously design and execute experiments. It can take high-level goals and call tools to write the code in real-time in order to control an Opentrons OT-2 liquid-handling robot. The architecture included tool calls: a web-search module, a documentation module (to read instrument manuals), a execution module (to run generated code in a sandbox), and an experiment execution module that sends code to actual lab equipment. If an error occurred, the system would get feedback and would debug its own code. successfully planned and executed multistep syntheses with minimal human intervention. For example, it efficiently optimized a palladium cross-coupling reaction with minimal human input, outperforming a standard Bayesian optimizer baseline in finding high-yield conditions.</p>
<p>Another example is [<span class="citation" data-cites="bran2024augmenting">Bran et al. (<a href="#ref-bran2024augmenting" role="doc-biblioref">2024</a>)</span>], a -based agent augmented with <span class="math inline">\(18\)</span> expert-designed tools for tasks like compound lookup, spectral analysis, and retrosynthesis. can perform tasks across synthesis planning, drug discovery, and materials design by invoking external software for things like retrosynthesis, property prediction, database queries, etc. It planned and executed the syntheses of an insect repellent, <span data-acronym-label="deet" data-acronym-form="singular+short">deet</span>, and three different organocatalysts and even guided the discovery of a new chromophore dye.</p>
<p>The interpreted paradigm is highly generalizable; in principle, the same <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> agent controlling a chemistry experiment could be re-purposed to a biology or materials experiment with minimal reprogramming because it operates at the level of intent and semantic understanding. However, fully autonomous labs featuring interpreted automation are still experimental themselves—ensuring their reliability and accuracy remains an open challenge.</p>
<p>Despite being labeled as “autonomous,” both systems mentioned above often need prompting nudges and human correction. In addition, these models can replicate known procedures and use databases, but they lack an understanding of mechanisms or underlying principles. Another issue is full reproducibility and long-term experiment tracking. Since the <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span>’s response might not be deterministic, small changes in prompts can yield different results and closed-source models like can change over time. Hallucinations remain a risk, especially in planning complex or sensitive reactions. In addition, allowing an agent to control hardware brings safety considerations; the flexibility of <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> means that they can devise unanticipated actions. Designing safety nets for these systems is an active area of research. (see <a href="06-safety.html#sec:safety" data-reference-type="ref+Label" data-reference="sec:safety">[sec:safety]</a>)</p>
</section>
<section id="hybrid-approaches" class="level3" data-number="7.6.3">
<h3 data-number="7.6.3" class="anchored" data-anchor-id="hybrid-approaches"><span class="header-section-number">7.6.3</span> Hybrid Approaches</h3>
<p>Between the two extremes of fully compiled vs.&nbsp;fully interpreted automation lies a hybrid approach that seeks to combine the best of both paradigms: the safety and reliability of compiled protocols and the <span data-acronym-label="ai" data-acronym-form="singular+short">ai</span>-driven flexibility of interpreted systems.</p>
<p>The key difference from purely interpreted systems is that during each experiment run, the plan is fixed, ensuring safety and reproducibility, but between runs, the plan can dynamically change based on the <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span>’s interpretation of results. Once the initial plan (ideally devised by the same <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> in a previous step) is provided to a hybrid system, instead of reducing it to smaller steps and directly sending the instructions to a laboratory one at a time, the protocol is first formalized—i.e., it is translated to a formal machine-readable format such as <span data-acronym-label="chidl" data-acronym-form="singular+short">chidl</span>. Once validated, the formalized protocol is compiled and executed. After the completion of execution, the <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> receives the results and decides what experiment to perform next. This cycle repeats, creating an autonomous optimization or discovery loop.</p>
<p>This hybrid strategy is attractive because it provides a safety net against mistakes made by the <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> interpreter; any generated procedure must pass through a formalization and verification stage before real execution, and therefore, erroneous or hallucinated steps can be caught. For example, if the interpreter hallucinated adding 1000&nbsp;mL of a solvent but the hardware has only 100&nbsp;mL capacity, it can be flagged as an error.</p>
<p>[<span class="citation" data-cites="darvish2025organa">Darvish et al. (<a href="#ref-darvish2025organa" role="doc-biblioref">2025</a>)</span>] is an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-based robotic assistant following this hybrid paradigm. It allows human chemists to describe their experimental goal in natural language. The system can converse with the user to clarify ambiguous requests (the agent would ask “do you mean X or Y?” if the instructions are unclear). Once the goal is understood, it uses [<span class="citation" data-cites="Yoshikawa2023CLAIRify">Yoshikawa et al. (<a href="#ref-Yoshikawa2023CLAIRify" role="doc-biblioref">2023</a>)</span>] to convert and validate the natural-language description of a chemistry experiment into a <span data-acronym-label="chidl" data-acronym-form="singular+short">chidl</span> script, which can be executed on a compatible platform. In one case, carried out a multistep electrochemistry procedure—polishing electrodes, running an experiment, and analyzing the data—involving 19 substeps that it coordinated in parallel. If an unexpected observation occurred (e.g., a solution does not change color when expected), the system can notice via image analysis and modify the plan or alert the user. In user studies, significantly reduced the manual labor and frustration for chemists, who could offload tedious tasks and trust the agent to handle low-level decisions.</p>
</section>
<section id="comparison-and-outlook" class="level3" data-number="7.6.4">
<h3 data-number="7.6.4" class="anchored" data-anchor-id="comparison-and-outlook"><span class="header-section-number">7.6.4</span> Comparison and Outlook</h3>
<p>While compiled paradigms continue to provide the backbone for reliable automation, interpreted paradigms will drive exploratory research, where adaptability is key. Hybrid systems are likely to be the bridge that brings <span data-acronym-label="ai" data-acronym-form="singular+short">ai</span> into mainstream lab operations, ensuring that flexibility comes with accountability. A brief comparison of the three mentioned approaches is given in <a href="#tab:execution_comparison" data-reference-type="ref+Label" data-reference="tab:execution_comparison">1</a>.</p>
<div id="tab:execution_comparison">
<table class="caption-top table">
<caption><strong>Comparison of the Compiled, Interpreted, and Hybrid Automation Paradigms</strong>. Each approach has its strengths and weaknesses. Compiled systems favor reliability, interpreted systems allow for more flexibility, while hybrid systems try to strike a balance.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Feature</strong></th>
<th style="text-align: center;"><strong>Compiled</strong></th>
<th style="text-align: center;"><strong>Interpreted</strong></th>
<th style="text-align: center;"><strong>Hybrid</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Flexibility</td>
<td style="text-align: center;"><span style="color: NegativeColor">Low</span></td>
<td style="text-align: center;"><span style="color: PositiveColor">High</span></td>
<td style="text-align: center;">Medium</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adaptivity</td>
<td style="text-align: center;"><span style="color: NegativeColor">None</span></td>
<td style="text-align: center;"><span style="color: PositiveColor">Real-time</span></td>
<td style="text-align: center;">Iterative</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Reproducibility</td>
<td style="text-align: center;"><span style="color: PositiveColor">High</span></td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;"><span style="color: PositiveColor">High</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Safety</td>
<td style="text-align: center;"><span style="color: PositiveColor">High</span></td>
<td style="text-align: center;"><span style="color: NegativeColor">Low</span></td>
<td style="text-align: center;">Medium</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Setup Overhead</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;"><span style="color: NegativeColor">High</span></td>
<td style="text-align: center;"><span style="color: NegativeColor">High</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Industrial Readiness</td>
<td style="text-align: center;"><span style="color: NegativeColor">Low</span></td>
<td style="text-align: center;"><span style="color: NegativeColor">Low</span></td>
<td style="text-align: center;"><span style="color: NegativeColor">Low</span></td>
</tr>
</tbody>
</table>
</div>
<p><span id="tab:execution_comparison" data-label="tab:execution_comparison"></span></p>
<p>While we are essentially witnessing the rise of self-driving laboratories, autonomous experimentation systems present a range of challenges.[<span class="citation" data-cites="Tom2024SDL">Tom et al. (<a href="#ref-Tom2024SDL" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="Seifrid2022SDL">Seifrid et al. (<a href="#ref-Seifrid2022SDL" role="doc-biblioref">2022</a>)</span>] First, translating high-level natural-language goals into precise laboratory actions remains difficult, as <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> can misinterpret ambiguous instructions, leading to invalid or unsafe procedures. This problem is compounded by the lack of universally adopted standards for protocol formalization; while languages like <span data-acronym-label="chidl" data-acronym-form="singular+short">chidl</span> show promise, inconsistencies in abstraction, device compatibility, and community uptake limit interoperability. Real-time execution adds further complexity, as systems must detect and respond to failures or unexpected behaviors; however, general-purpose validation mechanisms and recovery strategies remain underdeveloped. Hardware integration is another bottleneck; current commercial robotic platforms are prohibitively expensive and lab environments often rely on a patchwork of instruments with proprietary interfaces, and building robust, unified control layers demands considerable engineering overhead. Another challenge is multi-modality in chemistry; chemists use a wide variety of data (e.g., spectra, TLC plates, SEM images). Without integrating these forms of output, models will be limited in their decision-making. Finally, ensuring reproducibility and regulatory compliance requires that every step be logged, validated, and traceable at the level required for clinical or industrial adoption (see <a href="06-safety.html#sec:safety" data-reference-type="ref+Label" data-reference="sec:safety">[sec:safety]</a>. These challenges must be addressed in tandem to move from experimental demonstrations toward reliable, scalable, and trustworthy autonomous laboratories.</p>
</section>
</section>
<section id="data-analysis" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="data-analysis"><span class="header-section-number">7.7</span> Data Analysis</h2>
<p>The analysis of spectroscopic and experimental data in chemistry remains a predominantly manual process. Even seemingly straightforward steps, such as plotting or summarizing results, demand repeated manual intervention.</p>
<p>One key challenge that makes automation particularly difficult is the extreme heterogeneity of chemical data sources. Laboratories often rely on a wide variety of instruments, some of which are decades old, rarely standardized, or unique in configuration.[<span class="citation" data-cites="jablonka2022making">Jablonka, Patiny, and Smit (<a href="#ref-jablonka2022making" role="doc-biblioref">2022</a>)</span>] These devices output data in incompatible, non-standardized, or poorly documented formats, each requiring specialized processing pipelines. Despite efforts like [<span class="citation" data-cites="McDonald1988standard">McDonald and Wilks (<a href="#ref-McDonald1988standard" role="doc-biblioref">1988</a>)</span>], standardization attempts remain scarce and have generally failed to gain widespread use. This diversity makes rule-based or hard-coded solutions largely infeasible, as they cannot generalize across the long tail of edge cases and exceptions found in real-world workflows.</p>
<p>However, this exact complexity makes data analysis in chemistry a promising candidate for <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>. They are designed to operate flexibly across diverse tasks and formats, relying on implicit knowledge captured from broad training data. In other domains, <span class="citation" data-cites="narayan2022can">Narayan et al. (<a href="#ref-narayan2022can" role="doc-biblioref">2022</a>)</span> showed that models like can already perform classical data processing tasks such as cleaning, transformation, and error detection through prompting alone. <span class="citation" data-cites="kayali2023chorus">Kayali et al. (<a href="#ref-kayali2023chorus" role="doc-biblioref">2024</a>)</span> introduced that shows that <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can analyze heterogeneous tabular data without task-specific training. demonstrates that by converting tables into a standardized text format and using zero-shot prompting (i.e., prompts with no examples), <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can flexibly analyze tables even when they differ in structure, column names, or data types.</p>
<figure id="fig:anaylsis" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure17.png" width="100%" class="figure-img">
<figcaption>
<strong>Static conventional data analysis workflow vs.&nbsp;dynamic <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> generated workflow</strong>. The chemical analysis can be done with a variety of possible instruments and techniques, resulting in a large number of possible output data formats. The <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> can use these diverse, raw data and process it into easy-to-understand plots, analysis and reports. A hard-coded workflow, in contrast, is specifically made to analyze one specific data format and spectra and produces a fixed output format, e.g., the <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> of the analyzed molecule.
</figcaption>
</figure>
<section id="prompting" class="level3" data-number="7.7.1">
<h3 data-number="7.7.1" class="anchored" data-anchor-id="prompting"><span class="header-section-number">7.7.1</span> Prompting</h3>
<p>Initial evaluations demonstrated that <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> can support basic data analysis workflows. [<span class="citation" data-cites="Fu2025large">Fu et al. (<a href="#ref-Fu2025large" role="doc-biblioref">2025</a>)</span>] For example, in chemistry, this enabled the classification of <span data-acronym-label="xps" data-acronym-form="singular+short">xps</span> signals [<span class="citation" data-cites="decurt2024large">Curtò et al. (<a href="#ref-decurt2024large" role="doc-biblioref">2024</a>)</span>] based on peak positions, intensities, or characteristic spectral patterns).</p>
<p>Spectroscopic data are not always available in structured textual form. In many practical cases, it appears as raw plots or images, making direct interpretation by <span data-acronym-label="vlm" data-acronym-form="plural+short">vlms</span> a more natural starting point for automated analysis. A broad assessment of <span data-acronym-label="vlm" data-acronym-form="singular+short">vlm</span>-based spectral analysis was introduced with the benchmark [<span class="citation" data-cites="alampara2024probing">Alampara et al. (<a href="#ref-alampara2024probing" role="doc-biblioref">2024</a>)</span>], which systematically evaluates how <span data-acronym-label="vlm" data-acronym-form="plural+short">vlms</span> interpret experimental data in chemistry and materials science—including various types of spectra such as <span data-acronym-label="ir" data-acronym-form="singular+short">ir</span>, <span data-acronym-label="nmr" data-acronym-form="singular+short">nmr</span>, and <span data-acronym-label="xrd" data-acronym-form="singular+short">xrd</span>q—directly from images. They showed that while <span data-acronym-label="vlm" data-acronym-form="plural+short">vlms</span> can correctly extract isolated features from plots, the performance substantially drops in tasks requiring deeper spatial reasoning. To overcome these limitations, <span class="citation" data-cites="kawchak2024high">Kawchak (<a href="#ref-kawchak2024high" role="doc-biblioref">2024</a>)</span> explored two-step pipelines that decouple visual perception from chemical reasoning. First, the model interprets each spectrum individually (e.g., converting <span data-acronym-label="ir" data-acronym-form="singular+short">ir</span>, <span data-acronym-label="nmr" data-acronym-form="singular+short">nmr</span>, or <span data-acronym-label="ms" data-acronym-form="singular+short">ms</span> images into textual peak descriptions), and second, a <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> analyzes these outputs to propose a molecular structure based on the molecular formula.</p>
</section>
<section id="agentic-systems" class="level3" data-number="7.7.2">
<h3 data-number="7.7.2" class="anchored" data-anchor-id="agentic-systems"><span class="header-section-number">7.7.2</span> Agentic Systems</h3>
<p>Beyond zero-shot prompting of <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>, one can develop agentic systems that combine multiple analysis steps end-to-end. In this regard, <span class="citation" data-cites="ghareeb2025robin0">Ghareeb et al. (<a href="#ref-ghareeb2025robin0" role="doc-biblioref">2025</a>)</span> developed —a multi-agent system for assisting biological research with hypothesis generation (see <a href="#fig:hypothesis-generation" data-reference-type="ref+Label" data-reference="fig:hypothesis-generation">3</a>) and experimental analysis. The data analysis agent performs autonomous analysis of raw or preprocessed experimental data, such as <span data-acronym-label="rna" data-acronym-form="singular+short">rna</span> sequencing and flow cytometry. Given a user prompt (e.g., “<span data-acronym-label="rna" data-acronym-form="singular+short">rna</span> sequencing differential expression analysis”), executes code in a Jupyter notebook to process the data, apply relevant statistical methods, and generate interpretable outputs. For flow cytometry, this includes gating strategies and significance testing, while for <span data-acronym-label="rna" data-acronym-form="singular+short">rna</span> sequencing, it encompasses differential expression and gene ontology enrichment analysis. Currently, only these two data types are supported, and expert-designed prompts are still required to ensure reliable results.</p>
<p>Recent work extends agentic systems beyond single-step data evaluation toward executing and optimizing entire workflows. <span class="citation" data-cites="mandal2024autonomous">Mandal et al. (<a href="#ref-mandal2024autonomous" role="doc-biblioref">2024</a>)</span> introduced (Artificially Intelligent Lab Assistant) utilizing <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-agents to plan, code, execute, and revise complete <span data-acronym-label="afm" data-acronym-form="singular+short">afm</span> analysis pipelines. The system handles tasks such as image processing, defect detection, clustering, and extraction of physical parameters. Compared to systems like , shifts the focus from generating summaries to performing and improving full experimental analyses with minimal user input while maintaining transparency and reproducibility through code and reports.</p>
</section>
<section id="current-limitations" class="level3" data-number="7.7.3">
<h3 data-number="7.7.3" class="anchored" data-anchor-id="current-limitations"><span class="header-section-number">7.7.3</span> Current Limitations</h3>
<p>While <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> offer promising capabilities for automating scientific data analysis, several limitations remain. Recent evaluations such as [<span class="citation" data-cites="tian2024scicode">Tian et al. (<a href="#ref-tian2024scicode" role="doc-biblioref">2024</a>)</span>] have shown that even state-of-the-art models like and frequently produce syntactically correct but semantically incorrect code when tasked with common data analysis steps, such as reading files, applying filters, or generating plots. Typical issues include incorrect column usage, or inconsistent output formatting.</p>
<p>These technical shortcomings are reinforced by the model’s sensitivity to prompt formulation. As demonstrated by <span class="citation" data-cites="Yan2020auto">Yan and He (<a href="#ref-Yan2020auto" role="doc-biblioref">2020</a>)</span> and <span class="citation" data-cites="alampara2024probing">Alampara et al. (<a href="#ref-alampara2024probing" role="doc-biblioref">2024</a>)</span>, minor changes in wording or structure can lead to significantly different outputs, highlighting a lack of robustness in prompt-based control.</p>
<p>Together, these findings suggest that while foundation models can generalize across diverse data formats and analysis types, their current performance is not yet sufficient for fully autonomous use in scientific analysis settings. Robust prompting strategies, post-generation validation, and human oversight remain essential components in practice.</p>
</section>
</section>
<section id="reporting" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="reporting"><span class="header-section-number">7.8</span> Reporting</h2>
<p>To share insights obtained from data analysis, one often converts them into scientific reports. Also, in this step, <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> can take a central role, which we discuss in the following.</p>
<p>Reporting refers to converting scientific results into shareable reports, scientific publications, blogs, and other forms of content. This section describes two main applications of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> in scientific reporting: converting data into explanations and the first steps towards using these models as fully-fledged writing assistants.</p>
<section id="from-data-to-explanation" class="level3" data-number="7.8.1">
<h3 data-number="7.8.1" class="anchored" data-anchor-id="from-data-to-explanation"><span class="header-section-number">7.8.1</span> From Data to Explanation</h3>
<p>The lack of explainability of <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> predictions generates skepticism among experimental chemists[<span class="citation" data-cites="wellawatte2025human">Wellawatte and Schwaller (<a href="#ref-wellawatte2025human" role="doc-biblioref">2025</a>)</span>], hindering the wider adoption of such models.[<span class="citation" data-cites="wellawatte2022model">Wellawatte, Seshadri, and White (<a href="#ref-wellawatte2022model" role="doc-biblioref">2022</a>)</span>] One promising approach to address this challenge is to convey explanations of model predictions in natural language. An approach proposed by <span class="citation" data-cites="wellawatte2025human">Wellawatte and Schwaller (<a href="#ref-wellawatte2025human" role="doc-biblioref">2025</a>)</span> is to couple <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> with feature importance analysis tools, such as <span data-acronym-label="shap" data-acronym-form="singular+short">shap</span> or <span data-acronym-label="lime" data-acronym-form="singular+short">lime</span>. In this framework, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can additionally interact with tools such as <span data-acronym-label="rag" data-acronym-form="singular+short">rag</span> over to provide evidence-based explanations.</p>
</section>
<section id="writing-assistance" class="level3" data-number="7.8.2">
<h3 data-number="7.8.2" class="anchored" data-anchor-id="writing-assistance"><span class="header-section-number">7.8.2</span> Writing Assistance</h3>
<p>When considering <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span>-based assistance in scientific writing, we can distinguish two primary modes: systems that aid authors during the active writing process and tools that optimize or refine scientific articles after initial drafting.</p>
<p>The former refers to the use of writing copilots that can suggest syntax improvement, identify text redundancies,[<span class="citation" data-cites="khalifa2024using">Khalifa and Albadawy (<a href="#ref-khalifa2024using" role="doc-biblioref">2024</a>)</span>] caption figures and tables[<span class="citation" data-cites="hsu2021scicap">Hsu, Giles, and Huang (<a href="#ref-hsu2021scicap" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="selivanov2023medical">Selivanov et al. (<a href="#ref-selivanov2023medical" role="doc-biblioref">2023</a>)</span>], or provide caption-figure match evaluation[<span class="citation" data-cites="hsu2023gpt04">Hsu et al. (<a href="#ref-hsu2023gpt04" role="doc-biblioref">2023</a>)</span>], but also more specific applications like writing alt-text (descriptive text that explains the meaning and purpose of an image in digital content)[<span class="citation" data-cites="singh2024figura11y">Singh, Wang, and Bragg (<a href="#ref-singh2024figura11y" role="doc-biblioref">2024</a>)</span>].</p>
<p>Under the latter mode, <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> can be used to assist non-native English speakers with scientific writing [<span class="citation" data-cites="giglio2023use">Giglio and Costa (<a href="#ref-giglio2023use" role="doc-biblioref">2023</a>)</span>]. It could even allow authors to write in their native language and use <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> for communicating scientific results in English.</p>
<p>Another application of <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> is to assist with completing checklists before submitting a publication. For example, <span class="citation" data-cites="goldberg2024usefulness">Goldberg et al. (<a href="#ref-goldberg2024usefulness" role="doc-biblioref">2024</a>)</span> benchmark the use of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> in completing the author checklist for the <span data-acronym-label="neurips" data-acronym-form="singular+short">neurips</span> 2025. They concluded that <span class="math inline">\(70\%\)</span> of the authors found the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-assistant useful, with the same fraction indicating they would revise their own checklist based on the model feedback.</p>
</section>
<section id="vision" class="level3" data-number="7.8.3">
<h3 data-number="7.8.3" class="anchored" data-anchor-id="vision"><span class="header-section-number">7.8.3</span> Vision</h3>
<p>Few have ventured into fully automating the writing process.[<span class="citation" data-cites="yamada2025ai">Yamada et al. (<a href="#ref-yamada2025ai" role="doc-biblioref">2025</a>)</span>] While at its inception, reporting using <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> has tremendous potential. In <a href="#fig:writing_with_ml" data-reference-type="ref+Label" data-reference="fig:writing_with_ml">7</a> we showcase how the future of reporting could look like if we were to integrate <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> at each step of the process.</p>
<figure id="fig:writing_with_ml" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure18.png" width="100%" class="figure-img">
<figcaption>
<strong>Vision for <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> in reporting, a visualization of the scientific writing process</strong>. <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> can be used at every stage of the process. For creating the pre-print, we can utilize the multimodal capabilities of these models to write detailed captions for figures. For the peer-review process, we can harness the ability of <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> to summarize and prioritize information (e.g., design a time-efficient plan to address the peer review). When converting a document from a peer-reviewed pre-print, we often need to implement the publisher’s requirements. In this case, we can make use of agentic systems that would assist with minor text fixes or document restructuring.
</figcaption>
</figure>
<p>An idea entertained by <span class="citation" data-cites="li2023teach">C. Li et al. (<a href="#ref-li2023teach" role="doc-biblioref">2023</a>)</span> in the context of education is personalized writing. However, it is still widely unexplored in its goal: to make science accessible to everyone. A personalized model that learns user preferences and domain expertise can be used to deliver the message of a scientific article in simpler terms. As a result, we might observe a rise in cross-domain scientific collaborations and a rising interest in science.</p>
</section>
</section>
</section>
<section id="accelerating-applications" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Accelerating Applications</h1>
<p>The application of accelerated approaches in the scientific discovery cycle (see <a href="#fig:applications" data-reference-type="ref+Label" data-reference="fig:applications">1</a>) hinges on their ability to streamline and enhance each stage of the process. However, a fundamental challenge in effectively implementing these approaches lies in the choice of machine-readable representation.</p>
<p>This challenge is particularly evident in the representation of molecules and materials, which must balance computational efficiency with the preservation of structural, compositional, and functional properties. Take, for example, the high-temperature superconductor . While atomic positions and coordinates are theoretically sufficient to solve the Schrödinger equation and describe this material, such a representation may not provide the adaptability necessary for diverse tasks. What defines a good representation depends on the problem. [<span class="citation" data-cites="huang2016understanding">B. Huang and Lilienfeld (<a href="#ref-huang2016understanding" role="doc-biblioref">2016</a>)</span>]. A representation designed to predict critical temperature must efficiently encode the relationship between oxygen stoichiometry and superconducting properties, emphasizing features like oxygen vacancy patterns and charge transfer mechanisms. Conversely, a representation for structural stability might prioritize different geometric or bonding characteristics.</p>
<p>This tension has led to three primary strategies for representing molecules and materials (read <a href="03-architectures.html#sec:common_representations" data-reference-type="ref+Label" data-reference="sec:common_representations">[sec:common_representations]</a> to learn in detail about the different representations that currently exist). First, domain-specific text-based formats—such as <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> [<span class="citation" data-cites="weininger1988smiles">Weininger (<a href="#ref-weininger1988smiles" role="doc-biblioref">1988</a>)</span>], <span data-acronym-label="selfies" data-acronym-form="singular+short">selfies</span> [<span class="citation" data-cites="krenn2020self">Krenn et al. (<a href="#ref-krenn2020self" role="doc-biblioref">2020</a>)</span>], and <span data-acronym-label="cif" data-acronym-form="singular+short">cif</span> [<span class="citation" data-cites="hall1991crystallographic">Hall, Allen, and Brown (<a href="#ref-hall1991crystallographic" role="doc-biblioref">1991</a>)</span>]—offer compact, machine-readable encodings of structural information. While these necessarily omit certain physical details, their computational tractability has enabled breakthroughs, as demonstrated by <span class="citation" data-cites="jablonka2024leveraging">Jablonka et al. (<a href="#ref-jablonka2024leveraging" role="doc-biblioref">2024</a>)</span> in their <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-based generation of valid molecular and material structures.</p>
<p>Yet, the question remains: Which representation is optimal for a given task? Future advances in accelerated discovery will likely hinge on adaptive representations that dynamically balance these competing demands.</p>
<section id="sec:prediction" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="sec:prediction"><span class="header-section-number">8.1</span> Property Prediction</h2>
<p><span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> have emerged as a powerful tool for predicting molecular and material properties, offering an alternative to traditional quantum mechanical calculations or specialized <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> models. Current <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span>-driven property prediction tasks span both classification and regression. Unlike conventional approaches that rely on task-specific architectures and extensively labeled data, <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> have demonstrated strong generalization capabilities across diverse domains, efficiently adapting to various prediction tasks. Their success extends to multiple datasets, from standardized benchmarks such as [<span class="citation" data-cites="wu2018moleculenet">Z. Wu et al. (<a href="#ref-wu2018moleculenet" role="doc-biblioref">2018</a>)</span>], to curated datasets targeting specific applications such as antibacterial activity [<span class="citation" data-cites="chithrananda2020chemberta">Chithrananda, Grand, and Ramsundar (<a href="#ref-chithrananda2020chemberta" role="doc-biblioref">2020</a>)</span>] or photovoltaic efficiency[<span class="citation" data-cites="aneesh2025semantic">Aneesh et al. (<a href="#ref-aneesh2025semantic" role="doc-biblioref">2025</a>)</span>].</p>
<p>Three key methodologies have been explored to adapt <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> for property prediction: prompting techniques (see <a href="03-architectures.html#sec:prompting" data-reference-type="ref+Label" data-reference="sec:prompting">[sec:prompting]</a>), fine-tuning (see <a href="03-architectures.html#sec:fine-tuning" data-reference-type="ref+Label" data-reference="sec:fine-tuning">[sec:fine-tuning]</a>) on domain-specific data, and <span data-acronym-label="rag" data-acronym-form="singular+short">rag</span> (see <a href="03-architectures.html#sec:rag" data-reference-type="ref+Label" data-reference="sec:rag">[sec:rag]</a>) approaches that combine <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> with external knowledge bases.</p>
<div class="minipage">
<p><strong>Key:</strong> P = prompting; FT = fine-tuned model; RAG = retrieval-augmented generation; C = Classification; R = Regression</p>
</div>
<section id="prompting-1" class="level3" data-number="8.1.1">
<h3 data-number="8.1.1" class="anchored" data-anchor-id="prompting-1"><span class="header-section-number">8.1.1</span> Prompting</h3>
<p>Prompt engineering involves designing targeted instructions to guide <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> in performing specialized tasks without altering their underlying parameters by leveraging their embedded knowledge. In molecular and materials science, this strategy goes beyond simply asking a model to predict properties. It also includes carefully structured prompts to elicit detailed molecular and material descriptions directly from the model’s pre-trained knowledge.</p>
<p><span class="citation" data-cites="liu2025integrating">H. Liu et al. (<a href="#ref-liu2025integrating" role="doc-biblioref">2025</a>)</span> conducted a comprehensive evaluation of different prompting techniques to predict the properties of organic small molecules and crystal materials. Some of these techniques included domain-knowledge (prior knowledge was embedded in the prompt), expert (role-play instructions), and few-shot <span data-acronym-label="cot" data-acronym-form="singular+short">cot</span> (the text<em>“Let’s think step by step”</em> is added) prompting. Of these, domain knowledge achieved maximum performance. However, their evaluation was limited to a relatively small set of molecules and tasks, and the effectiveness of their domain-knowledge approach may not generalize to other molecular property domains.</p>
<p>Building on these foundational prompting strategies, few-shot prompting approaches leverage <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> to enhance performance through selected examples <span class="citation" data-cites="liu2024moleculargpt">Y. Liu et al. (<a href="#ref-liu2024moleculargpt" role="doc-biblioref">2024</a>)</span> used <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> string representations of molecules with few-shot <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span>, retrieving structurally similar molecules as demonstrations to enhance property prediction. This approach highlights how <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> can transfer knowledge from similar molecule examples without requiring model fine-tuning for each task. However, the effectiveness of <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> depends on the quality of retrieved examples.</p>
<p><span class="citation" data-cites="fifty2023incontext">Fifty, Leskovec, and Thrun (<a href="#ref-fifty2023incontext" role="doc-biblioref">2023</a>)</span> moved beyond direct text prompting of molecules and introduced <span data-acronym-label="camp" data-acronym-form="singular+short">camp</span>: an <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> algorithm that uses a two-stage encoding approach without relying on pre-trained <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>. First, a specialized <span data-acronym-label="mpnn" data-acronym-form="singular+short">mpnn</span> encodes molecule graphs into molecular embeddings rather than processing them as raw text. These embeddings are then fed into a transformer encoder, which learns contextualized representations across the support set (a small collection of labeled molecule-property pairs) and the unlabeled query molecules. They demonstrated <span data-acronym-label="camp" data-acronym-form="singular+short">camp</span>’s ability to outperform existing few-shot learning baselines by providing relevant molecular examples within the prompt context. However, this approach is constrained by the context-length limitations of the underlying <span data-acronym-label="lm" data-acronym-form="plural+short">lms</span> and the challenge of selecting optimal demonstration examples.</p>
<p>More sophisticated approaches have leveraged prompting as part of multi-modal frameworks. The pipeline by <span class="citation" data-cites="zheng2025large">Y. Zheng et al. (<a href="#ref-zheng2025large" role="doc-biblioref">2025</a>)</span> employs specialized prompts to guide <span data-acronym-label="lm" data-acronym-form="plural+short">lms</span> through their pre-trained knowledge on scientific literature, generating known rules (e.g., molecules weighing under 500&nbsp;Da are more likely to pass the blood-brain barrier) that transform molecules into feature vectors (e.g.&nbsp;could translate to a vector <span class="math inline">\([2,46.07,1,1]\)</span> where each number represents a feature of the molecule, in this example [# , MW, # -bond donors, # -bond acceptors]) for use with a random forest model, which they consider “interpretable”. This approach outperformed specialized <span data-acronym-label="sota" data-acronym-form="singular+short">sota</span> models across <span class="math inline">\(58\)</span> benchmark tasks, while providing interpretable reasoning about prediction logic (see <a href="#tab:property_prediction_models" data-reference-type="ref+Label" data-reference="tab:property_prediction_models">[tab:property_prediction_models]</a> for properties predicted by this model). However, its reliance on rule extraction may limit its ability to capture complex, non-linear relationships that specialized deep learning models can identify.</p>
<section id="llms-as-feature-extractors" class="level4" data-number="8.1.1.1">
<h4 data-number="8.1.1.1" class="anchored" data-anchor-id="llms-as-feature-extractors"><span class="header-section-number">8.1.1.1</span> <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> as Feature Extractors</h4>
<p>Another emerging application of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> is their use as “feature extractors”, where they generate textual or embedded representations of molecules or materials. For instance, in materials science, <span class="citation" data-cites="aneesh2025semantic">Aneesh et al. (<a href="#ref-aneesh2025semantic" role="doc-biblioref">2025</a>)</span> employed <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> to generate text embeddings of perovskite solar cell compositions. These embeddings were subsequently used to train a <span data-acronym-label="gnn" data-acronym-form="singular+short">gnn</span> for predicting power conversion efficiency, demonstrating the potential of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> to enhance feature representation in materials informatics. Similarly, in the molecular domain, <span class="citation" data-cites="srinivas2024crossmodal">Srinivas and Runkana (<a href="#ref-srinivas2024crossmodal" role="doc-biblioref">2024b</a>)</span> used zero-shot <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> prompting (see <a href="#box:%20cot_prompting" data-reference-type="ref+Label" data-reference="box: cot_prompting">[box: cot_prompting]</a> for prompt examples) to generate detailed textual descriptions of molecular functional groups, which are used to train a small <span data-acronym-label="lm" data-acronym-form="singular+short">lm</span>. This <span data-acronym-label="lm" data-acronym-form="singular+short">lm</span> is used to compute text-level embeddings of molecules. Simultaneously, they generate molecular graph-level embeddings from <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> string molecular graph inputs. They finally integrate the graph and text-level embeddings to produce a semantically enriched embedding.</p>
<div class="promptbox">
<p>[]{#box: cot_prompting label=“box: cot_prompting”} <strong><span data-acronym-label="cot" data-acronym-form="singular+short">cot</span> Prompting</strong>[<span class="citation" data-cites="srinivas2024crossmodal">Srinivas and Runkana (<a href="#ref-srinivas2024crossmodal" role="doc-biblioref">2024b</a>)</span>]<br>
Prompt 1: What is the molecular structure of this chemical <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> string? Could you describe its atoms, bonds, functional groups, and overall arrangement?<br>
Prompt 2: What are the physical properties of this molecule, such as its boiling point and melting point?<br>
...<br>
Prompt 14: Are there any environmental impacts associated with the production, use, or disposal of this molecule?</p>
</div>
<p>In a different implementation of fine-tuning, <span class="citation" data-cites="balaji2023gptmolberta">Balaji et al. (<a href="#ref-balaji2023gptmolberta" role="doc-biblioref">2023</a>)</span> used to generate text descriptions of molecules that were then used to train a (125M) model for property prediction, showing how <span data-acronym-label="lm" data-acronym-form="singular+short">lm</span>-generated representations can access latent spaces that <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> strings alone might not capture. Similarly, <span class="citation" data-cites="li2024unveiling">Z. Li et al. (<a href="#ref-li2024unveiling" role="doc-biblioref">2024</a>)</span> introduced the framework, which fine-tunes [<span class="citation" data-cites="ahmad2022chemberta">Ahmad et al. (<a href="#ref-ahmad2022chemberta" role="doc-biblioref">2022</a>)</span>] on Group <span data-acronym-label="selfies" data-acronym-form="singular+short">selfies</span> [<span class="citation" data-cites="cheng2023group">Cheng et al. (<a href="#ref-cheng2023group" role="doc-biblioref">2023</a>)</span>] (a functional group-based molecular representation) to then extract a single <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-derived embedding of molecules that captures the chemical semantics at the functional group level. This allowed them to determine which functional groups or fragments contribute to molecular properties, which in turn can be converted into reliable explanations of said properties.</p>
</section>
</section>
<section id="sec:prediction_FT" class="level3" data-number="8.1.2">
<h3 data-number="8.1.2" class="anchored" data-anchor-id="sec:prediction_FT"><span class="header-section-number">8.1.2</span> Fine-Tuning</h3>
<figure id="fig:gptchem" class="figure">
<img src="media/figures/property_gptchem.png" width="100%" class="figure-img">
<figcaption>
<strong>Fine-tuned for predicting solid-solution formation in high-entropy alloys</strong> Performance comparison of different <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> approaches as a function of the number of training points. Results are shown for (blue), transformer (orange), fine-tuned (red), with error bars showing standard error of the mean. The non-Google test set shows the fine-tuned model tested on compounds without an exact Google search match (dark red). The dashed line shows performance using random forest. achieves comparable accuracy to traditional approaches with significantly fewer training examples. Data adapted from <span class="citation" data-cites="jablonka2024leveraging">Jablonka et al. (<a href="#ref-jablonka2024leveraging" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
<section id="lift" class="level4" data-number="8.1.2.1">
<h4 data-number="8.1.2.1" class="anchored" data-anchor-id="lift"><span class="header-section-number">8.1.2.1</span> <span data-acronym-label="lift" data-acronym-form="singular+short">lift</span></h4>
<p><span class="citation" data-cites="dinh2022lift">Dinh et al. (<a href="#ref-dinh2022lift" role="doc-biblioref">2022</a>)</span> showed that reformulating regression and classification as <span data-acronym-label="qa" data-acronym-form="singular+short">qa</span> tasks enables the use of unmodified model architecture while improving performance (see <a href="03-architectures.html#sec:fine-tuning" data-reference-type="ref+Label" data-reference="sec:fine-tuning">[sec:fine-tuning]</a> for a deeper discussion of <span data-acronym-label="lift" data-acronym-form="singular+short">lift</span>). In recognizing the scarcity of experimental data and acknowledging the persistence of this limitation, <span class="citation" data-cites="jablonka2024leveraging">Jablonka et al. (<a href="#ref-jablonka2024leveraging" role="doc-biblioref">2024</a>)</span> designed a <span data-acronym-label="lift" data-acronym-form="singular+short">lift</span>-based framework using fine-tuned on task-specific small datasets (see <a href="#tab:property_prediction_models" data-reference-type="ref+Label" data-reference="tab:property_prediction_models">[tab:property_prediction_models]</a>). They seminally demonstrated that fine-tuned can match or surpass specialized <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> models in various chemistry tasks. A key finding was fine-tuned ’s ability to generalize beyond training data. When tested on compounds absent from Google Search (and likely its training data), it performed well, proving that it was not simply recalling memorized information (see <a href="#fig:gptchem" data-reference-type="ref+Label" data-reference="fig:gptchem">8</a>).</p>
<p>In a follow-up to <span class="citation" data-cites="jablonka2024leveraging">Jablonka et al. (<a href="#ref-jablonka2024leveraging" role="doc-biblioref">2024</a>)</span>’s work, <span class="citation" data-cites="vanherck2025assessment">Van Herck et al. (<a href="#ref-vanherck2025assessment" role="doc-biblioref">2025</a>)</span> systematically evaluated this approach across 22 diverse real-world chemistry case studies using three open-source models. They demonstrate that fine-tuned <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can effectively predict various material properties. For example, they achieved <span class="math inline">\(96\%\)</span> accuracy in predicting the adhesive free-energy of polymers, outperforming traditional <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> methods like random forest (<span class="math inline">\(90\%\)</span> accuracy). When predicting properties of monomers using <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> notation, the fine-tuned models reached average accuracies of <span class="math inline">\(84\%\)</span> across four different properties. Particularly notable was the ability of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> to work with non-standard inputs, like in a protein phase separation study they did, where raw protein sequences could be directly input without pre-processing and achieve <span class="math inline">\(95\%\)</span> prediction accuracy. At the same time, when training datasets were very small (15 data points), the predictive accuracy of all fine-tuned models was lower than the random baseline (e.g.&nbsp;MOF synthesis). These case studies preliminarily demonstrate that these models can achieve predictive performance with some small datasets, work with various chemical representations (<span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span>, <span data-acronym-label="mof" data-acronym-form="singular+short">mof</span>id, and <span data-acronym-label="iupac" data-acronym-form="singular+short">iupac</span> names), and can outperform traditional <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> approaches for some material property prediction tasks.</p>
<p>In the materials domain, fine-tunes [<span class="citation" data-cites="raffel2020exploring">Raffel et al. (<a href="#ref-raffel2020exploring" role="doc-biblioref">2020</a>)</span>] to predict crystalline material properties from text descriptions generated by [<span class="citation" data-cites="ganose2019robocrystallographer">Ganose and Jain (<a href="#ref-ganose2019robocrystallographer" role="doc-biblioref">2019</a>)</span>]. By discarding ’s decoder and adding task-specific prediction heads, the approach reduces computational overhead while leveraging the model’s ability to process structured crystal descriptions. The method demonstrates that natural language representations can effectively capture key material features, offering an alternative to traditional graph-based models like <span data-acronym-label="gnn" data-acronym-form="plural+short">gnns</span>.</p>
<p>Fine-tuning has been used to adapt <span data-acronym-label="ssm" data-acronym-form="plural+short">ssms</span> like Mamba (see <a href="03-architectures.html#sec:example_architectures" data-reference-type="ref+Label" data-reference="sec:example_architectures">[sec:example_architectures]</a>). By pre-training on 91 million molecules, the Mamba-based model outperformed transformer methods ([<span class="citation" data-cites="krzyzanowski2025exploring">Krzyzanowski, Pickett, and Pogány (<a href="#ref-krzyzanowski2025exploring" role="doc-biblioref">2025</a>)</span>]) in reaction yield prediction (e.g., Buchwald-Hartwig cross-coupling) and achieved competitive results in molecular property prediction benchmarks.[<span class="citation" data-cites="soares2025mamba-based">Soares et al. (<a href="#ref-soares2025mamba-based" role="doc-biblioref">2025</a>)</span>]</p>
</section>
<section id="foundational-gnns-and-mlips" class="level4" data-number="8.1.2.2">
<h4 data-number="8.1.2.2" class="anchored" data-anchor-id="foundational-gnns-and-mlips"><span class="header-section-number">8.1.2.2</span> Foundational <span data-acronym-label="gnn" data-acronym-form="plural+short">gnns</span> and <span data-acronym-label="mlip" data-acronym-form="plural+short">mlips</span></h4>
<p>The fine-tuning approach has been applied to “foundational <span data-acronym-label="gnn" data-acronym-form="plural+short">gnns</span>” [<span class="citation" data-cites="sypetkowski2024scalability">Sypetkowski et al. (<a href="#ref-sypetkowski2024scalability" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="shoghi2023molecules">Shoghi et al. (<a href="#ref-shoghi2023molecules" role="doc-biblioref">2023</a>)</span>] and <span data-acronym-label="mlip" data-acronym-form="plural+short">mlips</span>, approaches distinct from <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>. For example, [<span class="citation" data-cites="shoghi2023molecules">Shoghi et al. (<a href="#ref-shoghi2023molecules" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="sypetkowski2024scalability">Sypetkowski et al. (<a href="#ref-sypetkowski2024scalability" role="doc-biblioref">2024</a>)</span>] show <span data-acronym-label="sota" data-acronym-form="singular+short">sota</span> performance on property prediction tasks. “Foundational” <span data-acronym-label="mlip" data-acronym-form="plural+short">mlips</span> pre-trained on large datasets encompassing many chemical elements can be fine-tuned for specific downstream tasks [<span class="citation" data-cites="batatia2022mace">Batatia et al. (<a href="#ref-batatia2022mace" role="doc-biblioref">2022</a>)</span>], such as calculating sublimation enthalpies of molecular crystal polymorphs [<span class="citation" data-cites="kaur2025data">Kaur et al. (<a href="#ref-kaur2025data" role="doc-biblioref">2025</a>)</span>].</p>
</section>
<section id="limitations-1" class="level4" data-number="8.1.2.3">
<h4 data-number="8.1.2.3" class="anchored" data-anchor-id="limitations-1"><span class="header-section-number">8.1.2.3</span> Limitations</h4>
<p>One central challenge is finding balance in datasets. In practical applications, researchers often have many more examples of poor-performing materials than optimal ones, resulting in unbalanced datasets that can diminish model performance. <span class="citation" data-cites="vanherck2025assessment">Van Herck et al. (<a href="#ref-vanherck2025assessment" role="doc-biblioref">2025</a>)</span> point out that in the catalyzed cleavage reaction study, only <span class="math inline">\(3.8\%\)</span> of catalysts were labeled as “good”, forcing researchers to reduce their training set significantly to maintain balance. They also note that <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> struggle with highly complex or noisy datasets, as seen in their study of catalytic isomerization, where even after hyperparameter optimization, the models failed to achieve meaningful predictive power due to the high noise in the experimental data and limited sample size. Finally, they note that although <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can work with different chemical representations, the choice of representation significantly impacts performance. For example, when predicting polymerization rates, models using <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> notation significantly outperformed those using <span data-acronym-label="iupac" data-acronym-form="singular+short">iupac</span> names, indicating that representation selection remains an important consideration.</p>
<p>Fine-tuning effectively adapts <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> to specialized chemistry tasks, but its dependence on static datasets hinders adaptability to new or evolving knowledge. <span data-acronym-label="rag" data-acronym-form="singular+short">rag</span>, whose fundamentals are described in detail in <a href="03-architectures.html#sec:rag" data-reference-type="ref+Label" data-reference="sec:rag">[sec:rag]</a>, overcomes these limitations by dynamically integrating external data sources, enabling more flexible and up-to-date reasoning.</p>
</section>
</section>
<section id="agents" class="level3" data-number="8.1.3">
<h3 data-number="8.1.3" class="anchored" data-anchor-id="agents"><span class="header-section-number">8.1.3</span> Agents</h3>
<p>Caldas Ramos et al.&nbsp;introduce , a framework that processes natural-language queries about material properties using an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> to decide which of the available tools such as the Materials Project <span data-acronym-label="api" data-acronym-form="singular+short">api</span>, the Reaction-Network package, or Google Search to use to generate a response. [<span class="citation" data-cites="Jablonka2023">Jablonka et al. (<a href="#ref-Jablonka2023" role="doc-biblioref">2023</a>)</span>] employs a <span data-acronym-label="react" data-acronym-form="singular+short">react</span> prompt (see <a href="03-architectures.html#sec:arch_agents" data-reference-type="ref+Label" data-reference="sec:arch_agents">[sec:arch_agents]</a> to read more about <span data-acronym-label="react" data-acronym-form="singular+short">react</span>), to convert prompts such as <em>“Is <span class="math inline">\(Fe_2O_3\)</span> magnetic?”</em> or <em>“What is the band gap of Mg(Fe3O3)2?”</em> into queries for Materials Project <span data-acronym-label="api" data-acronym-form="singular+short">api</span>. The system processes multi-step prompts through logical reasoning, for example, when asked <em>“If Mn2FeO3 is not metallic, what is its band gap?”</em>, the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> system creates a two-step workflow to first verify metallicity before retrieving the band gap.</p>
<p>Building on this foundation of agent-based materials querying, <span class="citation" data-cites="chiang2024llamp">Chiang et al. (<a href="#ref-chiang2024llamp" role="doc-biblioref">2024</a>)</span> advanced the approach with , a framework that employs “hierarchical” <span data-acronym-label="react" data-acronym-form="singular+short">react</span> agents to interact with computational and experimental data. This “hierarchical” framework employs a supervisor-assistant agent architecture where a complex problem is broken down and tasks are delegated to domain-specific agents. addresses the challenge of hallucinations more effectively than standard <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> approaches by grounding responses in retrieved materials databases, retrieving materials data (e.g., crystal structures, elastic tensors) while counteracting systematic <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> biases in property predictions. These biases include the tendency for <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> to overestimate certain properties like bulk moduli and to exhibit errors in bandgap predictions based on compositional patterns learned during training rather than physical principles.</p>
</section>
<section id="sec:property_core_limits" class="level3" data-number="8.1.4">
<h3 data-number="8.1.4" class="anchored" data-anchor-id="sec:property_core_limits"><span class="header-section-number">8.1.4</span> Core Limitations</h3>
<figure id="fig:property_limitations" class="figure">
<img src="media/figures/property_mattext.png" width="100%" class="figure-img">
<figcaption>
<strong>Normalized error distributions for materials property prediction models across different architectures</strong>. Each point represents the normalized error of a model on a specific property prediction task. Normalization was achieved with min/max values of each dataset to produce a range of errors between 0 and 1. The first column (blue) shows <span data-acronym-label="gnn" data-acronym-form="singular+short">gnn</span> based models, the second column (red) displays <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> approaches, and the third column (orange) represents other baseline methods and <span data-acronym-label="sota" data-acronym-form="singular+short">sota</span> models including . <span class="citation" data-cites="Wang_2021">(<a href="#ref-Wang_2021" role="doc-biblioref">A. Y.-T. Wang et al. 2021</a>)</span> Lower values indicate better predictive performance. Data adapted from <span class="citation" data-cites="alampara2024mattext">Alampara, Miret, and Jablonka (<a href="#ref-alampara2024mattext" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
<p><span class="citation" data-cites="alampara2024mattext">Alampara, Miret, and Jablonka (<a href="#ref-alampara2024mattext" role="doc-biblioref">2024</a>)</span> introduced , a framework for evaluating <span data-acronym-label="lm" data-acronym-form="plural+short">lms</span> ability to predict properties of materials using text-based representations. Their findings indicate that current <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> (including pre-trained and fine-tuned ) are effective for tasks relying purely on compositional information (e.g., element types and local bonding patterns), but struggle to leverage geometric or positional information encoded in text, as reflected in <a href="#fig:property_limitations" data-reference-type="ref+Label" data-reference="fig:property_limitations">9</a>. This observation suggests that transformer-based architectures may be fundamentally limited to applications where spatial understanding is not required. Their experiments with data scaling and text representations reveal that increasing pre-training data or adding geometric details fails to improve downstream property prediction, challenging the conventional assumption that larger models and datasets universally enhance performance. [<span class="citation" data-cites="frey2023neural">Frey et al. (<a href="#ref-frey2023neural" role="doc-biblioref">2023</a>)</span>] Notably, <span class="citation" data-cites="frey2023neural">Frey et al. (<a href="#ref-frey2023neural" role="doc-biblioref">2023</a>)</span> demonstrated power-law scaling in chemical <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, but ’s results imply that such scaling may not overcome architectural biases against geometric reasoning in materials tasks.[<span class="citation" data-cites="gruver2024promises">Gruver et al. (<a href="#ref-gruver2024promises" role="doc-biblioref">2024</a>)</span>]</p>
</section>
</section>
<section id="sec:mol_generation" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="sec:mol_generation"><span class="header-section-number">8.2</span> Molecular and Material Generation</h2>
<figure id="fig:generation" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure21.png" width="100%" class="figure-img">
<figcaption>
<strong>Pipeline for molecular and materials generation</strong> The workflow begins with input structures represented in various formats, which are used to train <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> models to generate novel molecular and material structures. The generated structures should undergo a feedback loop through validation processes before being applied in the real world. Blue boxes indicate well-established areas of the pipeline with mature methodologies, while the red box represents critical bottlenecks.
</figcaption>
</figure>
<p>Early work in molecular and materials generation relied heavily on unconditional generation, where models produce novel structures without explicit guidance, relying solely on patterns learned from training data. For example, latent space sampling in autoencoders, where random vectors are decoded into new structures.[<span class="citation" data-cites="yoshikai2024novel">Yoshikai et al. (<a href="#ref-yoshikai2024novel" role="doc-biblioref">2024</a>)</span>] These methods excel at exploring chemical space broadly but lack fine-grained control. This limitation underscores the need for conditional generation, using explicit prompts or constraints (e.g., property targets, structural fragments), to steer <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> toward meaningful molecule or material designs. Beyond the generation step, as <a href="#fig:generation" data-reference-type="ref+Label" data-reference="fig:generation">10</a> shows, critical bottlenecks persist in synthesizability and physical consistency at the validation stage.</p>
<section id="sec:generation" class="level3" data-number="8.2.1">
<h3 data-number="8.2.1" class="anchored" data-anchor-id="sec:generation"><span class="header-section-number">8.2.1</span> Generation</h3>
<section id="prompting-2" class="level4" data-number="8.2.1.1">
<h4 data-number="8.2.1.1" class="anchored" data-anchor-id="prompting-2"><span class="header-section-number">8.2.1.1</span> Prompting</h4>
<p>While zero-shot and few-shot prompting strategies demonstrate promising flexibility for molecule generation, benchmark studies [<span class="citation" data-cites="guo2023large">T. Guo et al. (<a href="#ref-guo2023large" role="doc-biblioref">2023</a>)</span>] reveal significant limitations that restrict their practical utility. <span class="citation" data-cites="guo2023large">T. Guo et al. (<a href="#ref-guo2023large" role="doc-biblioref">2023</a>)</span> exposed fundamental gaps in <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>’ molecular design capabilities through a systematic evaluation. was reported to produce chemically valid <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> <span class="math inline">\(89\%\)</span> of the time but achieving less than <span class="math inline">\(20\%\)</span> accuracy in matching the target specifications. This result is far below specialized models like [<span class="citation" data-cites="edwards2022translation">Edwards et al. (<a href="#ref-edwards2022translation" role="doc-biblioref">2022</a>)</span>]. They conclude that this performance gap stems from <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>’ inadequate understanding of <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> syntax and structure-property relationships. Subsequent work by <span class="citation" data-cites="bhattacharya2024large">Bhattacharya et al. (<a href="#ref-bhattacharya2024large" role="doc-biblioref">2024</a>)</span> explored whether systematic prompt engineering could overcome these limitations, demonstrating that these prompts could guide to generate chemically valid molecules (<span class="math inline">\(97\%\)</span> syntactic validity) with controlled modifications, including fine-grained structural changes (median Tanimoto similarity <span class="math inline">\(0.67\)</span>–<span class="math inline">\(0.69\)</span>) and predictable electronic property shifts (0.14&nbsp;eV–0.27&nbsp;eV <span data-acronym-label="homo" data-acronym-form="singular+short">homo</span> energy changes). Hybrid approaches like extend this method with knowledge-augmented prompting, where <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> generate both molecule predictions and explanations that are used to fine-tune smaller <span data-acronym-label="lm" data-acronym-form="plural+short">lms</span>, with all resulting embeddings ultimately combined via hierarchical attention mechanisms to produce the final <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> representation[<span class="citation" data-cites="srinivas2024crossing">Srinivas and Runkana (<a href="#ref-srinivas2024crossing" role="doc-biblioref">2024a</a>)</span>]. It showed improved accuracy over pure prompting strategies but sacrificed the generalizability that makes <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> attractive, as the model requires re-training for each new molecular domain.</p>
</section>
<section id="fine-tuning" class="level4" data-number="8.2.1.2">
<h4 data-number="8.2.1.2" class="anchored" data-anchor-id="fine-tuning"><span class="header-section-number">8.2.1.2</span> Fine-Tuning</h4>
<p>To overcome the limitations of prompting, fine-tuning has been adopted in molecular and materials generation, much like its use in property prediction with <span data-acronym-label="lift" data-acronym-form="singular+short">lift</span>-based frameworks (see <a href="03-architectures.html#sec:fine-tuning" data-reference-type="ref+Label" data-reference="sec:fine-tuning">[sec:fine-tuning]</a> for a deeper explanation of <span data-acronym-label="lift" data-acronym-form="singular+short">lift</span> and <a href="#sec:prediction_FT" data-reference-type="ref+Label" data-reference="sec:prediction_FT">2.1.2</a> for a discussion of <span data-acronym-label="lift" data-acronym-form="singular+short">lift</span> applied to property prediction tasks). <span class="citation" data-cites="yu2024llasmol">B. Yu et al. (<a href="#ref-yu2024llasmol" role="doc-biblioref">2024</a>)</span> demonstrated that systematic fine-tuning in various chemical tasks including molecule generation from captions can improve performance while remaining parameter-efficient, using only <span class="math inline">\(0.58\%\)</span> of trainable parameters via <span data-acronym-label="lora" data-acronym-form="singular+short">lora</span>.</p>
<p>The molecule-caption translation task (), which involves generating textual descriptions from molecular representations and vice versa (Cap2Mol), has become a standard benchmark for evaluating <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> for molecule generation. [<span class="citation" data-cites="edwards2022translation">Edwards et al. (<a href="#ref-edwards2022translation" role="doc-biblioref">2022</a>)</span>] Under the “Mol2Cap”/“Cap2Mol” task paradigm, <span data-acronym-label="icma" data-acronym-form="singular+short">icma</span> avoids domain-specific pre-training by combining retrieval-augmented in-context learning with fine-tuning on <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> examples.[<span class="citation" data-cites="li2025large">J. Li et al. (<a href="#ref-li2025large" role="doc-biblioref">2025</a>)</span>] On the ChEBI-20[<span class="citation" data-cites="edwards2021text2mol">Edwards, Zhai, and Ji (<a href="#ref-edwards2021text2mol" role="doc-biblioref">2021</a>)</span>] and PubChem324k[<span class="citation" data-cites="liu2023molca">Zhiyuan Liu et al. (<a href="#ref-liu2023molca" role="doc-biblioref">2023</a>)</span>] datasets, <span data-acronym-label="icma" data-acronym-form="singular+short">icma</span> nearly doubles baseline performance, with <span data-acronym-label="icma" data-acronym-form="singular+short">icma</span> powered by achieving a 0.581 <span data-acronym-label="bleu" data-acronym-form="singular+short">bleu</span> score in and <span class="math inline">\(46.0\%\)</span> exact match in .[<span class="citation" data-cites="li2025large">J. Li et al. (<a href="#ref-li2025large" role="doc-biblioref">2025</a>)</span>] However, its reliance on retrieved examples raises concerns about generalization to novel scaffolds. Similarly, enhances fine-grained alignment through a teacher-student framework, where a larger <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> (e.g., ) extracts substructure-aware captions to guide a smaller model (), improving accuracy while reducing hallucinations.[<span class="citation" data-cites="li2024molreflect">J. Li et al. (<a href="#ref-li2024molreflect" role="doc-biblioref">2024</a>)</span>] Meanwhile, extends the task to property-conditioned generation, using instructions (<span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span>-text-property tuples) to optimize for captioning and prediction jointly.[<span class="citation" data-cites="lin2025property">Lin et al. (<a href="#ref-lin2025property" role="doc-biblioref">2025</a>)</span>]</p>
<p>Fine-tuned <span data-acronym-label="lm" data-acronym-form="plural+short">lms</span> have shown promise in molecule and materials generation. However, their reliance on decoding and <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span>/<span data-acronym-label="selfies" data-acronym-form="singular+short">selfies</span> representations introduces fundamental limitations: degeneracy (multiple valid <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> for the same molecule) and difficulty capturing complex structural relationships implicit in textual descriptions.</p>
</section>
<section id="diffusion-and-flow-matching" class="level4" data-number="8.2.1.3">
<h4 data-number="8.2.1.3" class="anchored" data-anchor-id="diffusion-and-flow-matching"><span class="header-section-number">8.2.1.3</span> Diffusion and Flow Matching</h4>
<p>Diffusion and flow-based models operate directly on latent representations, enabling more flexible generation of diverse and novel structures.[<span class="citation" data-cites="zhu20243m-diffusion">Zhu, Xiao, and Honavar (<a href="#ref-zhu20243m-diffusion" role="doc-biblioref">2024</a>)</span>] Moreover, emerging hybrid architectures combine the strengths of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> with diffusion and flow matching models to overcome the limitations of each paradigm individually [<span class="citation" data-cites="sriram2024flowllm">Sriram et al. (<a href="#ref-sriram2024flowllm" role="doc-biblioref">2024</a>)</span>].</p>
<p>Beyond text-based representations, introduced a multimodal <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> approach capable of text and graph generation by integrating a base <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> with graph diffusion transformers and graph neural networks for multi-conditional molecular generation and retrosynthetic planning. Specifically they used different trigger (<code>&lt;design&gt;</code> and <code>&lt;retro&gt;</code>) and query (<code>&lt;query&gt;</code>) tokens for switching between them and improved success in synthesis success rates from <span class="math inline">\(5\%\)</span> to <span class="math inline">\(35\%\)</span> . [<span class="citation" data-cites="liu2024multimodal">G. Liu et al. (<a href="#ref-liu2024multimodal" role="doc-biblioref">2024</a>)</span>]</p>
<p>A unique challenge with crystalline materials is generating a material that possesses both discrete (atom type) and continuous (atomic position and lattice geometry) variables. <span class="citation" data-cites="sriram2024flowllm">Sriram et al. (<a href="#ref-sriram2024flowllm" role="doc-biblioref">2024</a>)</span> developed to address this challenge. They recognized that the respective strengths of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, modeling discrete values and conditional prompting, and denoising models, modeling continuous values and equivariances, could be combined to create a hybrid architecture. A fine-tuned <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> is used to learn an effective base distribution of metastable crystals via text-based representations, which is then iteratively refined through <span data-acronym-label="rfm" data-acronym-form="singular+short">rfm</span> to optimize atomic coordinates and lattice parameters.[<span class="citation" data-cites="sriram2024flowllm">Sriram et al. (<a href="#ref-sriram2024flowllm" role="doc-biblioref">2024</a>)</span>]</p>
</section>
<section id="reinforcement-learning-and-preference-optimization" class="level4" data-number="8.2.1.4">
<h4 data-number="8.2.1.4" class="anchored" data-anchor-id="reinforcement-learning-and-preference-optimization"><span class="header-section-number">8.2.1.4</span> Reinforcement Learning and Preference Optimization</h4>
<p>Translating <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> generated outputs to the real world requires designing molecules and materials with specific target properties. <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> and preference optimization techniques[<span class="citation" data-cites="lee2024fine-tuning">D. Lee and Cho (<a href="#ref-lee2024fine-tuning" role="doc-biblioref">2024</a>)</span>] have emerged as powerful solutions for this challenge. For instance, <span class="citation" data-cites="jang2025can">Jang et al. (<a href="#ref-jang2025can" role="doc-biblioref">2025</a>)</span> combined <span data-acronym-label="sft" data-acronym-form="singular+short">sft</span> and <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> using <span data-acronym-label="ppo" data-acronym-form="singular+short">ppo</span> to generate diverse molecular sequences auto-regressively. This approach excels in exploring a broad chemical space, but incurs high computational costs due to its reliance on iterative, sequence-based generation. In contrast, <span class="citation" data-cites="cavanagh2024smileyllama">Cavanagh et al. (<a href="#ref-cavanagh2024smileyllama" role="doc-biblioref">2024</a>)</span> employed <span data-acronym-label="dpo" data-acronym-form="singular+short">dpo</span> with <span data-acronym-label="sft" data-acronym-form="singular+short">sft</span> to fine-tune <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> for molecular design, leveraging <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> representations to optimize drug-like properties (e.g., hydrogen bond donors/acceptors and LogP). While <span data-acronym-label="dpo" data-acronym-form="singular+short">dpo</span> reduces computational overhead in comparison to <span data-acronym-label="ppo" data-acronym-form="singular+short">ppo</span>, it trades off molecular diversity, a key strength of the work by <span class="citation" data-cites="jang2025can">Jang et al. (<a href="#ref-jang2025can" role="doc-biblioref">2025</a>)</span>, due to the inherent constraints of preference-based fine-tuning.</p>
<p>Beyond these methods, <span data-acronym-label="era" data-acronym-form="singular+short">era</span> introduces a different optimization paradigm. [<span class="citation" data-cites="chennakesavalu2025aligning">Chennakesavalu et al. (<a href="#ref-chennakesavalu2025aligning" role="doc-biblioref">2025</a>)</span>] Unlike <span data-acronym-label="ppo" data-acronym-form="singular+short">ppo</span> or <span data-acronym-label="dpo" data-acronym-form="singular+short">dpo</span>, <span data-acronym-label="era" data-acronym-form="singular+short">era</span> uses gradient-based objectives to guide word-by-word generation with explicit reward functions, converging to a physics-inspired probability distribution that allows fine control over the generation process. In single-property optimization tasks, <span data-acronym-label="era" data-acronym-form="singular+short">era</span> successfully aligned molecular transformers to generate compounds with targeted chemical properties (QED, LogP, ring count, molar refractivity) while maintaining <span class="math inline">\(59-84\%\)</span> chemical validity without regularization. For multi-objective optimization, it achieved precise control over property trade-offs using weighted energy functions.</p>
<p><span class="citation" data-cites="calanzone2025mol-moe">Calanzone, D’Oro, and Bacon (<a href="#ref-calanzone2025mol-moe" role="doc-biblioref">2025</a>)</span> also address the challenge of multi-objective molecular generation with , a <span data-acronym-label="moe" data-acronym-form="singular+short">moe</span> framework (see <a href="03-architectures.html#sec:arch-moes" data-reference-type="ref+Label" data-reference="sec:arch-moes">[sec:arch-moes]</a> to learn more about <span data-acronym-label="moe" data-acronym-form="singular+short">moe</span> architectures). dynamically combines property-specific expert models at test time using preference-guided routers toward drug-relevant molecular properties enabling flexible steering across multiple objectives without re-training. Compared to alternatives like [<span class="citation" data-cites="zhou2024one-preference-fits-all">Z. Zhou et al. (<a href="#ref-zhou2024one-preference-fits-all" role="doc-biblioref">2024</a>)</span>], <span data-acronym-label="sft" data-acronym-form="singular+short">sft</span> with rewards-in-context, and simple model merging such as Rewarded Soups[<span class="citation" data-cites="rame2023rewarded">Ramé et al. (<a href="#ref-rame2023rewarded" role="doc-biblioref">2023</a>)</span>]), achieves superior performance in both property optimization and steerability—particularly in out-of-distribution scenarios where other methods struggle.</p>
<p>uses <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> fine-tuning to optimize [<span class="citation" data-cites="cao2024space">Cao et al. (<a href="#ref-cao2024space" role="doc-biblioref">2024</a>)</span>], a transformer-based crystal generator, with rewards from discriminative models (e.g., property predictors)[<span class="citation" data-cites="cao2025crystalformer-rl">Cao and Wang (<a href="#ref-cao2025crystalformer-rl" role="doc-biblioref">2025</a>)</span>]. <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> improves stability (lower energy above convex hull) and enables property-guided generation (e.g., high dielectric constant + band gap). Here, <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> fine-tuning is shown to outperform supervised fine-tuning, enhancing both novel material discovery and retrieval of high-performing candidates from the pre-training dataset.</p>
</section>
<section id="agents-1" class="level4" data-number="8.2.1.5">
<h4 data-number="8.2.1.5" class="anchored" data-anchor-id="agents-1"><span class="header-section-number">8.2.1.5</span> Agents</h4>
<p>Agent-based frameworks leveraging <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, deeply explained in <a href="03-architectures.html#sec:agents" data-reference-type="ref+Label" data-reference="sec:agents">[sec:agents]</a>, have emerged as approaches for autonomous molecular and materials generation, demonstrating capabilities that extend beyond simple prompting or fine-tuning by incorporating iterative feedback loops, tool integration, and human-<span data-acronym-label="ai" data-acronym-form="singular+short">ai</span> collaboration. The framework implements this approach for the inverse design of materials, where agents input initial <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> strings with optimization task descriptions and generate validated candidate molecules by retrieving domain knowledge from the literature.[<span class="citation" data-cites="ansari2024dziner">Ansari et al. (<a href="#ref-ansari2024dziner" role="doc-biblioref">2024</a>)</span>] It also uses domain-expert surrogate models to evaluate the required property in the new molecule/material. These surrogate models are highly customizable to the desired property and give the user the option to train their own <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> model or using an existing <span data-acronym-label="sota" data-acronym-form="singular+short">sota</span> model. <span class="citation" data-cites="ansari2024dziner">Ansari et al. (<a href="#ref-ansari2024dziner" role="doc-biblioref">2024</a>)</span> demonstrated ’s capabilities in generating surfactants for critical micelle concentration reduction, WDR5 inhibitors, and optimizing <span data-acronym-label="mof" data-acronym-form="singular+short">mof</span> organic linkers for adsorption. The framework adopts a <span data-acronym-label="rag" data-acronym-form="singular+short">rag</span>-enhanced multi-agent approach where specialized teams including “Planning”, “Knowledge Graph”, and “Molecular Understanding” collaborate to dynamically retrieve and integrate external biochemical knowledge for drug discovery tasks without requiring domain-specific fine-tuning.[<span class="citation" data-cites="lee2025rag-enhanced">N. Lee et al. (<a href="#ref-lee2025rag-enhanced" role="doc-biblioref">2025</a>)</span>]</p>
</section>
</section>
<section id="validation" class="level3" data-number="8.2.2">
<h3 data-number="8.2.2" class="anchored" data-anchor-id="validation"><span class="header-section-number">8.2.2</span> Validation</h3>
<section id="general-validation" class="level4" data-number="8.2.2.1">
<h4 data-number="8.2.2.1" class="anchored" data-anchor-id="general-validation"><span class="header-section-number">8.2.2.1</span> General validation</h4>
<p>The most fundamental validation approaches use cheminformatics tools like to verify molecular validity. provides robust tools for validating molecules through its ability to parse and sanitize molecules from <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> strings. If a step in the <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> to structure conversion process fails, then the molecule is considered invalid. More sophisticated validation involves quantum mechanical calculations to compute molecular properties such as formation energies[<span class="citation" data-cites="kingsbury2022flexible">Kingsbury et al. (<a href="#ref-kingsbury2022flexible" role="doc-biblioref">2022</a>)</span>]. These computationally expensive operations provide deeper insights into whether generated structures are viable. Models are also evaluated for their ability to generate unique molecules by calculating the proportion of unique molecules in generated sets, often using molecular fingerprints or structural descriptors.</p>
<p>The gold standard for validation is experimental synthesis, but significant gaps exist between computational generation and laboratory realization. Preliminarily, metrics like Tanimoto similarity and Fréchet ChemNet distance [<span class="citation" data-cites="preuer2018frechet">Preuer et al. (<a href="#ref-preuer2018frechet" role="doc-biblioref">2018</a>)</span>] quantify structural resemblance, which can indicate synthetic feasibility when training data consists of known compounds. Retrosynthesis prediction algorithms attempt to bridge this gap by evaluating synthetic accessibility and proposing potential synthesis routes (see <a href="#sec:retrosynthesis" data-reference-type="ref+Label" data-reference="sec:retrosynthesis">2.3</a>). However, these methods still face limitations in accurately predicting real-world synthesizability [<span class="citation" data-cites="zunger2019beware">Zunger (<a href="#ref-zunger2019beware" role="doc-biblioref">2019</a>)</span>].</p>
</section>
<section id="conditional-generation-validation" class="level4" data-number="8.2.2.2">
<h4 data-number="8.2.2.2" class="anchored" data-anchor-id="conditional-generation-validation"><span class="header-section-number">8.2.2.2</span> Conditional Generation Validation</h4>
<p>Beyond establishing the general validity of generated molecules, evaluation methods can assess both their novelty relative to training data and their ability to meet specific design goals. For inverse design tasks, such as optimizing binding affinity or solubility, the <em>de novo</em> molecule generation benchmark GuacaMol differentiates between <em>distribution-learning</em> (e.g., generating diverse, valid molecules) and <em>goal-directed</em> optimization (e.g., rediscovering known drugs or meeting multi-objective constraints) [<span class="citation" data-cites="brown2019guacamol">Brown et al. (<a href="#ref-brown2019guacamol" role="doc-biblioref">2019</a>)</span>]. In the materials paradigm, frameworks such as evaluate analogous challenges such as stability, electronic properties, and synthesizability, but adapt metrics to periodic systems, such as energy above hull or band gap prediction accuracy[<span class="citation" data-cites="riebesell2025framework">Riebesell et al. (<a href="#ref-riebesell2025framework" role="doc-biblioref">2025</a>)</span>]. Recently, they introduced the “discovery acceleration factor”, which quantifies how effective a model is at finding stable structures relative to a random baseline.</p>
</section>
</section>
</section>
<section id="sec:retrosynthesis" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="sec:retrosynthesis"><span class="header-section-number">8.3</span> Retrosynthesis</h2>
<p>The practical utility of <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> for generating molecules and materials remains limited by a persistent gap in their synthetic feasibility. Early work by <span class="citation" data-cites="schwaller2021mapping">Schwaller et al. (<a href="#ref-schwaller2021mapping" role="doc-biblioref">2021</a>)</span> laid important groundwork by demonstrating how attention-based neural networks can learn meaningful representations of chemical reactions, enabling accurate classification and prediction of reaction outcomes. Their model, trained on millions of reactions from patent and literature data, showed that learned reaction embeddings were capable of capturing nuanced chemical relationships.</p>
<p>Recent efforts have built on this foundation by integrating synthesizability directly into molecular and materials generation pipelines that leverage both domain-specific tools and <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>. For example, <span class="citation" data-cites="sun2025synllama">Sun et al. (<a href="#ref-sun2025synllama" role="doc-biblioref">2025</a>)</span> adapted and to predict retrosynthetic pathways and identify commercially available building blocks for experimentally validated SARS-CoV-2 Mpro inhibitors. Similarly, <span class="citation" data-cites="liu2024multimodal">G. Liu et al. (<a href="#ref-liu2024multimodal" role="doc-biblioref">2024</a>)</span> introduced a multimodal framework that combines reaction databases with chemical intuition encoded in <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>, improving the prioritization of high-yield, low-cost synthetic routes.</p>
<p>More recent work has explored how fully fine-tuned <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can serve as comprehensive chemistry assistants for experimental guidance. <span class="citation" data-cites="zhang2025large">Y. Zhang et al. (<a href="#ref-zhang2025large" role="doc-biblioref">2025</a>)</span> developed , a fine-tuned model trained on 1.28 million chemical reaction question-answer pairs. Through an active learning framework that incorporates experimental feedback (see <a href="03-architectures.html#sec:rl" data-reference-type="ref+Label" data-reference="sec:rl">[sec:rl]</a> to learn more about <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span>), human- collaboration successfully optimized an unreported Suzuki-Miyaura cross-coupling reaction within only 15 experimental runs.</p>
<p>Predictive retrosynthesis has also extended to the inorganic domain. <span class="citation" data-cites="kim2024large">Kim, Jung, and Schrier (<a href="#ref-kim2024large" role="doc-biblioref">2024</a>)</span> demonstrated that fine-tuned and can predict both the synthesizability of inorganic compounds from their chemical formulas and select appropriate precursors for synthesis, achieving performance comparable to specialized <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> models with minimal development time and cost. In a follow-up work, they extended this approach to structure-based predictions of inorganic crystal polymorphs, where <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> provided human-readable explanations for their synthesizability assessments[<span class="citation" data-cites="kim2025explainable">Kim, Schrier, and Jung (<a href="#ref-kim2025explainable" role="doc-biblioref">2025</a>)</span>]. Notably, their structure-aware models correctly identified twelve hypothetical compounds as non-synthesizable despite their thermodynamic stability, perfectly matching experimental outcomes where synthesis attempts failed.</p>
<p>Beyond retrosynthetic prediction, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> have also been deployed as reasoning engines for autonomous design. <span class="citation" data-cites="bran2024augmenting">Bran et al. (<a href="#ref-bran2024augmenting" role="doc-biblioref">2024</a>)</span> developed , an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-based system that autonomously plans and executes the synthesis of novel compounds by integrating specialized tools like a retrosynthesis planner (see <a href="#sec:planning" data-reference-type="ref+Label" data-reference="sec:planning">1.5</a> to read more about this capability of and its limitations) and reaction predictors. This approach mirrors the iterative experimental design cycle employed by human chemists, but is equipped with the scalability of automation. Notably, systems like rely on high-quality reaction data to ground their reasoning in empirically viable chemistry, which, depending on the design space, could be a limitation.</p>
</section>
<section id="sec:llm-optimizers" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="sec:llm-optimizers"><span class="header-section-number">8.4</span> LLMs as Optimizers</h2>
<figure id="fig:optimization" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure22.png" width="100%" class="figure-img">
<figcaption>
<strong>Overview of the iterative optimization loop that mirrors the structure of the optimization section</strong>. The blue boxes contain the different roles that the <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> play in the loop, and which are described in the main text. References in which the use of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> for that step are detailed inside the small boxes inside each of the components of the loop. The example shown is about obtaining molecules with high <code>logP</code>.
</figcaption>
</figure>
<p>Discovering novel compounds and reactions in chemistry and materials science has long relied on iterative trial-and-error processes rooted in existing domain knowledge [<span class="citation" data-cites="Taylor2023brief">Taylor et al. (<a href="#ref-Taylor2023brief" role="doc-biblioref">2023</a>)</span>]. While, as explained in <a href="#sec:retrosynthesis" data-reference-type="ref+Label" data-reference="sec:retrosynthesis">2.3</a>, those methods are used to accelerate this process, optimization methods help improve conditions, binding affinity, etc. These approaches are slow and labor-intensive. Traditional data-driven methods aimed to address these limitations by combining predictive <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> models with optimization frameworks such as <span data-acronym-label="bo" data-acronym-form="singular+short">bo</span> or <span data-acronym-label="ea" data-acronym-form="plural+short">eas</span>. These frameworks balance exploration of uncharted regions in chemical space with exploitation of known high-performing regions [<span class="citation" data-cites="Li2024sequential">X. Li et al. (<a href="#ref-Li2024sequential" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="Hse2021gryffin">Häse et al. (<a href="#ref-Hse2021gryffin" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="Shields2021bayesian">Shields et al. (<a href="#ref-Shields2021bayesian" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="Griffiths2020constrained">Griffiths and Hernández-Lobato (<a href="#ref-Griffiths2020constrained" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="RajabiKochi2025adaptive">Rajabi-Kochi et al. (<a href="#ref-RajabiKochi2025adaptive" role="doc-biblioref">2025</a>)</span>].</p>
<p>Recent advances in <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> have unlocked potential for addressing optimization challenges in chemistry and related domains [<span class="citation" data-cites="fernando2023promptbreeder0">Fernando et al. (<a href="#ref-fernando2023promptbreeder0" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="yang2023large">C. Yang et al. (<a href="#ref-yang2023large" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="chen2024instruct">L. Chen et al. (<a href="#ref-chen2024instruct" role="doc-biblioref">2024</a>)</span>]. A key strength of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> lies in their capacity to frame optimization tasks through natural language, which enhances knowledge incorporation, improves candidate comparisons, and increases interpretability. This aligns well with chemical problem-solving, where complex phenomena, such as reaction pathways or material behaviors, are often poorly captured by standard nomenclature; however, they can still be intuitively explained through natural language. Moreover, <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span>’ general capabilities provide flexibility beyond classical methods, which have to be trained from scratch if the optimization problem or any of its variables changes. By encoding domain-specific knowledge—including reaction rules, thermodynamic principles, and structure-property relationships—into structured prompts, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> can synergize expertise with their ability to navigate complex chemical optimization problems.</p>
<p>Current <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> applications in chemistry optimization vary in scope and methodology. Many studies integrate <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> into <span data-acronym-label="bo" data-acronym-form="singular+short">bo</span> frameworks, where models guide experimental design by predicting promising candidates [<span class="citation" data-cites="rankovic2023bochemian">Ranković and Schwaller (<a href="#ref-rankovic2023bochemian" role="doc-biblioref">2023</a>)</span>]. Others employ <span data-acronym-label="ga" data-acronym-form="plural+short">gas</span> or hybrid strategies that combine <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-generated hypotheses with computational screening [<span class="citation" data-cites="cisse2025language0based">Cissé et al. (<a href="#ref-cisse2025language0based" role="doc-biblioref">2025</a>)</span>].</p>
<section id="llms-as-surrogate-models" class="level3" data-number="8.4.1">
<h3 data-number="8.4.1" class="anchored" data-anchor-id="llms-as-surrogate-models"><span class="header-section-number">8.4.1</span> LLMs as Surrogate Models</h3>
<p>A prominent <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-driven strategy positions these models as surrogate models within optimization loops. Typically implemented as <span data-acronym-label="gpr" data-acronym-form="singular+short">gpr</span>, surrogate models learn from prior data to approximate costly feature-outcome landscapes, which are often computationally and time-consuming to evaluate, thereby guiding the acquisition. <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> offer major advantages in this role primarily through strong low-data performance. Their <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> capability enables task demonstration with minimal prompt examples while leveraging chemical knowledge from pre-training to generate accurate predictions. This allows <span data-acronym-label="gpm" data-acronym-form="plural+short">gpms</span> to compensate for sparse experimental data effectively.</p>
<p><span class="citation" data-cites="ramos2023bayesian">Ramos et al. (<a href="#ref-ramos2023bayesian" role="doc-biblioref">2023</a>)</span> demonstrated the viability of this paradigm through a simple yet effective framework that combines <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> using only one example in the prompt with a <span data-acronym-label="bo" data-acronym-form="singular+short">bo</span> workflow. Their <span data-acronym-label="bo" data-acronym-form="singular+short">bo</span>-<span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> approach uses few-shot examples formatted as question-answer pairs, where the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> generates candidate solutions conditioned on prior successful iterations. These candidates are ranked using an acquisition function, with top-<span class="math inline">\(k\)</span> selections integrated into subsequent prompts to refine predictions iteratively. Remarkably, this method achieved high performance in optimizing catalytic reaction conditions, even matching the top-1 accuracies observed in experimental benchmarks. This emphasizes the potential of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> as accessible, <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> optimizers when coupled with well-designed prompts.</p>
<p>To address limitations in base <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span>’ inherent chemical knowledge—particularly their grasp of specialized representations like <span data-acronym-label="smiles" data-acronym-form="singular+short">smiles</span> or structure-property mappings—<span class="citation" data-cites="yu2025collaborative">J. Yu et al. (<a href="#ref-yu2025collaborative" role="doc-biblioref">2025</a>)</span> introduced a hybrid architecture augmenting pre-trained <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> with task-specific embedding and prediction layers. These layers, fine-tuned on domain data, align latent representations of input-output pairs (denoted as <code>&lt;x&gt;</code> and <code>&lt;y&gt;</code> in prompts), enabling the model to map chemical structures and properties into a unified, interpretable space. Crucially, the added layers enhance chemical reasoning without sacrificing the flexibility of <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span>, allowing the system to adapt to trends across iterations, similarly to what was done by <span class="citation" data-cites="ramos2023bayesian">Ramos et al. (<a href="#ref-ramos2023bayesian" role="doc-biblioref">2023</a>)</span>. In their evaluations of molecular optimization benchmarks, such as the <span data-acronym-label="pmo" data-acronym-form="singular+short">pmo</span> [<span class="citation" data-cites="gao2022sample">W. Gao et al. (<a href="#ref-gao2022sample" role="doc-biblioref">2022</a>)</span>], they revealed improvements over conventional methods, including <span data-acronym-label="bo" data-acronym-form="singular+short">bo</span>-<span data-acronym-label="gp" data-acronym-form="singular+short">gp</span>, <span data-acronym-label="rl" data-acronym-form="singular+short">rl</span> methods, and <span data-acronym-label="ga" data-acronym-form="singular+short">ga</span>.</p>
<p><span class="citation" data-cites="yu2025collaborative">J. Yu et al. (<a href="#ref-yu2025collaborative" role="doc-biblioref">2025</a>)</span> further highlighted the framework’s extensibility to diverse black-box optimization challenges beyond chemistry. This represents one of the most important advantages of using <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> as orchestrators of the optimization process. The flexibility of natural language in this process enables the procedure to be applied to any optimization process. In contrast, classical methods are constrained to the specific task for which they are designed due to the need to train the surrogate model.</p>
</section>
<section id="llms-as-next-candidate-generators" class="level3" data-number="8.4.2">
<h3 data-number="8.4.2" class="anchored" data-anchor-id="llms-as-next-candidate-generators"><span class="header-section-number">8.4.2</span> LLMs as Next Candidate Generators</h3>
<p>Recent studies demonstrate the potential of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> to enhance <span data-acronym-label="ea" data-acronym-form="plural+short">eas</span> [<span class="citation" data-cites="lu2024generative">Lu et al. (<a href="#ref-lu2024generative" role="doc-biblioref">2025</a>)</span>] and <span data-acronym-label="bo" data-acronym-form="singular+short">bo</span> [<span class="citation" data-cites="amin2025towards">Amin, Raja, and Krishnapriyan (<a href="#ref-amin2025towards" role="doc-biblioref">2025</a>)</span>] frameworks by leveraging their embedded chemical knowledge and ability to integrate prior information, thereby reducing computational effort while improving output quality. Within <span data-acronym-label="ea" data-acronym-form="plural+short">eas</span>, <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> refine molecular candidates through mutations (modifying molecular substructures) or crossovers (combining parent molecules). In <span data-acronym-label="bo" data-acronym-form="singular+short">bo</span> frameworks, they serve as acquisition functions, utilizing surrogate model predictions—both mean and uncertainty—to select optimal molecules or reaction conditions for evaluation.</p>
<p>For molecule optimization, <span class="citation" data-cites="yu2025collaborative">J. Yu et al. (<a href="#ref-yu2025collaborative" role="doc-biblioref">2025</a>)</span> introduced , a dual-<span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> system where one model proposes candidates and the other supplies domain knowledge (see <a href="#sec:opt-llm-know-source" data-reference-type="ref+Label" data-reference="sec:opt-llm-know-source">2.4.3</a>). By fine-tuning the “worker” <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> to recognize molecular scaffolds and target properties, and expanding the training pool to include a million-size pre-training dataset, they achieved hit rates exceeding <span class="math inline">\(90\%\)</span>. Similarly, <span class="citation" data-cites="wang2024efficient">H. Wang, Skreta, et al. (<a href="#ref-wang2024efficient" role="doc-biblioref">2025</a>)</span> developed , integrating an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> into an <span data-acronym-label="ea" data-acronym-form="singular+short">ea</span> to replace random mutations with <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-guided modifications. Here, generated optimized offspring from parent molecules, significantly accelerating convergence to high fitness scores. Notably, while domain-specialized models (, ) underperformed, the general-purpose excelled—a finding that underscores the context-dependent utility of <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span></p>
<p>In a related approach, <span class="citation" data-cites="lu2024generative">Lu et al. (<a href="#ref-lu2024generative" role="doc-biblioref">2025</a>)</span> showed that well-designed prompts—incorporating task-specific constraints, objectives, and few-shot examples—enable general <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> (, ) to generate high-quality candidates without fine-tuning, outperforming both random selection and vanilla <span data-acronym-label="ga" data-acronym-form="plural+short">gas</span> in functional <span data-acronym-label="tmc" data-acronym-form="singular+short">tmc</span> design.</p>
</section>
<section id="sec:opt-llm-know-source" class="level3" data-number="8.4.3">
<h3 data-number="8.4.3" class="anchored" data-anchor-id="sec:opt-llm-know-source"><span class="header-section-number">8.4.3</span> LLMs as Prior Knowledge Sources</h3>
<p>A key advantage of integrating <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> into optimization frameworks is their ability to encode and deploy prior knowledge within the optimization loop. As illustrated in <a href="#fig:optimization" data-reference-type="ref+Label" data-reference="fig:optimization">11</a>, this knowledge can be directed into either the surrogate model or candidate generation module, significantly reducing the number of optimization steps required through high-quality guidance.</p>
<p>For example, <span class="citation" data-cites="yu2025collaborative">J. Yu et al. (<a href="#ref-yu2025collaborative" role="doc-biblioref">2025</a>)</span> deployed a “research” agent that leverages search and to verify and rank molecules generated by “worker” agents against target features and properties. Their results demonstrate substantial improvements when this filtering mechanism is applied.</p>
<p>Similarly, <span class="citation" data-cites="cisse2025language0based">Cissé et al. (<a href="#ref-cisse2025language0based" role="doc-biblioref">2025</a>)</span> introduced , which contextualizes conventional black-box <span data-acronym-label="bo" data-acronym-form="singular+short">bo</span> using an <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>. maintains standard <span data-acronym-label="bo" data-acronym-form="singular+short">bo</span> as the core driver but strategically activates the <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span> when progress stalls. This leverages the model’s <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> capabilities to hypothesize promising search regions and propose new samples, regulated by a lightweight heuristic policy that manages costs and incorporates domain knowledge (or user input). Evaluations on synthetic benchmarks such as the catalyst optimization task for hydrogen generation show that accelerates exploration, improves convergence, and outperforms existing <span data-acronym-label="llm" data-acronym-form="singular+short">llm</span>-<span data-acronym-label="bo" data-acronym-form="singular+short">bo</span> hybrids.</p>
</section>
<section id="how-to-face-optimization-problems" class="level3" data-number="8.4.4">
<h3 data-number="8.4.4" class="anchored" data-anchor-id="how-to-face-optimization-problems"><span class="header-section-number">8.4.4</span> How to Face Optimization Problems?</h3>
<p>Published works explore different ways of using <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> for optimization problems in chemistry, from simple approaches, such as just prompting the model with some initial random set of experimental candidates and iterating [<span class="citation" data-cites="ramos2023bayesian">Ramos et al. (<a href="#ref-ramos2023bayesian" role="doc-biblioref">2023</a>)</span>], to fine-tuning models in <span data-acronym-label="bo" data-acronym-form="singular+short">bo</span> fashion [<span class="citation" data-cites="rankovic2025gollum0">Ranković and Schwaller (<a href="#ref-rankovic2025gollum0" role="doc-biblioref">2025</a>)</span>]. The most efficient initial point is by relying entirely on a <span data-acronym-label="icl" data-acronym-form="singular+short">icl</span> approach, which allows one to obtain a first signal rapidly. Such initial results will enable to determine whether a more complex, computationally intensive approach is necessary or whether prompt engineering is reliable enough for the application. Fine-tuning can be used as a way to enhance the chemical knowledge of the <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> and can lead to improvements in optimization tasks where the model requires such knowledge to choose or generate better candidates. Fine-tuning might not be a game-changer for other approaches that rely more on sampling methods [<span class="citation" data-cites="wang2025llm0augmented">H. Wang, Guo, et al. (<a href="#ref-wang2025llm0augmented" role="doc-biblioref">2025</a>)</span>].</p>
<p>While some initial works showed that <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> trained specifically on chemistry perform better for optimization tasks [<span class="citation" data-cites="kristiadi2024sober">Kristiadi et al. (<a href="#ref-kristiadi2024sober" role="doc-biblioref">2024</a>)</span>], other works showed that a <span data-acronym-label="gpm" data-acronym-form="singular+short">gpm</span> such as combined with an <span data-acronym-label="ea" data-acronym-form="singular+short">ea</span> outperformed all other models [<span class="citation" data-cites="wang2024efficient">H. Wang, Skreta, et al. (<a href="#ref-wang2024efficient" role="doc-biblioref">2025</a>)</span>]. Is it better to incorporate a general model or a chemistry <span data-acronym-label="lm" data-acronym-form="singular+short">lm</span> into the optimization frameworks? We hypothesize that for models of the same size (in number of parameters) and similar training size—attending to <span data-acronym-label="pflop" data-acronym-form="plural+short">pflops</span>—a chemical <span data-acronym-label="lm" data-acronym-form="singular+short">lm</span> (a specialized model) will consistently outperform general models. If the models differ significantly in size, the larger model will typically perform better.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ahmad2022chemberta" class="csl-entry" role="listitem">
Ahmad, Walid, Elana Simon, Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. 2022. <span>“<span class="nocase">Chemberta-2: Towards chemical foundation models</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2209.01712">https://doi.org/10.48550/arXiv.2209.01712</a>.
</div>
<div id="ref-ahn2022can" class="csl-entry" role="listitem">
Ahn, Michael, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, et al. 2022. <span>“Do as i Can, Not as i Say: Grounding Language in Robotic Affordances.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2204.01691">https://doi.org/10.48550/arXiv.2204.01691</a>.
</div>
<div id="ref-ai2024extracting" class="csl-entry" role="listitem">
Ai, Qianxiang, Fanwang Meng, Jiale Shi, Brenden Pelkie, and Connor W Coley. 2024. <span>“Extracting Structured Data from Organic Synthesis Procedures Using a Fine-Tuned Large Language Model.”</span> <em>Digital Discovery</em> 3 (9): 1822–31. <a href="https://doi.org/10.1039/d4dd00091a">https://doi.org/10.1039/d4dd00091a</a>.
</div>
<div id="ref-alampara2024mattext" class="csl-entry" role="listitem">
Alampara, Nawaf, Santiago Miret, and Kevin Maik Jablonka. 2024. <span>“<span class="nocase">MatText: Do language models need more than text &amp; scale for materials modeling?</span>”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2406.17295">https://doi.org/10.48550/arXiv.2406.17295</a>.
</div>
<div id="ref-alampara2024probing" class="csl-entry" role="listitem">
Alampara, Nawaf, Mara Schilling-Wilhelmi, Martiño Rı́os-Garcı́a, Indrajeet Mandal, Pranav Khetarpal, Hargun Singh Grover, NM Krishnan, and Kevin Maik Jablonka. 2024. <span>“<span class="nocase">Probing the limitations of multimodal language models for chemistry and materials research</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2411.16955">https://doi.org/10.48550/arXiv.2411.16955</a>.
</div>
<div id="ref-amin2025towards" class="csl-entry" role="listitem">
Amin, Ishan, Sanjeev Raja, and Aditi Krishnapriyan. 2025. <span>“<span class="nocase">Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2501.09009">https://doi.org/10.48550/arXiv.2501.09009</a>.
</div>
<div id="ref-ananthanarayanan2010biocoder" class="csl-entry" role="listitem">
Ananthanarayanan, Vaishnav, and William Thies. 2010. <span>“BioCoder: A Programming Language for Standardizing and Automating Biology Protocols.”</span> <em>Journal of Biological Engineering</em> 4: 13. <a href="https://doi.org/10.1186/1754-1611-4-13">https://doi.org/10.1186/1754-1611-4-13</a>.
</div>
<div id="ref-aneesh2025semantic" class="csl-entry" role="listitem">
Aneesh, Anagha, Nawaf Alampara, José A. Márquez, and Kevin Maik Jablonka. 2025. <span>“Semantic Device Graphs for Perovskite Solar Cell Design.”</span> <em>The Thirsteenth International Conference on Learning Representations Workshop on AI for Materials Science, <span>ICLR-AI4MAT</span></em>. <a href="https://openreview.net/forum?id=AGCClISEXL&amp;referrer=%5Bthe%20profile%20of%20Anagha%20Aneesh%5D(%2Fprofile%3Fid%3D~Anagha_Aneesh1)">https://openreview.net/forum?id=AGCClISEXL&amp;referrer=%5Bthe%20profile%20of%20Anagha%20Aneesh%5D(%2Fprofile%3Fid%3D~Anagha_Aneesh1)</a>.
</div>
<div id="ref-ansari2024agent" class="csl-entry" role="listitem">
Ansari, Mehrad, and Seyed Mohamad Moosavi. 2024. <span>“Agent-Based Learning of Materials Datasets from the Scientific Literature.”</span> <em>Digital Discovery</em> 3 (12): 2607–17. <a href="https://doi.org/10.1039/D4DD00252K">https://doi.org/10.1039/D4DD00252K</a>.
</div>
<div id="ref-ansari2024dziner" class="csl-entry" role="listitem">
Ansari, Mehrad, Jeffrey Watchorn, Carla E. Brown, and Joseph S. Brown. 2024. <span>“<span class="nocase">dZiner</span>: <span>Rational</span> <span>Inverse</span> <span>Design</span> of <span>Materials</span> with <span>AI</span> <span>Agents</span>.”</span> <em>Arxiv Preprint</em>, October. <a href="https://doi.org/10.48550/arXiv.2410.03963">https://doi.org/10.48550/arXiv.2410.03963</a>.
</div>
<div id="ref-arlt2024meta0designing" class="csl-entry" role="listitem">
Arlt, Sören, Haonan Duan, Felix Li, Sang Michael Xie, Yuhuai Wu, and Mario Krenn. 2024. <span>“Meta-Designing Quantum Experiments with Language Models.”</span> <em>arXiv Preprint arXiv: 2406.02470</em>. <a href="https://doi.org/10.48550/arXiv.2406.02470">https://doi.org/10.48550/arXiv.2406.02470</a>.
</div>
<div id="ref-balaji2023gptmolberta" class="csl-entry" role="listitem">
Balaji, Suryanarayanan, Rishikesh Magar, Yayati Jadhav, and Amir Barati Farimani. 2023. <span>“<span>GPT</span>-<span>MolBERTa</span>: <span>GPT</span> <span>Molecular</span> <span>Features</span> <span>Language</span> <span>Model</span> for Molecular Property Prediction.”</span> <em>Arxiv Preprint arXiv:2310.03030</em>, October. <a href="https://doi.org/10.48550/arXiv.2310.03030">https://doi.org/10.48550/arXiv.2310.03030</a>.
</div>
<div id="ref-batatia2022mace" class="csl-entry" role="listitem">
Batatia, Ilyes, D. Kov’acs, G. Simm, C. Ortner, and Gábor Csányi. 2022. <span>“MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields.”</span> <em>Neural Information Processing Systems</em>. <a href="https://doi.org/10.48550/arXiv.2206.07697">https://doi.org/10.48550/arXiv.2206.07697</a>.
</div>
<div id="ref-beltagy2019scibert0" class="csl-entry" role="listitem">
Beltagy, Iz, Kyle Lo, and Arman Cohan. 2019. <span>“SciBERT: A Pretrained Language Model for Scientific Text.”</span> <em>Conference on Empirical Methods in Natural Language Processing</em>. <a href="https://doi.org/10.18653/v1/D19-1371">https://doi.org/10.18653/v1/D19-1371</a>.
</div>
<div id="ref-bhattacharya2024large" class="csl-entry" role="listitem">
Bhattacharya, Debjyoti, Harrison J. Cassady, Michael A. Hickner, and Wesley F. Reinhart. 2024. <span>“Large <span>Language</span> <span>Models</span> as <span>Molecular</span> <span>Design</span> <span>Engines</span>.”</span> <em>Journal of Chemical Information and Modeling</em> 64 (18): 7086–96. <a href="https://doi.org/10.1021/acs.jcim.4c01396">https://doi.org/10.1021/acs.jcim.4c01396</a>.
</div>
<div id="ref-boiko2023autonomous" class="csl-entry" role="listitem">
Boiko, Daniil A, Robert MacKnight, Ben Kline, and Gabe Gomes. 2023. <span>“<span class="nocase">Autonomous chemical research with large language models</span>.”</span> <em>Nature</em> 624 (7992): 570–78. <a href="https://doi.org/10.1038/s41586-023-06792-0">https://doi.org/10.1038/s41586-023-06792-0</a>.
</div>
<div id="ref-bojanowski2017enriching" class="csl-entry" role="listitem">
Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. <span>“Enriching Word Vectors with Subword Information.”</span> <em>Transactions of the Association for Computational Linguistics</em> 5: 135–46. <a href="https://doi.org/10.1162/tacl_a_00051">https://doi.org/10.1162/tacl_a_00051</a>.
</div>
<div id="ref-bonet2012action" class="csl-entry" role="listitem">
Bonet, Blai, and Hector Geffner. 2012. <span>“Action Selection for MDPs: Anytime AOversus UCT.”</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 26 (1): 1749–55. <a href="https://doi.org/10.1609/aaai.v26i1.8369">https://doi.org/10.1609/aaai.v26i1.8369</a>.
</div>
<div id="ref-bran2024augmenting" class="csl-entry" role="listitem">
Bran, Andres M., Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and Philippe Schwaller. 2024. <span>“Augmenting Large Language Models with Chemistry Tools.”</span> <em>Nature Machine Intelligence</em> 6 (5). <a href="https://doi.org/10.1038/s42256-024-00832-8">https://doi.org/10.1038/s42256-024-00832-8</a>.
</div>
<div id="ref-brown2019guacamol" class="csl-entry" role="listitem">
Brown, Nathan, Marco Fiscato, Marwin H. S. Segler, and Alain C. Vaucher. 2019. <span>“GuacaMol: Benchmarking Models for de Novo Molecular Design.”</span> <em>Journal of Chemical Information and Modeling</em> 59 (3): 1096–1108. <a href="https://doi.org/10.1021/acs.jcim.8b00839">https://doi.org/10.1021/acs.jcim.8b00839</a>.
</div>
<div id="ref-calanzone2025mol-moe" class="csl-entry" role="listitem">
Calanzone, Diego, Pierluca D’Oro, and Pierre-Luc Bacon. 2025. <span>“Mol-<span>MoE</span>: <span>Training</span> <span>Preference</span>-<span>Guided</span> <span>Routers</span> for <span>Molecule</span> <span>Generation</span>.”</span> <em>Arxiv Preprint arXiv:2502.05633</em>, February. <a href="https://doi.org/10.48550/arXiv.2502.05633">https://doi.org/10.48550/arXiv.2502.05633</a>.
</div>
<div id="ref-Campbell2025MDCrow" class="csl-entry" role="listitem">
Campbell, Quintina, Sam Cox, Jorge Medina, Brittany Watterson, and Andrew D. White. 2025. <span>“MDCrow: Automating Molecular Dynamics Workflows with Large Language Models.”</span> <em>arXiv Preprint arXiv:2502.09565</em>. <a href="https://doi.org/10.48550/arXiv.2502.09565">https://doi.org/10.48550/arXiv.2502.09565</a>.
</div>
<div id="ref-cao2024space" class="csl-entry" role="listitem">
Cao, Zhendong, Xiaoshan Luo, Jian Lv, and Lei Wang. 2024. <span>“Space Group Informed Transformer for Crystalline Materials Generation.”</span> <em>arXiv Preprint arXiv: 2403.15734</em>. <a href="https://doi.org/10.48550/arXiv.2403.15734">https://doi.org/10.48550/arXiv.2403.15734</a>.
</div>
<div id="ref-cao2025crystalformer-rl" class="csl-entry" role="listitem">
Cao, Zhendong, and Lei Wang. 2025. <span>“<span>CrystalFormer</span>-<span>RL</span>: <span>Reinforcement</span> <span>Fine</span>-<span>Tuning</span> for <span>Materials</span> <span>Design</span>.”</span> <em>Arxiv Preprint arXiv:2504.02367</em>, April. <a href="https://doi.org/10.48550/arXiv.2504.02367">https://doi.org/10.48550/arXiv.2504.02367</a>.
</div>
<div id="ref-Carlson2006millennium" class="csl-entry" role="listitem">
Carlson, James, Arthur Jaffe, and Andrew Wiles, eds. 2006. <em>The Millennium Prize Problems</em>. Providence, RI: American Mathematical Society &amp; Clay Mathematics Institute.
</div>
<div id="ref-cavanagh2024smileyllama" class="csl-entry" role="listitem">
Cavanagh, Joseph M., Kunyang Sun, Andrew Gritsevskiy, Dorian Bagni, Thomas D. Bannister, and Teresa Head-Gordon. 2024. <span>“SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration.”</span> <em>arXiv Preprint arXiv: 2409.02231</em>. <a href="https://doi.org/10.48550/arXiv.2409.02231">https://doi.org/10.48550/arXiv.2409.02231</a>.
</div>
<div id="ref-chan2024mle" class="csl-entry" role="listitem">
Chan, Jun Shern, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, et al. 2024. <span>“Mle-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering.”</span> <em>arXiv Preprint arXiv:2410.07095</em>. <a href="https://doi.org/10.48550/arXiv.2410.07095">https://doi.org/10.48550/arXiv.2410.07095</a>.
</div>
<div id="ref-chen2024autonomous" class="csl-entry" role="listitem">
Chen, Kexin, Hanqun Cao, Junyou Li, Yuyang Du, Menghao Guo, Xin Zeng, Lanqing Li, Jiezhong Qiu, Pheng Ann Heng, and Guangyong Chen. 2024. <span>“An Autonomous Large Language Model Agent for Chemical Literature Data Mining.”</span> <em>arXiv Preprint arXiv: 2402.12993</em>. <a href="https://doi.org/10.48550/arXiv.2402.12993">https://doi.org/10.48550/arXiv.2402.12993</a>.
</div>
<div id="ref-chen2024instruct" class="csl-entry" role="listitem">
Chen, Lichang, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. 2024. <span>“InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models.”</span> <em>Forty-First International Conference on Machine Learning, <span>ICML</span> 2024</em>. <a href="https://openreview.net/forum?id=rADFNrIss3">https://openreview.net/forum?id=rADFNrIss3</a>.
</div>
<div id="ref-chen2022deep" class="csl-entry" role="listitem">
Chen, Pengzhan, Jiean Pei, Weiqing Lu, and Mingzhen Li. 2022. <span>“<span class="nocase">A deep reinforcement learning based method for real-time path planning and dynamic obstacle avoidance</span>.”</span> <em>Neurocomputing</em> 497: 64–75. <a href="https://doi.org/10.1016/j.neucom.2022.05.006">https://doi.org/10.1016/j.neucom.2022.05.006</a>.
</div>
<div id="ref-cheng2023group" class="csl-entry" role="listitem">
Cheng, Austin H, Andy Cai, Santiago Miret, Gustavo Malkomes, Mariano Phielipp, and Alán Aspuru-Guzik. 2023. <span>“Group SELFIES: A Robust Fragment-Based Molecular String Representation.”</span> <em>Digital Discovery</em> 2 (3): 748–58. <a href="https://doi.org/10.1039/D3DD00012E">https://doi.org/10.1039/D3DD00012E</a>.
</div>
<div id="ref-chennakesavalu2025aligning" class="csl-entry" role="listitem">
Chennakesavalu, Shriram, Frank Hu, Sebastian Ibarraran, and Grant M. Rotskoff. 2025. <span>“Aligning <span>Transformers</span> with <span>Continuous</span> <span>Feedback</span> via <span>Energy</span> <span>Rank</span> <span>Alignment</span>.”</span> <em>Arxiv Preprint arXiv:2405.12961</em>, May. <a href="https://doi.org/10.48550/arXiv.2405.12961">https://doi.org/10.48550/arXiv.2405.12961</a>.
</div>
<div id="ref-chiang2024llamp" class="csl-entry" role="listitem">
Chiang, Yuan, Elvis Hsieh, Chia-Hong Chou, and Janosh Riebesell. 2024. <span>“<span class="nocase"><span>LLaMP</span>: <span>Large</span> <span>Language</span> <span>Model</span> <span>Made</span> <span>Powerful</span> for <span>High</span>-fidelity <span>Materials</span> <span>Knowledge</span> <span>Retrieval</span> and <span>Distillation</span></span>.”</span> <em>Arxiv</em>, October. <a href="https://doi.org/10.48550/arXiv.2401.17244">https://doi.org/10.48550/arXiv.2401.17244</a>.
</div>
<div id="ref-chithrananda2020chemberta" class="csl-entry" role="listitem">
Chithrananda, Seyone, Gabriel Grand, and Bharath Ramsundar. 2020. <span>“<span class="nocase"><span>ChemBERTa</span>: <span>Large</span>-<span>Scale</span> <span>Self</span>-<span>Supervised</span> <span>Pretraining</span> for <span>Molecular</span> <span>Property</span> <span>Prediction</span></span>.”</span> <em>Arxiv</em>, October. <a href="https://doi.org/10.48550/arXiv.2010.09885">https://doi.org/10.48550/arXiv.2010.09885</a>.
</div>
<div id="ref-choi2024lota" class="csl-entry" role="listitem">
Choi, Jae-Woo, Youngwoo Yoon, Hyobin Ong, Jaehong Kim, and Minsu Jang. 2024. <span>“Lota-Bench: Benchmarking Language-Oriented Task Planners for Embodied Agents.”</span> <em>arXiv Preprint arXiv:2402.08178</em>. <a href="https://doi.org/10.48550/arXiv.2402.08178">https://doi.org/10.48550/arXiv.2402.08178</a>.
</div>
<div id="ref-dimitrios2023unifying" class="csl-entry" role="listitem">
Christofidellis, Dimitrios, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica. 2023. <span>“Unifying Molecular and Textual Representations via Multi-Task Language Modelling.”</span> <em>International Conference on Machine Learning, <span>ICML</span> 2023</em>, Proceedings of machine learning research, 202: 6140–57. <a href="https://doi.org/10.48550/arXiv.2301.12586">https://doi.org/10.48550/arXiv.2301.12586</a>.
</div>
<div id="ref-Chu_2021" class="csl-entry" role="listitem">
Chu, Johan S. G., and James A. Evans. 2021. <span>“Slowed Canonical Progress in Large Fields of Science.”</span> <em>Proceedings of the National Academy of Sciences</em> 118 (41). <a href="https://doi.org/10.1073/pnas.2021636118">https://doi.org/10.1073/pnas.2021636118</a>.
</div>
<div id="ref-cisse2025language0based" class="csl-entry" role="listitem">
Cissé, Abdoulatif, Xenophon Evangelopoulos, Vladimir V. Gusev, and Andrew I. Cooper. 2025. <span>“Language-Based Bayesian Optimization Research Assistant (BORA).”</span> <em>arXiv Preprint arXiv: 2501.16224</em>. <a href="https://doi.org/10.48550/arXiv.2501.16224">https://doi.org/10.48550/arXiv.2501.16224</a>.
</div>
<div id="ref-clune2019ai0gas0" class="csl-entry" role="listitem">
Clune, Jeff. 2019. <span>“AI-GAs: AI-Generating Algorithms, an Alternate Paradigm for Producing General Artificial Intelligence.”</span> <em>arXiv Preprint arXiv: 1905.10985</em>. <a href="https://doi.org/10.48550/arXiv.1905.10985">https://doi.org/10.48550/arXiv.1905.10985</a>.
</div>
<div id="ref-coley2020autonomous" class="csl-entry" role="listitem">
Coley, Connor W, Natalie S Eyke, and Klavs F Jensen. 2020. <span>“Autonomous Discovery in the Chemical Sciences Part i: Progress.”</span> <em>Angewandte Chemie International Edition</em> 59 (51): 22858–93. <a href="https://doi.org/10.1002/anie.201909987">https://doi.org/10.1002/anie.201909987</a>.
</div>
<div id="ref-conrad2025lowering" class="csl-entry" role="listitem">
Conrad, Stefan, Philipp Auth, Tom Masselter, and Thomas Speck. 2025. <span>“Lowering the Entrance Hurdle for Lab Automation: An Artificial Intelligence‐supported, Interactive Robotic Arm for Automated, Repeated Testing Procedures.”</span> <em>Advanced Intelligent Systems</em>. <a href="https://doi.org/10.1002/aisy.202401086">https://doi.org/10.1002/aisy.202401086</a>.
</div>
<div id="ref-corey1972computer" class="csl-entry" role="listitem">
Corey, Elias J, Richard D Cramer III, and W Jeffrey Howe. 1972. <span>“<span class="nocase">Computer-assisted synthetic analysis for complex molecules. Methods and procedures for machine generation of synthetic intermediates</span>.”</span> <em>Journal of the American Chemical Society</em> 94 (2): 440–59. <a href="https://doi.org/10.1021/ja00757a022">https://doi.org/10.1021/ja00757a022</a>.
</div>
<div id="ref-decurt2024large" class="csl-entry" role="listitem">
Curtò, J. de, I. de Zarzà, Gemma Roig, and Carlos T. Calafate. 2024. <span>“Large Language Model-Informed x-Ray Photoelectron Spectroscopy Data Analysis.”</span> <em>Signals</em> 5 (2): 181–201. <a href="https://doi.org/10.3390/signals5020010">https://doi.org/10.3390/signals5020010</a>.
</div>
<div id="ref-dagan2023dynamic" class="csl-entry" role="listitem">
Dagan, Gautier, Frank Keller, and Alex Lascarides. 2023. <span>“Dynamic Planning with a Llm.”</span> <em>arXiv Preprint arXiv:2308.06391</em>. <a href="https://doi.org/10.48550/arXiv.2308.06391">https://doi.org/10.48550/arXiv.2308.06391</a>.
</div>
<div id="ref-dagdelen2024structured" class="csl-entry" role="listitem">
Dagdelen, John, Alexander Dunn, Sanghoon Lee, Nicholas Walker, Andrew S Rosen, Gerbrand Ceder, Kristin A Persson, and Anubhav Jain. 2024. <span>“Structured Information Extraction from Scientific Text with Large Language Models.”</span> <em>Nature Communications</em> 15 (1): 1418. <a href="https://doi.org/10.1038/s41467-024-45563-x">https://doi.org/10.1038/s41467-024-45563-x</a>.
</div>
<div id="ref-darvish2025organa" class="csl-entry" role="listitem">
Darvish, Kourosh, Marta Skreta, Yuchi Zhao, Naruki Yoshikawa, Sagnik Som, Miroslav Bogdanovic, Yang Cao, et al. 2025. <span>“ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization.”</span> <em>Matter</em> 8 (2). <a href="https://doi.org/10.1016/j.matt.2024.10.015">https://doi.org/10.1016/j.matt.2024.10.015</a>.
</div>
<div id="ref-dinh2022lift" class="csl-entry" role="listitem">
Dinh, Tuan, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dimitris Papailiopoulos, and Kangwook Lee. 2022. <span>“<span class="nocase"><span>LIFT</span>: <span>Language</span>-<span>Interfaced</span> <span>Fine</span>-<span>Tuning</span> for <span>Non</span>-language <span>Machine</span> <span>Learning</span> <span>Tasks</span></span>.”</span> <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em> 35: 11763–84. <a href="https://doi.org/10.48550/arXiv.2206.06565">https://doi.org/10.48550/arXiv.2206.06565</a>.
</div>
<div id="ref-du2023improving" class="csl-entry" role="listitem">
Du, Yilun, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2023. <span>“Improving Factuality and Reasoning in Language Models Through Multiagent Debate.”</span> <em>Forty-First International Conference on Machine Learning</em>. <a href="https://doi.org/10.48550/arXiv.2305.14325">https://doi.org/10.48550/arXiv.2305.14325</a>.
</div>
<div id="ref-edwards2022translation" class="csl-entry" role="listitem">
Edwards, Carl, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. 2022. <span>“Translation Between Molecules and Natural Language.”</span> <em>Arxiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2204.11817">https://doi.org/10.48550/arXiv.2204.11817</a>.
</div>
<div id="ref-edwards2021text2mol" class="csl-entry" role="listitem">
Edwards, Carl, ChengXiang Zhai, and Heng Ji. 2021. <span>“<span>T</span>ext2<span>M</span>ol: Cross-Modal Molecule Retrieval with Natural Language Queries.”</span> <em>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, November, 595–607. <a href="https://doi.org/10.18653/v1/2021.emnlp-main.47">https://doi.org/10.18653/v1/2021.emnlp-main.47</a>.
</div>
<div id="ref-fernando2023promptbreeder0" class="csl-entry" role="listitem">
Fernando, Chrisantha, Dylan Banarse, H. Michalewski, Simon Osindero, and Tim Rocktäschel. 2023. <span>“Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution.”</span> <em>International Conference on Machine Learning</em>. <a href="https://doi.org/10.48550/arXiv.2309.16797">https://doi.org/10.48550/arXiv.2309.16797</a>.
</div>
<div id="ref-fifty2023incontext" class="csl-entry" role="listitem">
Fifty, Christopher, Jure Leskovec, and Sebastian Thrun. 2023. <span>“<span class="nocase">In-Context Learning for Few-Shot Molecular Property Prediction</span>.”</span> <em>arXiv Preprint arXiv: 2310.08863</em>. <a href="https://doi.org/10.48550/arXiv.2310.08863">https://doi.org/10.48550/arXiv.2310.08863</a>.
</div>
<div id="ref-Fleming1929antibacterial" class="csl-entry" role="listitem">
Fleming, Alexander. 1929. <span>“On the Antibacterial Action of Cultures of a <em>Penicillium</em>, with Special Reference to Their Use in the Isolation of <em>b. Influenzae</em>.”</span> <em>British Journal of Experimental Pathology</em> 10 (3): 226–36. <a href="https://www.jstor.org/stable/4452419">https://www.jstor.org/stable/4452419</a>.
</div>
<div id="ref-Fleming1945penicillin" class="csl-entry" role="listitem">
———. 1964. <span>“Penicillin.”</span> In <em>Nobel Lectures, Physiology or Medicine 1942–1962</em>, 83–93. Amsterdam: Elsevier. <a href="https://www.nobelprize.org/uploads/2018/06/fleming-lecture.pdf">https://www.nobelprize.org/uploads/2018/06/fleming-lecture.pdf</a>.
</div>
<div id="ref-frey2023neural" class="csl-entry" role="listitem">
Frey, Nathan C., Ryan Soklaski, Simon Axelrod, Siddharth Samsi, Rafael Gómez-Bombarelli, Connor W. Coley, and Vijay Gadepally. 2023. <span>“Neural Scaling of Deep Chemical Models.”</span> <em>Nature Machine Intelligence</em> 5 (11): 1297–1305. <a href="https://doi.org/10.1038/s42256-023-00740-3">https://doi.org/10.1038/s42256-023-00740-3</a>.
</div>
<div id="ref-Fu2025large" class="csl-entry" role="listitem">
Fu, Li, Qingwei Zhou, Meiqing Jin, and Weihong Wu. 2025. <span>“Large Language Models as Spectrographic Assistants: Opportunities and Challenges in Laboratory Data Analysis.”</span> <em>Environmental Chemistry and Safety</em>, April. <a href="https://doi.org/10.26599/ecs.2025.9600002">https://doi.org/10.26599/ecs.2025.9600002</a>.
</div>
<div id="ref-Gadde2025chatbot" class="csl-entry" role="listitem">
Gadde, Rohit S. K., Sreelaya Devaguptam, Fangning Ren, Rajat Mittal, Lechen Dong, Yao Wang, and Fang Liu. 2025. <span>“Chatbot-Assisted Quantum Chemistry for Explicitly Solvated Molecules.”</span> <em>Chemical Science</em> 16 (9): 3852–64. <a href="https://doi.org/10.1039/D4SC08677E">https://doi.org/10.1039/D4SC08677E</a>.
</div>
<div id="ref-ganose2019robocrystallographer" class="csl-entry" role="listitem">
Ganose, Alex M, and Anubhav Jain. 2019. <span>“<span class="nocase">Robocrystallographer: automated crystal structure text descriptions and analysis</span>.”</span> <em>MRS Communications</em> 9 (3): 874–81. <a href="https://doi.org/10.1557/mrc.2019.94">https://doi.org/10.1557/mrc.2019.94</a>.
</div>
<div id="ref-gao2022sample" class="csl-entry" role="listitem">
Gao, Wenhao, Tianfan Fu, Jimeng Sun, and Connor W. Coley. 2022. <span>“Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization.”</span> <em>Neural Information Processing Systems</em>. <a href="https://doi.org/10.48550/arXiv.2206.12411">https://doi.org/10.48550/arXiv.2206.12411</a>.
</div>
<div id="ref-gao2025synergizing" class="csl-entry" role="listitem">
Gao, Yunfan, Yun Xiong, Yijie Zhong, Yuxi Bi, Ming Xue, and Haofen Wang. 2025. <span>“Synergizing Rag and Reasoning: A Systematic Review.”</span> <em>arXiv Preprint arXiv:2504.15909</em>. <a href="https://doi.org/10.48550/arXiv.2504.15909">https://doi.org/10.48550/arXiv.2504.15909</a>.
</div>
<div id="ref-Ghafarollahi2024" class="csl-entry" role="listitem">
Ghafarollahi, Alireza, and Markus J. Buehler. 2024. <span>“SciAgents: Automating Scientific Discovery Through Bioinspired Multi-Agent Intelligent Graph Reasoning.”</span> <em>Advanced Materials</em>, December. <a href="https://doi.org/10.1002/adma.202413523">https://doi.org/10.1002/adma.202413523</a>.
</div>
<div id="ref-ghareeb2025robin0" class="csl-entry" role="listitem">
Ghareeb, Ali Essam, Benjamin Chang, Ludovico Mitchener, Angela Yiu, Caralyn J. Szostkiewicz, Jon M. Laurent, Muhammed T. Razzak, Andrew D. White, Michaela M. Hinks, and Samuel G. Rodriques. 2025. <span>“Robin: A Multi-Agent System for Automating Scientific Discovery.”</span> <em>arXiv Preprint arXiv: 2505.13400</em>. <a href="https://doi.org/10.48550/arXiv.2505.13400">https://doi.org/10.48550/arXiv.2505.13400</a>.
</div>
<div id="ref-giglio2023use" class="csl-entry" role="listitem">
Giglio, Auro Del, and Mateus Uerlei Pereira da Costa. 2023. <span>“The Use of Artificial Intelligence to Improve the Scientific Writing of Non-Native English Speakers.”</span> <em>Revista Da Associa<span>ç</span><span>ã</span>o M<span>é</span>dica Brasileira</em> 69 (9): e20230560. <a href="https://doi.org/10.1590/1806-9282.20230560">https://doi.org/10.1590/1806-9282.20230560</a>.
</div>
<div id="ref-goldberg2024usefulness" class="csl-entry" role="listitem">
Goldberg, Alexander, Ihsan Ullah, Thanh Gia Hieu Khuong, Benedictus Kent Rachmat, Zhen Xu, Isabelle Guyon, and Nihar B. Shah. 2024. <span>“Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS’24 Experiment.”</span> <em>arXiv Preprint arXiv: 2411.03417</em>. <a href="https://doi.org/10.48550/arXiv.2411.03417">https://doi.org/10.48550/arXiv.2411.03417</a>.
</div>
<div id="ref-gottweis2025towards" class="csl-entry" role="listitem">
Gottweis, Juraj, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, et al. 2025. <span>“Towards an <span>AI</span> Co-Scientist.”</span> <em>Arxiv Preprint arXiv:2502.18864</em>, February. <a href="https://doi.org/10.48550/arXiv.2502.18864">https://doi.org/10.48550/arXiv.2502.18864</a>.
</div>
<div id="ref-Griffiths2020constrained" class="csl-entry" role="listitem">
Griffiths, Ryan-Rhys, and José Miguel Hernández-Lobato. 2020. <span>“Constrained Bayesian Optimization for Automatic Chemical Design Using Variational Autoencoders.”</span> <em>Chemical Science</em> 11 (2): 577–86. <a href="https://doi.org/10.1039/c9sc04026a">https://doi.org/10.1039/c9sc04026a</a>.
</div>
<div id="ref-xdl2023spec" class="csl-entry" role="listitem">
Group, Cronin. 2023. <span>“XDL 2.0 Standard Specification.”</span> <a href="https://gitlab.com/croningroup/chi-dl-specification">https://gitlab.com/croningroup/chi-dl-specification</a>.
</div>
<div id="ref-gruver2024promises" class="csl-entry" role="listitem">
Gruver, Nate, Marc Anton Finzi, Dylan Sam, J. Zico Kolter, Ben Athiwaratkun, and Andrew Gordon Wilson. 2024. <span>“The Promises and Pitfalls of Language Models for Structured Numerical Data.”</span> <em>OpenReview.net</em>, October. <a href="https://openreview.net/forum?id=SZpygmv3G1">https://openreview.net/forum?id=SZpygmv3G1</a>.
</div>
<div id="ref-grzybowski2018chematica" class="csl-entry" role="listitem">
Grzybowski, Bartosz A, Sara Szymkuć, Ewa P Gajewska, Karol Molga, Piotr Dittwald, Agnieszka Wołos, and Tomasz Klucznik. 2018. <span>“<span class="nocase">Chematica: a story of computer code that started to think like a chemist</span>.”</span> <em>Chem</em> 4 (3): 390–98. <a href="https://doi.org/10.1016/j.chempr.2018.02.024">https://doi.org/10.1016/j.chempr.2018.02.024</a>.
</div>
<div id="ref-gu2024interesting" class="csl-entry" role="listitem">
Gu, Xuemei, and Mario Krenn. 2024. <span>“Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders.”</span> <em>arXiv Preprint arXiv: 2405.17044</em>. <a href="https://doi.org/10.48550/arXiv.2405.17044">https://doi.org/10.48550/arXiv.2405.17044</a>.
</div>
<div id="ref-Gu2025forecasting" class="csl-entry" role="listitem">
———. 2025. <span>“Forecasting High-Impact Research Topics via Machine Learning on Evolving Knowledge Graphs.”</span> <em>Machine Learning: Science and Technology</em> 6 (2): 025041. <a href="https://doi.org/10.1088/2632-2153/add6ef">https://doi.org/10.1088/2632-2153/add6ef</a>.
</div>
<div id="ref-Guo2021" class="csl-entry" role="listitem">
Guo, Jiang, A. Santiago Ibanez-Lopez, Hanyu Gao, Victor Quach, Connor W. Coley, Klavs F. Jensen, and Regina Barzilay. 2021. <span>“Automated Chemical Reaction Extraction from Scientific Literature.”</span> <em>Journal of Chemical Information and Modeling</em> 62 (9): 2035–45. <a href="https://doi.org/10.1021/acs.jcim.1c00284">https://doi.org/10.1021/acs.jcim.1c00284</a>.
</div>
<div id="ref-guo2023large" class="csl-entry" role="listitem">
Guo, Taicheng, Kehan Guo, B. Nan, Zhengwen Liang, Zhichun Guo, N. Chawla, O. Wiest, and Xiangliang Zhang. 2023. <span>“<span class="nocase">What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks</span>.”</span> <em>Neural Information Processing Systems</em>. <a href="https://doi.org/10.48550/arXiv.2305.18365">https://doi.org/10.48550/arXiv.2305.18365</a>.
</div>
<div id="ref-gupta2024data" class="csl-entry" role="listitem">
Gupta, Sonakshi, Akhlak Mahmood, Pranav Shetty, Aishat Adeboye, and Rampi Ramprasad. 2024. <span>“Data Extraction from Polymer Literature Using Large Language Models.”</span> <em>Communications Materials</em> 5 (1): 269. <a href="https://doi.org/10.1038/s43246-024-00708-9">https://doi.org/10.1038/s43246-024-00708-9</a>.
</div>
<div id="ref-hall1991crystallographic" class="csl-entry" role="listitem">
Hall, S. R., F. H. Allen, and I. D. Brown. 1991. <span>“The Crystallographic Information File (<span>CIF</span>): A New Standard Archive File for Crystallography.”</span> <em>Acta Crystallographica Section A</em> 47 (6): 655–85. <a href="https://doi.org/10.1107/S010876739101067X">https://doi.org/10.1107/S010876739101067X</a>.
</div>
<div id="ref-hammer2021chemputation" class="csl-entry" role="listitem">
Hammer, Alexander J. S., Andrei I. Leonov, Nicholas L. Bell, and Leroy Cronin. 2021. <span>“Chemputation and the Standardization of Chemical Informatics.”</span> <em>JACS Au</em> 1 (10): 1572–87. <a href="https://doi.org/10.1021/jacsau.1c00303">https://doi.org/10.1021/jacsau.1c00303</a>.
</div>
<div id="ref-hao2023reasoning" class="csl-entry" role="listitem">
Hao, Shibo, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. <span>“<span class="nocase">Reasoning with language model is planning with world model</span>.”</span> <em>arXiv Preprint arXiv:2305.14992</em>. <a href="https://doi.org/10.48550/arXiv.2305.14992">https://doi.org/10.48550/arXiv.2305.14992</a>.
</div>
<div id="ref-Hse2021gryffin" class="csl-entry" role="listitem">
Häse, Florian, Matteo Aldeghi, Riley J. Hickman, Loı̈c M. Roch, and Alán Aspuru-Guzik. 2021. <span>“G&lt;scp&gt;ryffin&lt;/Scp&gt;: An Algorithm for Bayesian Optimization of Categorical Variables Informed by Expert Knowledge.”</span> <em>Applied Physics Reviews</em> 8 (3). <a href="https://doi.org/10.1063/5.0048164">https://doi.org/10.1063/5.0048164</a>.
</div>
<div id="ref-hira2024reconstructing" class="csl-entry" role="listitem">
Hira, Kausik, Mohd Zaki, Dhruvil Sheth, NM Anoop Krishnan, et al. 2024. <span>“Reconstructing the Materials Tetrahedron: Challenges in Materials Information Extraction.”</span> <em>Digital Discovery</em> 3 (5): 1021–37. <a href="https://doi.org/10.1039/d4dd00032c">https://doi.org/10.1039/d4dd00032c</a>.
</div>
<div id="ref-hsu2021scicap" class="csl-entry" role="listitem">
Hsu, Ting-Yao, C Lee Giles, and Ting-Hao’Kenneth’Huang. 2021. <span>“<span class="nocase">SciCap: Generating captions for scientific figures</span>.”</span> <em>arXiv Preprint arXiv:2110.11624</em>. <a href="https://doi.org/10.48550/arXiv.2110.11624">https://doi.org/10.48550/arXiv.2110.11624</a>.
</div>
<div id="ref-hsu2023gpt04" class="csl-entry" role="listitem">
Hsu, Ting-Yao, Chieh-Yang Huang, Ryan Rossi, Sungchul Kim, C. Lee Giles, and Ting-Hao K. Huang. 2023. <span>“<span class="nocase">GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions</span>.”</span> <em>arXiv Preprint arXiv: 2310.15405</em>. <a href="https://doi.org/10.48550/arXiv.2310.15405">https://doi.org/10.48550/arXiv.2310.15405</a>.
</div>
<div id="ref-hu2024automated" class="csl-entry" role="listitem">
Hu, Shengran, Cong Lu, and Jeff Clune. 2024. <span>“Automated Design of Agentic Systems.”</span> <em>arXiv Preprint arXiv: 2408.08435</em>. <a href="https://doi.org/10.48550/arXiv.2408.08435">https://doi.org/10.48550/arXiv.2408.08435</a>.
</div>
<div id="ref-huang2016understanding" class="csl-entry" role="listitem">
Huang, Bing, and O. Anatole von Lilienfeld. 2016. <span>“Understanding Molecular Representations in Machine Learning: The Role of Uniqueness and Target Similarity.”</span> <em>arXiv Preprint arXiv: 1608.06194</em>. <a href="https://doi.org/10.48550/arXiv.1608.06194">https://doi.org/10.48550/arXiv.1608.06194</a>.
</div>
<div id="ref-huang2023mlagentbench0" class="csl-entry" role="listitem">
Huang, Qian, Jian Vora, Percy Liang, and J. Leskovec. 2023. <span>“MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation.”</span> <em>International Conference on Machine Learning</em>. <a href="https://doi.org/10.48550/arXiv.2310.03302">https://doi.org/10.48550/arXiv.2310.03302</a>.
</div>
<div id="ref-huang2022batterybert" class="csl-entry" role="listitem">
Huang, Shu, and Jacqueline M Cole. 2022. <span>“BatteryBERT: A Pretrained Language Model for Battery Database Enhancement.”</span> <em>Journal of Chemical Information and Modeling</em> 62 (24): 6365–77.
</div>
<div id="ref-huang2022language" class="csl-entry" role="listitem">
Huang, Wenlong, Fei Fei, Trevor Darrell, and Yuke Zhu. 2022. <span>“Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents.”</span> <em>Proceedings of the 39th International Conference on Machine Learning (ICML)</em>. <a href="https://doi.org/10.48550/arXiv.2201.07207">https://doi.org/10.48550/arXiv.2201.07207</a>.
</div>
<div id="ref-inagaki2023robotic" class="csl-entry" role="listitem">
Inagaki, Takashi, Akari Kato, Koichi Takahashi, Haruka Ozaki, and Genki N. Kanda. 2023. <span>“LLMs Can Generate Robotic Scripts from Goal-Oriented Instructions in Biological Laboratory Automation.”</span> <em>arXiv Preprint arXiv:2304.10267</em>, April. <a href="https://doi.org/10.48550/arXiv.2304.10267">https://doi.org/10.48550/arXiv.2304.10267</a>.
</div>
<div id="ref-intologyai2025zochi" class="csl-entry" role="listitem">
Intology.ai. 2025. <span>“Zochi Publishes a* Paper.”</span> <a href="https://www.intology.ai/blog/zochi-acl">https://www.intology.ai/blog/zochi-acl</a>.
</div>
<div id="ref-Jablonka2023" class="csl-entry" role="listitem">
Jablonka, Kevin Maik, Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar, Joshua D. Bocarsly, Andres M. Bran, Stefan Bringuier, et al. 2023. <span>“<span class="nocase">14 examples of how LLMs can transform materials science and chemistry: a reflection on a large language model hackathon</span>.”</span> <em>Digital Discovery</em> 2 (5): 1233–50. <a href="https://doi.org/10.1039/d3dd00113j">https://doi.org/10.1039/d3dd00113j</a>.
</div>
<div id="ref-jablonka2022making" class="csl-entry" role="listitem">
Jablonka, Kevin Maik, Luc Patiny, and Berend Smit. 2022. <span>“<span class="nocase">Making the collective knowledge of chemistry open and machine actionable</span>.”</span> <em>Nature Chemistry</em> 14 (4): 365–76. <a href="https://doi.org/10.1038/s41557-022-00910-7">https://doi.org/10.1038/s41557-022-00910-7</a>.
</div>
<div id="ref-jablonka2024leveraging" class="csl-entry" role="listitem">
Jablonka, Kevin Maik, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. 2024. <span>“<span class="nocase">Leveraging large language models for predictive chemistry</span>.”</span> <em>Nature Machine Intelligence</em> 6 (2): 161–69. <a href="https://doi.org/10.1038/s42256-023-00788-1">https://doi.org/10.1038/s42256-023-00788-1</a>.
</div>
<div id="ref-Jacobs2025orca" class="csl-entry" role="listitem">
Jacobs, Pieter Floris, and Robert Pollice. 2025. <span>“Developing Large Language Models for Quantum Chemistry Simulation Input Generation.”</span> <em>Digital Discovery</em> 4 (3): 762–75. <a href="https://doi.org/10.1039/D4DD00366G">https://doi.org/10.1039/D4DD00366G</a>.
</div>
<div id="ref-jang2025can" class="csl-entry" role="listitem">
Jang, Hyosoon, Yunhui Jang, Jaehyung Kim, and Sungsoo Ahn. 2025. <span>“Can <span>LLMs</span> <span>Generate</span> <span>Diverse</span> <span>Molecules</span>? <span>Towards</span> <span>Alignment</span> with <span>Structural</span> <span>Diversity</span>.”</span> <em>Arxiv Preprint arXiv:2410.03138</em>, February. <a href="https://doi.org/10.48550/arXiv.2410.03138">https://doi.org/10.48550/arXiv.2410.03138</a>.
</div>
<div id="ref-jansen2025codescientist0" class="csl-entry" role="listitem">
Jansen, Peter, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Daniel S. Weld, and Peter Clark. 2025. <span>“CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-Based Experimentation.”</span> <em>arXiv Preprint arXiv: 2503.22708</em>. <a href="https://doi.org/10.48550/arXiv.2503.22708">https://doi.org/10.48550/arXiv.2503.22708</a>.
</div>
<div id="ref-ji2025test" class="csl-entry" role="listitem">
Ji, Yixin, Juntao Li, Hai Ye, Kaixin Wu, Kai Yao, Jia Xu, Linjian Mo, and Min Zhang. 2025. <span>“A Survey of Test-Time Compute: From Intuitive Inference to Deliberate Reasoning.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2501.02497">https://doi.org/10.48550/arXiv.2501.02497</a>.
</div>
<div id="ref-jiang2024protocode" class="csl-entry" role="listitem">
Jiang, Shuo, Daniel Evans-Yamamoto, Dennis Bersenev, Sucheendra K Palaniappan, and Ayako Yachie-Kinoshita. 2024. <span>“ProtoCode: Leveraging Large Language Models (LLMs) for Automated Generation of Machine-Readable PCR Protocols from Scientific Publications.”</span> <em>SLAS Technology</em> 29 (3): 100134. <a href="https://doi.org/10.1016/j.slast.2024.100134">https://doi.org/10.1016/j.slast.2024.100134</a>.
</div>
<div id="ref-Jing2022roles" class="csl-entry" role="listitem">
Jing, Xia, Vimla L Patel, James J Cimino, Jay H Shubrook, Yuchun Zhou, Chang Liu, and Sonsoles De Lacalle. 2022. <span>“The Roles of a Secondary Data Analytics Tool and Experience in Scientific Hypothesis Generation in Clinical Research: Protocol for a Mixed Methods Study.”</span> <em>JMIR Research Protocols</em> 11 (7): e39414. <a href="https://doi.org/10.2196/39414">https://doi.org/10.2196/39414</a>.
</div>
<div id="ref-kahneman2011thinking" class="csl-entry" role="listitem">
Kahneman, Daniel. 2011. <em>Thinking, Fast and Slow</em>. New York: Farrar, Straus; Giroux.
</div>
<div id="ref-kambhampati2023llmplanning" class="csl-entry" role="listitem">
Kambhampati, Subbarao, Karthik Valmeekam, Miquel Marquez, and Luyang Guan. 2023. <span>“<span class="nocase">On the Role of Large Language Models in Planning</span>.”</span> Tutorial presented at the International Conference on Automated Planning and Scheduling (ICAPS). <a href="https://yochan-lab.github.io/tutorial/ICAPS-2023/">https://yochan-lab.github.io/tutorial/ICAPS-2023/</a>.
</div>
<div id="ref-kang2024chatmof" class="csl-entry" role="listitem">
Kang, Yeonghun, and Jihan Kim. 2024. <span>“ChatMOF: An Artificial Intelligence System for Predicting and Generating Metal-Organic Frameworks Using Large Language Models.”</span> <em>Nature Communications</em> 15 (1): 4705. <a href="https://doi.org/10.1038/s41467-024-48998-4">https://doi.org/10.1038/s41467-024-48998-4</a>.
</div>
<div id="ref-kaur2025data" class="csl-entry" role="listitem">
Kaur, Harveen, Flaviano Della Pia, Ilyes Batatia, Xavier R Advincula, Benjamin X Shi, Jinggang Lan, Gábor Csányi, Angelos Michaelides, and Venkat Kapil. 2025. <span>“Data-Efficient Fine-Tuning of Foundational Models for First-Principles Quality Sublimation Enthalpies.”</span> <em>Faraday Discussions</em> 256: 120–38. <a href="https://doi.org/10.1039/d4fd00107a">https://doi.org/10.1039/d4fd00107a</a>.
</div>
<div id="ref-kawchak2024high" class="csl-entry" role="listitem">
Kawchak, Kevin. 2024. <span>“High Dimensional and Complex Spectrometric Data Analysis of an Organic Compound Using Large Multimodal Models and Chained Outputs.”</span> <em>ChemRxiv Preprint</em>, September. <a href="https://doi.org/10.26434/chemrxiv-2024-06gf1">https://doi.org/10.26434/chemrxiv-2024-06gf1</a>.
</div>
<div id="ref-kayali2023chorus" class="csl-entry" role="listitem">
Kayali, Moe, Anton Lykov, Ilias Fountalis, Nikolaos Vasiloglou, Dan Olteanu, and Dan Suciu. 2024. <span>“<span>CHORUS:</span> Foundation Models for Unified Data Discovery and Exploration.”</span> <em>Proc. <span>VLDB</span> Endow.</em> 17 (8): 2104–14. <a href="https://doi.org/10.14778/3659437.3659461">https://doi.org/10.14778/3659437.3659461</a>.
</div>
<div id="ref-Kearnes_2021" class="csl-entry" role="listitem">
Kearnes, Steven M., Michael R. Maser, Michael Wleklinski, Anton Kast, Abigail G. Doyle, Spencer D. Dreher, Joel M. Hawkins, Klavs F. Jensen, and Connor W. Coley. 2021. <span>“<span>The Open Reaction Database</span>.”</span> <em>J. Am. Chem. Soc.</em> 143 (45): 18820–26. <a href="https://doi.org/10.1021/jacs.1c09820">https://doi.org/10.1021/jacs.1c09820</a>.
</div>
<div id="ref-khalifa2024using" class="csl-entry" role="listitem">
Khalifa, Mohamed, and Mona Albadawy. 2024. <span>“<span class="nocase">Using artificial intelligence in academic writing and research: An essential productivity tool</span>.”</span> <em>Computer Methods and Programs in Biomedicine Update</em>, 100145. <a href="https://doi.org/10.1016/j.cmpbup.2024.100145">https://doi.org/10.1016/j.cmpbup.2024.100145</a>.
</div>
<div id="ref-kim2024large" class="csl-entry" role="listitem">
Kim, Seongmin, Yousung Jung, and Joshua Schrier. 2024. <span>“Large Language Models for Inorganic Synthesis Predictions.”</span> <em>Journal of the American Chemical Society</em>.
</div>
<div id="ref-kim2025explainable" class="csl-entry" role="listitem">
Kim, Seongmin, Joshua Schrier, and Yousung Jung. 2025. <span>“Explainable Synthesizability Prediction of Inorganic Crystal Polymorphs Using Large Language Models.”</span> <em>Angewandte Chemie International Edition</em>. <a href="https://doi.org/10.1002/anie.202423950">https://doi.org/10.1002/anie.202423950</a>.
</div>
<div id="ref-kingsbury2022flexible" class="csl-entry" role="listitem">
Kingsbury, Ryan S., Andrew S. Rosen, Ayush S. Gupta, Jason M. Munro, Shyue Ping Ong, Anubhav Jain, Shyam Dwaraknath, Matthew K. Horton, and Kristin A. Persson. 2022. <span>“A Flexible and Scalable Scheme for Mixing Computed Formation Energies from Different Levels of Theory.”</span> <em>Npj Computational Materials</em>. <a href="https://doi.org/10.1038/s41524-022-00881-w">https://doi.org/10.1038/s41524-022-00881-w</a>.
</div>
<div id="ref-kinney2023semantic" class="csl-entry" role="listitem">
Kinney, Rodney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, et al. 2023. <span>“The Semantic Scholar Open Data Platform.”</span> <em>arXiv Preprint arXiv: 2301.10140</em>. <a href="https://doi.org/10.48550/arXiv.2301.10140">https://doi.org/10.48550/arXiv.2301.10140</a>.
</div>
<div id="ref-kon2025exp0bench0" class="csl-entry" role="listitem">
Kon, Patrick Tser Jern, Jiachen Liu, Xinyi Zhu, Qiuyi Ding, Jingjia Peng, Jiarong Xing, Yibo Huang, et al. 2025. <span>“EXP-Bench: Can AI Conduct AI Research Experiments?”</span> <em>arXiv Preprint arXiv: 2505.24785</em>. <a href="https://doi.org/10.48550/arXiv.2505.24785">https://doi.org/10.48550/arXiv.2505.24785</a>.
</div>
<div id="ref-kosso2017whatgoesup" class="csl-entry" role="listitem">
Kosso, Peter. 2017. <em>What Goes up... Gravity and Scientific Method</em>. Cambridge: Cambridge University Press. <a href="https://doi.org/10.1017/9781316417003">https://doi.org/10.1017/9781316417003</a>.
</div>
<div id="ref-krenn2020self" class="csl-entry" role="listitem">
Krenn, Mario, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. 2020. <span>“<span class="nocase">Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation</span>.”</span> <em>Machine Learning: Science and Technology</em> 1 (4): 045024. <a href="https://doi.org/10.1088/2632-2153/aba947">https://doi.org/10.1088/2632-2153/aba947</a>.
</div>
<div id="ref-kristiadi2024sober" class="csl-entry" role="listitem">
Kristiadi, Agustinus, Felix Strieth-Kalthoff, Marta Skreta, Pascal Poupart, Alán Aspuru-Guzik, and Geoff Pleiss. 2024. <span>“A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization over Molecules?”</span> <em>Forty-First International Conference on Machine Learning, <span>ICML</span> 2024</em>. <a href="https://doi.org/10.48550/arXiv.2402.05015">https://doi.org/10.48550/arXiv.2402.05015</a>.
</div>
<div id="ref-krzyzanowski2025exploring" class="csl-entry" role="listitem">
Krzyzanowski, Adrian, Stephen D. Pickett, and Peter Pogány. 2025. <span>“Exploring <span>BERT</span> for Reaction Yield Prediction: Evaluating the Impact of Tokenization, Molecular Representation, and Pretraining Data Augmentation.”</span> <em>Journal of Chemical Information and Modeling</em> 65 (9): 4381–4402. <a href="https://doi.org/10.1021/acs.jcim.5c00359">https://doi.org/10.1021/acs.jcim.5c00359</a>.
</div>
<div id="ref-Kuhn1962Structure" class="csl-entry" role="listitem">
Kuhn, Thomas S. 1962. <em>The Structure of Scientific Revolutions</em>. Vol. 2. International Encyclopedia of Unified Science 2. Chicago: University of Chicago Press.
</div>
<div id="ref-kumar2025mechbert" class="csl-entry" role="listitem">
Kumar, Pankaj, Saurabh Kabra, and Jacqueline M Cole. 2025. <span>“MechBERT: Language Models for Extracting Chemical and Property Relationships about Mechanical Stress and Strain.”</span> <em>Journal of Chemical Information and Modeling</em>.
</div>
<div id="ref-kumbhar2025hypothesis" class="csl-entry" role="listitem">
Kumbhar, Shrinidhi, Venkatesh Mishra, Kevin Coutinho, Divij Handa, Ashif Iquebal, and Chitta Baral. 2025. <span>“Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents.”</span> <em>North American Chapter of the Association for Computational Linguistics</em>. <a href="https://doi.org/10.48550/arXiv.2501.13299">https://doi.org/10.48550/arXiv.2501.13299</a>.
</div>
<div id="ref-Lakatos1970falsification" class="csl-entry" role="listitem">
Lakatos, Imre. 1970. <span>“Falsification and the Methodology of Scientific Research Programmes.”</span> In <em>Criticism and the Growth of Knowledge</em>, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.
</div>
<div id="ref-lee2024fine-tuning" class="csl-entry" role="listitem">
Lee, Daeseok, and Yongjun Cho. 2024. <span>“<span>FINE</span>-<span>TUNING</span> <span>POCKET</span>-<span>CONDITIONED</span> <span>3D</span> <span>MOLECULE</span> <span>GENERATION</span> <span>VIA</span> <span>REINFORCEMENT</span> <span>LEARNING</span>.”</span> <em>The Twelfth International Conference on Learning Representations Workshop on Generative and Experimental Perspectives for Biomolecular Design, <span>ICLR-GEM</span></em>. <a href="https://openreview.net/forum?id=hlzRzr9ksu">https://openreview.net/forum?id=hlzRzr9ksu</a>.
</div>
<div id="ref-lee2025rag-enhanced" class="csl-entry" role="listitem">
Lee, Namkyeong, Edward De Brouwer, Ehsan Hajiramezanali, Tommaso Biancalani, Chanyoung Park, and Gabriele Scalia. 2025. <span>“RAG-Enhanced Collaborative LLM Agents for Drug Discovery.”</span> <em>arXiv Preprint arXiv: 2502.17506</em>. <a href="https://doi.org/10.48550/arXiv.2502.17506">https://doi.org/10.48550/arXiv.2502.17506</a>.
</div>
<div id="ref-leonov2024integrated" class="csl-entry" role="listitem">
Leonov, Artem I., Alexander J. S. Hammer, Sławomir Lach, S. Hessam M. Mehr, Dario Caramelli, Davide Angelone, Aamir Khan, et al. 2024. <span>“An Integrated Self-Optimizing Programmable Chemical Synthesis and Reaction Engine.”</span> <em>Nature Communications</em> 15 (1): 4544. <a href="https://doi.org/10.1038/s41467-024-45444-3">https://doi.org/10.1038/s41467-024-45444-3</a>.
</div>
<div id="ref-li2023teach" class="csl-entry" role="listitem">
Li, Cheng, Mingyang Zhang, Qiaozhu Mei, Yaqing Wang, Spurthi Amba Hombaiah, Yi Liang, and Michael Bendersky. 2023. <span>“Teach LLMs to Personalize - an Approach Inspired by Writing Education.”</span> <em>arXiv Preprint arXiv: 2308.07968</em>. <a href="https://doi.org/10.48550/arXiv.2308.07968">https://doi.org/10.48550/arXiv.2308.07968</a>.
</div>
<div id="ref-li2025large" class="csl-entry" role="listitem">
Li, Jiatong, Wei Liu, Zhihao Ding, Wenqi Fan, Yuqiang Li, and Qing Li. 2025. <span>“Large <span>Language</span> <span>Models</span> Are in-<span>Context</span> <span>Molecule</span> <span>Learners</span>.”</span> <em>IEEE Transactions on Knowledge and Data Engineering</em> 37 (7). <a href="https://doi.org/10.1109/TKDE.2025.3557697">https://doi.org/10.1109/TKDE.2025.3557697</a>.
</div>
<div id="ref-li2024molreflect" class="csl-entry" role="listitem">
Li, Jiatong, Yunqing Liu, Wei Liu, Jingdi Le, Di Zhang, Wenqi Fan, Dongzhan Zhou, Yuqiang Li, and Qing Li. 2024. <span>“<span>MolReFlect</span>: <span>Towards</span> <span>In</span>-<span>Context</span> <span>Fine</span>-Grained <span>Alignments</span> Between <span>Molecules</span> and <span>Texts</span>.”</span> <em>Arxiv Preprint arXiv:2411.14721</em>, November. <a href="https://doi.org/10.48550/arXiv.2411.14721">https://doi.org/10.48550/arXiv.2411.14721</a>.
</div>
<div id="ref-Li2024sequential" class="csl-entry" role="listitem">
Li, Xiaobo, Yu Che, Linjiang Chen, Tao Liu, Kewei Wang, Lunjie Liu, Haofan Yang, Edward O. Pyzer-Knapp, and Andrew I. Cooper. 2024. <span>“Sequential Closed-Loop Bayesian Optimization as a Guide for Organic Molecular Metallophotocatalyst Formulation Discovery.”</span> <em>Nature Chemistry</em> 16 (8): 1286–94. <a href="https://doi.org/10.1038/s41557-024-01546-5">https://doi.org/10.1038/s41557-024-01546-5</a>.
</div>
<div id="ref-li2024unveiling" class="csl-entry" role="listitem">
Li, Zhuoran, Xu Sun, Wanyu Lin, and Jiannong Cao. 2024. <span>“<span class="nocase">Unveiling Molecular Secrets: An LLM-Augmented Linear Model for Explainable and Calibratable Molecular Property Prediction</span>.”</span> <em>arXiv Preprint arXiv: 2410.08829</em>. <a href="https://doi.org/10.48550/arXiv.2410.08829">https://doi.org/10.48550/arXiv.2410.08829</a>.
</div>
<div id="ref-lim2021predicting" class="csl-entry" role="listitem">
Lim, Sangrak, and Yong Oh Lee. 2020. <span>“Predicting Chemical Properties Using Self-Attention Multi-Task Learning Based on <span>SMILES</span> Representation.”</span> <em>25th International Conference on Pattern Recognition, <span>ICPR</span> 2020, Virtual Event / Milan, Italy, January 10-15, 2021</em>, 3146–53. <a href="https://doi.org/10.1109/ICPR48806.2021.9412555">https://doi.org/10.1109/ICPR48806.2021.9412555</a>.
</div>
<div id="ref-lin2025property" class="csl-entry" role="listitem">
Lin, Xuan, Long Chen, Yile Wang, Xiangxiang Zeng, and Philip S. Yu. 2025. <span>“Property <span>Enhanced</span> <span>Instruction</span> <span>Tuning</span> for <span>Multi</span>-Task <span>Molecule</span> <span>Generation</span> with <span>Large</span> <span>Language</span> <span>Models</span>.”</span> <em>Arxiv Preprint arXiv:2412.18084</em>, May. <a href="https://doi.org/10.48550/arXiv.2412.18084">https://doi.org/10.48550/arXiv.2412.18084</a>.
</div>
<div id="ref-Listgarten2024perpetual" class="csl-entry" role="listitem">
Listgarten, Jennifer. 2024. <span>“The Perpetual Motion Machine of AI-Generated Data and the Distraction of ChatGPT as a <span>‘Scientist’</span>.”</span> <em>Nature Biotechnology</em> 42 (3): 371–73. <a href="https://doi.org/10.1038/s41587-023-02103-0">https://doi.org/10.1038/s41587-023-02103-0</a>.
</div>
<div id="ref-liu2023llm" class="csl-entry" role="listitem">
Liu, Bo, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023. <span>“<span class="nocase">Llm+ p: Empowering large language models with optimal planning proficiency</span>.”</span> <em>arXiv Preprint arXiv:2304.11477</em>. <a href="https://doi.org/10.48550/arXiv.2304.11477">https://doi.org/10.48550/arXiv.2304.11477</a>.
</div>
<div id="ref-liu2024multimodal" class="csl-entry" role="listitem">
Liu, Gang, Michael Sun, Wojciech Matusik, Meng Jiang, and Jie Chen. 2024. <span>“Multimodal <span>Large</span> <span>Language</span> <span>Models</span> for <span>Inverse</span> <span>Molecular</span> <span>Design</span> with <span>Retrosynthetic</span> <span>Planning</span>.”</span> <em>Arxiv Preprint arXiv: 2410.04223</em>, October. <a href="https://doi.org/10.48550/arXiv.2410.04223">https://doi.org/10.48550/arXiv.2410.04223</a>.
</div>
<div id="ref-liu2025integrating" class="csl-entry" role="listitem">
Liu, Hongxuan, Haoyu Yin, Zhiyao Luo, and Xiaonan Wang. 2025. <span>“Integrating Chemistry Knowledge in Large Language Models via Prompt Engineering.”</span> <em>Synthetic and Systems Biotechnology</em> 10 (1): 23–38. <a href="https://doi.org/10.1016/j.synbio.2024.07.004">https://doi.org/10.1016/j.synbio.2024.07.004</a>.
</div>
<div id="ref-liu2024moleculargpt" class="csl-entry" role="listitem">
Liu, Yuyan, Sirui Ding, Sheng Zhou, Wenqi Fan, and Qiaoyu Tan. 2024. <span>“<span>MolecularGPT</span>: <span>Open</span> <span>Large</span> <span>Language</span> <span>Model</span> (<span>LLM</span>) for <span>Few</span>-<span>Shot</span> <span>Molecular</span> <span>Property</span> <span>Prediction</span>.”</span> <em>Arxiv Preprint arXiv:2406.12950</em>, October. <a href="https://doi.org/10.48550/arXiv.2406.12950">https://doi.org/10.48550/arXiv.2406.12950</a>.
</div>
<div id="ref-Liu2025ASA" class="csl-entry" role="listitem">
Liu, Zhihan, Yubo Chai, and Jianfeng Li. 2025. <span>“Toward Automated Simulation Research Workflow Through LLM Prompt Engineering Design.”</span> <em>Journal of Chemical Information and Modeling</em> 65 (1): 114–24. <a href="https://doi.org/10.1021/acs.jcim.4c01653">https://doi.org/10.1021/acs.jcim.4c01653</a>.
</div>
<div id="ref-liu2023molca" class="csl-entry" role="listitem">
Liu, Zhiyuan, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. 2023. <span>“<span>MolCA</span>: <span>Molecular</span> <span>Graph</span>-<span>Language</span> <span>Modeling</span> with <span>Cross</span>-<span>Modal</span> <span>Projector</span> and <span>Uni</span>-<span>Modal</span> <span>Adapter</span>.”</span> <em>arXiv Preprint arXiv:2310.12798v4</em>, October. <a href="https://doi.org/10.48550/arXiv.2310.12798">https://doi.org/10.48550/arXiv.2310.12798</a>.
</div>
<div id="ref-livne2024nach0" class="csl-entry" role="listitem">
Livne, Micha, Zulfat Miftahutdinov, Elena Tutubalina, Maksim Kuznetsov, Daniil Polykovskiy, Annika Brundyn, Aastha Jhunjhunwala, et al. 2024. <span>“<span class="nocase">nach0: Multimodal natural and chemical languages foundation model</span>.”</span> <em>Chemical Science</em> 15 (22): 8380–89. <a href="https://doi.org/10.1039/d4sc00966e">https://doi.org/10.1039/d4sc00966e</a>.
</div>
<div id="ref-lu2024generative" class="csl-entry" role="listitem">
Lu, Jieyu, Zhangde Song, Qiyuan Zhao, Yuanqi Du, Yirui Cao, Haojun Jia, and Chenru Duan. 2025. <span>“Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge and Reasoning Capability of Large Language Models.”</span> <em>Journal of the American Chemical Society</em>, July. <a href="https://doi.org/10.1021/jacs.5c02097">https://doi.org/10.1021/jacs.5c02097</a>.
</div>
<div id="ref-mehr2023digitizing" class="csl-entry" role="listitem">
M. Mehr, S Hessam, Dario Caramelli, and Leroy Cronin. 2023. <span>“Digitizing Chemical Discovery with a Bayesian Explorer for Interpreting Reactivity Data.”</span> <em>Proceedings of the National Academy of Sciences</em> 120 (17): e2220045120. <a href="https://doi.org/10.1073/pnas.2220045120">https://doi.org/10.1073/pnas.2220045120</a>.
</div>
<div id="ref-malkov2018efficient" class="csl-entry" role="listitem">
Malkov, Yu A, and Dmitry A Yashunin. 2018. <span>“Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 42 (4): 824–36. <a href="https://doi.org/10.1109/tpami.2018.2889473">https://doi.org/10.1109/tpami.2018.2889473</a>.
</div>
<div id="ref-mandal2024autonomous" class="csl-entry" role="listitem">
Mandal, Indrajeet, Jitendra Soni, Mohd Zaki, Morten M. Smedskjaer, Katrin Wondraczek, Lothar Wondraczek, Nitya Nand Gosvami, and N. M. Anoop Krishnan. 2024. <span>“<span class="nocase">Autonomous Microscopy Experiments through Large Language Model Agents</span>.”</span> <em>arXiv Preprint arXiv: 2501.10385</em>. <a href="https://doi.org/10.48550/arXiv.2501.10385">https://doi.org/10.48550/arXiv.2501.10385</a>.
</div>
<div id="ref-McDonald1988standard" class="csl-entry" role="listitem">
McDonald, Robert S., and Paul A. Wilks. 1988. <span>“JCAMP-DX: A Standard Form for Exchange of Infrared Spectra in Computer Readable Form.”</span> <em>Applied Spectroscopy</em> 42 (1): 151–62. <a href="https://doi.org/10.1366/0003702884428734">https://doi.org/10.1366/0003702884428734</a>.
</div>
<div id="ref-mehr2020universal" class="csl-entry" role="listitem">
Mehr, Saman H. M., Mark Craven, Andrei I. Leonov, Graham Keenan, and Leroy Cronin. 2020. <span>“A Universal System for Digitization and Automatic Execution of the Chemical Synthesis Literature.”</span> <em>Science</em> 370 (6512): 101–8. <a href="https://doi.org/10.1126/science.abc2986">https://doi.org/10.1126/science.abc2986</a>.
</div>
<div id="ref-Mendible-Barreto2025DynaMate" class="csl-entry" role="listitem">
Mendible-Barreto, Orlando A., Misael Díaz-Maldonado, Fernando J. Carmona Esteva, J. Emmanuel Torres, Ubaldo M. Córdova-Figueroa, and Yamil J. Colón. 2025. <span>“DynaMate: Leveraging AI-Agents for Customized Research Workflows.”</span> <em>Molecular Systems Design &amp; Engineering</em> 10: 585–98. <a href="https://doi.org/10.1039/D5ME00062A">https://doi.org/10.1039/D5ME00062A</a>.
</div>
<div id="ref-miret2024llms" class="csl-entry" role="listitem">
Miret, Santiago, and N M Anoop Krishnan. 2024. <span>“Are LLMs Ready for Real-World Materials Discovery?”</span> <em>arXiv Preprint arXiv: 2402.05200</em>. <a href="https://doi.org/10.48550/arXiv.2402.05200">https://doi.org/10.48550/arXiv.2402.05200</a>.
</div>
<div id="ref-mirza2024large" class="csl-entry" role="listitem">
Mirza, Adrian, Nawaf Alampara, Sreekanth Kunchapu, Martiño Rı́os-Garcı́a, Benedict Emoekabu, Aswanth Krishnan, Tanya Gupta, et al. 2025. <span>“A Framework for Evaluating the Chemical Knowledge and Reasoning Abilities of Large Language Models Against the Expertise of Chemists.”</span> <em>Nature Chemistry</em>, 1–8. <a href="https://doi.org/10.1038/s41557-025-01815-x">https://doi.org/10.1038/s41557-025-01815-x</a>.
</div>
<div id="ref-mishra2024foundational" class="csl-entry" role="listitem">
Mishra, Vaibhav, Somaditya Singh, Dhruv Ahlawat, Mohd Zaki, Vaibhav Bihani, Hargun Singh Grover, Biswajit Mishra, Santiago Miret, Mausam, and N. M. Anoop Krishnan. 2024. <span>“Foundational Large Language Models for Materials Research.”</span> <em>arXiv Preprint arXiv: 2412.09560</em>. <a href="https://doi.org/10.48550/arXiv.2412.09560">https://doi.org/10.48550/arXiv.2412.09560</a>.
</div>
<div id="ref-narayan2022can" class="csl-entry" role="listitem">
Narayan, Avanika, Ines Chami, Laurel Orr, Simran Arora, and Christopher Ré. 2022. <span>“Can Foundation Models Wrangle Your Data?”</span> <em>Arxiv Preprint arXiv:2205.09911</em>. <a href="https://doi.org/10.48550/ARXIV.2205.09911">https://doi.org/10.48550/ARXIV.2205.09911</a>.
</div>
<div id="ref-narayanan2025training" class="csl-entry" role="listitem">
Narayanan, Siddharth M., James D. Braza, Ryan-Rhys Griffiths, Albert Bou, Geemi Wellawatte, Mayk Caldas Ramos, Ludovico Mitchener, Samuel G. Rodriques, and Andrew D. White. 2025. <span>“Training a Scientific Reasoning Model for Chemistry.”</span> <em>arXiv Preprint arXiv: 2506.17238</em>. <a href="https://doi.org/10.48550/arXiv.2506.17238">https://doi.org/10.48550/arXiv.2506.17238</a>.
</div>
<div id="ref-naumov2025dora" class="csl-entry" role="listitem">
Naumov, Vladimir, Diana Zagirova, Sha Lin, Yupeng Xie, Wenhao Gou, Anatoly Urban, Nina Tikhonova, et al. 2025. <span>“DORA AI Scientist: Multi-Agent Virtual Research Team for Scientific Exploration Discovery and Automated Report Generation.”</span> <em>bioRxiv</em>, March. <a href="https://doi.org/10.1101/2025.03.06.641840">https://doi.org/10.1101/2025.03.06.641840</a>.
</div>
<div id="ref-ORCA5" class="csl-entry" role="listitem">
Neese, Frank. 2022. <span>“Software Update: The ORCA Program System, Version 5.0.”</span> <em>Wiley Interdisciplinary Reviews: Computational Molecular Science</em> 12 (1): e1606. <a href="https://doi.org/10.1002/wcms.1606">https://doi.org/10.1002/wcms.1606</a>.
</div>
<div id="ref-newton1999principia" class="csl-entry" role="listitem">
Newton, Isaac. 1999. <em>The Principia: Mathematical Principles of Natural Philosophy</em>. Translated by I. Bernard Cohen and Anne Whitman. Berkeley: University of California Press.
</div>
<div id="ref-novikov2025alphaevolve" class="csl-entry" role="listitem">
Novikov, Alexander, Ngân Vũ, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, et al. 2025. <span>“Alpha<span>E</span>volve: A Coding Agent for Scientific and Algorithmic Discovery.”</span> <span>Google DeepMind</span>. <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf">https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf</a>.
</div>
<div id="ref-o2023bioplanner" class="csl-entry" role="listitem">
O’Donoghue, Odhran, Aleksandar Shtedritski, John Ginger, Ralph Abboud, Ali Essa Ghareeb, Justin Booth, and Samuel G Rodriques. 2023. <span>“BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology.”</span> <em>arXiv Preprint arXiv:2310.10632</em>. <a href="https://doi.org/10.48550/arXiv.2310.10632">https://doi.org/10.48550/arXiv.2310.10632</a>.
</div>
<div id="ref-oneill2025sparks" class="csl-entry" role="listitem">
O’Neill, Charles, Tirthankar Ghosal, Roberta Răileanu, Mike Walmsley, Thang Bui, Kevin Schawinski, and Ioana Ciucă. 2025. <span>“Sparks of Science: Hypothesis Generation Using Structured Paper Data.”</span> <em>arXiv Preprint arXiv: 2504.12976</em>. <a href="https://doi.org/10.48550/arXiv.2504.12976">https://doi.org/10.48550/arXiv.2504.12976</a>.
</div>
<div id="ref-Pagel2024LLMChemputer" class="csl-entry" role="listitem">
Pagel, Sebastian, Michal Jirásek, and Leroy Cronin. 2024. <span>“Validation of the Scientific Literature via Chemputation Augmented by Large Language Models.”</span> <em>arXiv Preprint arXiv:2410.06384</em>, October. <a href="https://doi.org/10.48550/arXiv.2410.06384">https://doi.org/10.48550/arXiv.2410.06384</a>.
</div>
<div id="ref-Park2023CMDL" class="csl-entry" role="listitem">
Park, Nathaniel H., Matteo Manica, Jannis Born, James L. Hedrick, Tim Erdmann, Dmitry Yu. Zubarev, Nil Adell-Mill, Pedro L. Arrechea, et al. 2023. <span>“Artificial Intelligence Driven Design of Catalysts and Materials for Ring Opening Polymerization Using a Domain-Specific Language.”</span> <em>Nature Communications</em> 14 (1). <a href="https://doi.org/10.1038/s41467-023-39396-3">https://doi.org/10.1038/s41467-023-39396-3</a>.
</div>
<div id="ref-Patiny2023automatic" class="csl-entry" role="listitem">
Patiny, Luc, and Guillaume Godin. 2023. <span>“Automatic Extraction of FAIR Data from Publications Using LLM.”</span> <em>ChemRxiv Preprint</em>. <a href="https://doi.org/10.26434/chemrxiv-2023-05v1b-v2">https://doi.org/10.26434/chemrxiv-2023-05v1b-v2</a>.
</div>
<div id="ref-polak2024extracting" class="csl-entry" role="listitem">
Polak, Maciej P, and Dane Morgan. 2024. <span>“Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering.”</span> <em>Nature Communications</em> 15 (1): 1569. <a href="https://doi.org/10.1038/s41467-024-45914-8">https://doi.org/10.1038/s41467-024-45914-8</a>.
</div>
<div id="ref-popper1959logic" class="csl-entry" role="listitem">
Popper, Karl R. 1959. <em>The Logic of Scientific Discovery</em>. London: Routledge.
</div>
<div id="ref-preuer2018frechet" class="csl-entry" role="listitem">
Preuer, Kristina, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Günter Klambauer. 2018. <span>“Fréchet ChemNet Distance: A Metric for Generative Models for Molecules in Drug Discovery.”</span> <em>Journal of Chemical Information and Modeling</em> 58 (9): 1736–41. <a href="https://doi.org/10.1021/acs.jcim.8b00234">https://doi.org/10.1021/acs.jcim.8b00234</a>.
</div>
<div id="ref-qu2023leveraging" class="csl-entry" role="listitem">
Qu, Jiaxing, Yuxuan Richard Xie, Kamil M. Ciesielski, Claire E. Porter, Eric S. Toberer, and Elif Ertekin. 2023. <span>“Leveraging <span>Language</span> <span>Representation</span> for <span>Material</span> <span>Recommendation</span>, <span>Ranking</span>, and <span>Exploration</span>.”</span> <em>Arxiv Preprint arXiv: 2305.01101</em>, May. <a href="https://doi.org/10.48550/arXiv.2305.01101">https://doi.org/10.48550/arXiv.2305.01101</a>.
</div>
<div id="ref-raffel2020exploring" class="csl-entry" role="listitem">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. <span>“<span class="nocase">Exploring the limits of transfer learning with a unified text-to-text transformer</span>.”</span> <em>Journal of Machine Learning Research</em> 21 (140): 1–67. <a href="https://www.jmlr.org/papers/v21/20-074.html">https://www.jmlr.org/papers/v21/20-074.html</a>.
</div>
<div id="ref-RajabiKochi2025adaptive" class="csl-entry" role="listitem">
Rajabi-Kochi, Mahyar, Negareh Mahboubi, Aseem Partap Singh Gill, and Seyed Mohamad Moosavi. 2025. <span>“Adaptive Representation of Molecules and Materials in Bayesian Optimization.”</span> <em>Chemical Science</em> 16 (13): 5464–74. <a href="https://doi.org/10.1039/d5sc00200a">https://doi.org/10.1039/d5sc00200a</a>.
</div>
<div id="ref-rame2023rewarded" class="csl-entry" role="listitem">
Ramé, Alexandre, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, and Matthieu Cord. 2023. <span>“Rewarded Soups: Towards <span>Pareto</span>-Optimal Alignment by Interpolating Weights Fine-Tuned on Diverse Rewards.”</span> <em>Arxiv Preprint arXiv:2306.04488</em>, October. <a href="https://doi.org/10.48550/arXiv.2306.04488">https://doi.org/10.48550/arXiv.2306.04488</a>.
</div>
<div id="ref-ramos2023bayesian" class="csl-entry" role="listitem">
Ramos, Mayk Caldas, Shane S. Michtavy, Marc D. Porosoff, and Andrew D. White. 2023. <span>“Bayesian Optimization of Catalysis with in-Context Learning.”</span> <em>arXiv Preprint arXiv: 2304.05341</em>. <a href="https://doi.org/10.48550/arXiv.2304.05341">https://doi.org/10.48550/arXiv.2304.05341</a>.
</div>
<div id="ref-rankovic2023bochemian" class="csl-entry" role="listitem">
Ranković, Bojana, and Philippe Schwaller. 2023. <span>“BoChemian: Large Language Model Embeddings for Bayesian Optimization of Chemical Reactions.”</span> <em>NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World</em>. <a href="https://openreview.net/forum?id=A1RVn1m3J3">https://openreview.net/forum?id=A1RVn1m3J3</a>.
</div>
<div id="ref-rankovic2025gollum0" class="csl-entry" role="listitem">
———. 2025. <span>“GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning Through Bayesian Optimization.”</span> <em>arXiv Preprint arXiv: 2504.06265</em>. <a href="https://doi.org/10.48550/arXiv.2504.06265">https://doi.org/10.48550/arXiv.2504.06265</a>.
</div>
<div id="ref-rauschen2024universal" class="csl-entry" role="listitem">
Rauschen, Robert, Mason Guy, Jason E. Hein, and Leroy Cronin. 2024. <span>“Universal Chemical Programming Language for Robotic Synthesis Repeatability.”</span> <em>Nature Synthesis</em> 3 (4). <a href="https://doi.org/10.1038/s44160-023-00473-6">https://doi.org/10.1038/s44160-023-00473-6</a>.
</div>
<div id="ref-renze2024self0reflection" class="csl-entry" role="listitem">
Renze, Matthew, and Erhan Guven. 2024. <span>“Self-Reflection in LLM Agents: Effects on Problem-Solving Performance.”</span> <em>arXiv Preprint arXiv: 2405.06682</em>. <a href="https://doi.org/10.48550/arXiv.2405.06682">https://doi.org/10.48550/arXiv.2405.06682</a>.
</div>
<div id="ref-riebesell2025framework" class="csl-entry" role="listitem">
Riebesell, Janosh, Rhys E. A. Goodall, Philipp Benner, Yuan Chiang, Bowen Deng, Gerbrand Ceder, Mark Asta, Alpha A. Lee, Anubhav Jain, and Kristin A. Persson. 2025. <span>“A Framework to Evaluate Machine Learning Crystal Stability Predictions.”</span> <em>Nature Machine Intelligence</em>. <a href="https://doi.org/10.1038/s42256-025-01055-1">https://doi.org/10.1038/s42256-025-01055-1</a>.
</div>
<div id="ref-rios2025llm" class="csl-entry" role="listitem">
Rı́os-Garcı́a, Martiño, and Kevin Maik Jablonka. 2025. <span>“<span>LLM</span>-as-Judge Meets <span>LLM</span>-as-Optimizer: Enhancing Organic Data Extraction Evaluations Through Dual <span>LLM</span> Approaches.”</span> <em>AI for Accelerated Materials Design - ICLR</em>. <a href="https://openreview.net/forum?id=MjQml5U1Xq">https://openreview.net/forum?id=MjQml5U1Xq</a>.
</div>
<div id="ref-rock2018hypothesis" class="csl-entry" role="listitem">
Rock, Charles. 2018. <span>“A Hypothesis Can’t Be Right Unless It Can Be Proven Wrong.”</span> <a href="https://www.stjude.org/research/progress/2018/hypothesis-must-be-falsifiable.html">https://www.stjude.org/research/progress/2018/hypothesis-must-be-falsifiable.html</a>.
</div>
<div id="ref-Lamas2024DSLXpert" class="csl-entry" role="listitem">
Sardiña, Víctor Juan Lamas, Daniel García-González, and Miguel Rodríguez Luaces. 2024. <span>“DSL-Xpert: LLM-Driven Generic DSL Code Generation.”</span> <em>Proceedings of the 27th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS Companion ’24)</em>, September, 5 pages. <a href="https://doi.org/10.1145/3652620.3687782">https://doi.org/10.1145/3652620.3687782</a>.
</div>
<div id="ref-schilling2024using" class="csl-entry" role="listitem">
Schilling-Wilhelmi, Mara, and Kevin Maik Jablonka. 2024. <span>“Using Machine-Learning and Large-Language-Model Extracted Data to Predict Copolymerizations.”</span> <em>AI for Accelerated Materials Design</em>. <a href="https://openreview.net/forum?id=zlutCyZ12H">https://openreview.net/forum?id=zlutCyZ12H</a>.
</div>
<div id="ref-schilling2025text" class="csl-entry" role="listitem">
Schilling-Wilhelmi, Mara, Martiño Rı́os-Garcı́a, Sherjeel Shabih, Marı́a Victoria Gil, Santiago Miret, Christoph T Koch, José A Márquez, and Kevin Maik Jablonka. 2025. <span>“<span class="nocase">From text to insight: large language models for chemical data extraction</span>.”</span> <em>Chemical Society Reviews</em>. <a href="https://doi.org/10.1039/d4cs00913d">https://doi.org/10.1039/d4cs00913d</a>.
</div>
<div id="ref-schmidgall2025agentrxiv" class="csl-entry" role="listitem">
Schmidgall, Samuel, and Michael Moor. 2025. <span>“AgentRxiv: Towards Collaborative Autonomous Research.”</span> <em>arXiv Preprint arXiv: 2503.18102</em>. <a href="https://doi.org/10.48550/arXiv.2503.18102">https://doi.org/10.48550/arXiv.2503.18102</a>.
</div>
<div id="ref-schmidgall2025agent" class="csl-entry" role="listitem">
Schmidgall, Samuel, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Michael Moor, Zicheng Liu, and Emad Barsoum. 2025. <span>“Agent Laboratory: Using LLM Agents as Research Assistants.”</span> <em>arXiv Preprint arXiv: 2501.04227</em>. <a href="https://doi.org/10.48550/arXiv.2501.04227">https://doi.org/10.48550/arXiv.2501.04227</a>.
</div>
<div id="ref-schwaller2021mapping" class="csl-entry" role="listitem">
Schwaller, Philippe, Daniel Probst, Alain C. Vaucher, Vishnu H. Nair, David Kreutter, Teodoro Laino, and Jean-Louis Reymond. 2021. <span>“Mapping the Space of Chemical Reactions Using Attention-Based Neural Networks.”</span> <em>Nature Machine Intelligence</em> 3 (2): 144–52. <a href="https://doi.org/10.1038/s42256-020-00284-w">https://doi.org/10.1038/s42256-020-00284-w</a>.
</div>
<div id="ref-segler2017towards" class="csl-entry" role="listitem">
Segler, Marwin, Mike Preuß, and Mark P Waller. 2017. <span>“Towards" Alphachem": Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies.”</span> <em>arXiv Preprint arXiv:1702.00020</em>. <a href="https://doi.org/10.48550/arXiv.1702.00020">https://doi.org/10.48550/arXiv.1702.00020</a>.
</div>
<div id="ref-Seifrid2022SDL" class="csl-entry" role="listitem">
Seifrid, Martin, Robert Pollice, Andrés Aguilar-Granda, Zamyla Morgan Chan, Kazuhiro Hotta, Cher Tian Ser, Jenya Vestfrid, Tony C. Wu, and Alán Aspuru-Guzik. 2022. <span>“Autonomous Chemical Experiments: Challenges and Perspectives on Establishing a Self-Driving Lab.”</span> <em>Accounts of Chemical Research</em> 55 (17): 2454–66. <a href="https://doi.org/10.1021/acs.accounts.2c00220">https://doi.org/10.1021/acs.accounts.2c00220</a>.
</div>
<div id="ref-selivanov2023medical" class="csl-entry" role="listitem">
Selivanov, Alexander, Oleg Y Rogov, Daniil Chesakov, Artem Shelmanov, Irina Fedulova, and Dmitry V Dylov. 2023. <span>“<span class="nocase">Medical image captioning via generative pretrained transformers</span>.”</span> <em>Scientific Reports</em> 13 (1): 4171. <a href="https://doi.org/10.1038/s41598-023-31223-5">https://doi.org/10.1038/s41598-023-31223-5</a>.
</div>
<div id="ref-shabih2025automated" class="csl-entry" role="listitem">
Shabih, Sherjeel, Christoph T Koch, Kevin Maik Jablonka, and José A. Márquez. 2025. <span>“Automated Data Extraction from Solar Cell Literature Using Large Language Models.”</span> <em>AI for Accelerated Materials Design - ICLR</em>. <a href="https://openreview.net/forum?id=gwLX7cdESk">https://openreview.net/forum?id=gwLX7cdESk</a>.
</div>
<div id="ref-Shields2021bayesian" class="csl-entry" role="listitem">
Shields, Benjamin J., Jason Stevens, Jun Li, Marvin Parasram, Farhan Damani, Jesus I. Martinez Alvarado, Jacob M. Janey, Ryan P. Adams, and Abigail G. Doyle. 2021. <span>“Bayesian Reaction Optimization as a Tool for Chemical Synthesis.”</span> <em>Nature</em> 590 (7844): 89–96. <a href="https://doi.org/10.1038/s41586-021-03213-y">https://doi.org/10.1038/s41586-021-03213-y</a>.
</div>
<div id="ref-shoghi2023molecules" class="csl-entry" role="listitem">
Shoghi, Nima, Adeesh Kolluru, John R. Kitchin, Zachary W. Ulissi, C. L. Zitnick, and Brandon M. Wood. 2023. <span>“From Molecules to Materials: Pre-Training Large Generalizable Models for Atomic Property Prediction.”</span> <em>International Conference on Learning Representations</em>. <a href="https://doi.org/10.48550/arXiv.2310.16802">https://doi.org/10.48550/arXiv.2310.16802</a>.
</div>
<div id="ref-si2025ideation1execution" class="csl-entry" role="listitem">
Si, Chenglei, Tatsunori Hashimoto, and Diyi Yang. 2025. <span>“The Ideation-Execution Gap: Execution Outcomes of LLM-Generated Versus Human Research Ideas.”</span> <em>arXiv Preprint arXiv: 2506.20803</em>. <a href="https://doi.org/10.48550/arXiv.2506.20803">https://doi.org/10.48550/arXiv.2506.20803</a>.
</div>
<div id="ref-si2025llms" class="csl-entry" role="listitem">
Si, Chenglei, Diyi Yang, and Tatsunori Hashimoto. 2025. <span>“Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers.”</span> <em>International Conference on Learning Representations</em>. <a href="https://doi.org/10.48550/arXiv.2409.04109">https://doi.org/10.48550/arXiv.2409.04109</a>.
</div>
<div id="ref-singh2024figura11y" class="csl-entry" role="listitem">
Singh, Nikhil, Lucy Lu Wang, and Jonathan Bragg. 2024. <span>“<span class="nocase">Figura11y: Ai assistance for writing scientific alt text</span>.”</span> <em>Proceedings of the 29th International Conference on Intelligent User Interfaces</em>, 886–906. <a href="https://doi.org/10.1145/3640543.3645212">https://doi.org/10.1145/3640543.3645212</a>.
</div>
<div id="ref-skarlinski2024language" class="csl-entry" role="listitem">
Skarlinski, Michael D, Sam Cox, Jon M Laurent, James D Braza, Michaela Hinks, Michael J Hammerling, Manvitha Ponnapati, Samuel G Rodriques, and Andrew D White. 2024. <span>“Language Agents Achieve Superhuman Synthesis of Scientific Knowledge.”</span> <em>arXiv Preprint arXiv:2409.13740</em>. <a href="https://doi.org/10.48550/arXiv.2409.13740">https://doi.org/10.48550/arXiv.2409.13740</a>.
</div>
<div id="ref-soares2025mamba-based" class="csl-entry" role="listitem">
Soares, Eduardo, Emilio Vital Brazil, Victor Shirasuna, Dmitry Zubarev, Renato Cerqueira, and Kristin Schmidt. 2025. <span>“A Mamba-Based Foundation Model for Materials.”</span> <em>Npj Artificial Intelligence</em> 1 (1): 1–8. <a href="https://doi.org/10.1038/s44387-025-00009-7">https://doi.org/10.1038/s44387-025-00009-7</a>.
</div>
<div id="ref-son2025ai" class="csl-entry" role="listitem">
Son, Guijin, Jiwoo Hong, Honglu Fan, Heejeong Nam, Hyunwoo Ko, Seungwon Lim, Jinyeop Song, et al. 2025. <span>“When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research.”</span> <em>arXiv Preprint arXiv: 2505.11855</em>. <a href="https://doi.org/10.48550/arXiv.2505.11855">https://doi.org/10.48550/arXiv.2505.11855</a>.
</div>
<div id="ref-song2023llm" class="csl-entry" role="listitem">
Song, Chan Hee, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. 2023. <span>“Llm-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models.”</span> <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2998–3009. <a href="https://doi.org/10.1109/ICCV51070.2023.00280">https://doi.org/10.1109/ICCV51070.2023.00280</a>.
</div>
<div id="ref-srinivas2024crossing" class="csl-entry" role="listitem">
Srinivas, Sakhinana Sagar, and Venkataramana Runkana. 2024a. <span>“Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based de Novo Molecule Design.”</span> <em>arXiv Preprint arXiv: 2408.11866</em>. <a href="https://doi.org/10.48550/arXiv.2408.11866">https://doi.org/10.48550/arXiv.2408.11866</a>.
</div>
<div id="ref-srinivas2024crossmodal" class="csl-entry" role="listitem">
———. 2024b. <span>“Cross-<span>Modal</span> <span>Learning</span> for <span>Chemistry</span> <span>Property</span> <span>Prediction</span>: <span>Large</span> <span>Language</span> <span>Models</span> <span>Meet</span> <span>Graph</span> <span>Machine</span> <span>Learning</span>.”</span> <em>Arxiv Preprint arXiv: 2408.14964</em>, August. <a href="https://doi.org/10.48550/arXiv.2408.14964">https://doi.org/10.48550/arXiv.2408.14964</a>.
</div>
<div id="ref-sriram2024flowllm" class="csl-entry" role="listitem">
Sriram, Anuroop, Benjamin Kurt Miller, Ricky T. Q. Chen, and Brandon M. Wood. 2024. <span>“<span>FlowLLM</span>: <span>Flow</span> <span>Matching</span> for <span>Material</span> <span>Generation</span> with <span>Large</span> <span>Language</span> <span>Models</span> as <span>Base</span> <span>Distributions</span>.”</span> <em>Arxiv Preprint arXiv</em>, October. <a href="https://doi.org/10.48550/arXiv.2410.23405">https://doi.org/10.48550/arXiv.2410.23405</a>.
</div>
<div id="ref-stanley2015greatness" class="csl-entry" role="listitem">
Stanley, Kenneth O., and Joel Lehman. 2015. <em>Why Greatness Cannot Be Planned: The Myth of the Objective</em>. Cham, Switzerland: Springer. <a href="https://doi.org/10.1007/978-3-319-15524-1">https://doi.org/10.1007/978-3-319-15524-1</a>.
</div>
<div id="ref-stanley2017openendedness" class="csl-entry" role="listitem">
Stanley, Kenneth O., Joel Lehman, and Lisa Soros. 2017. <span>“Open-Endedness: The Last Grand Challenge You’ve Never Heard Of.”</span> <a href="https://www.oreilly.com/radar/open-endedness-the-last-grand-challenge-youve-never-heard-of/">https://www.oreilly.com/radar/open-endedness-the-last-grand-challenge-youve-never-heard-of/</a>.
</div>
<div id="ref-starace2025paperbench0" class="csl-entry" role="listitem">
Starace, Giulio, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, et al. 2025. <span>“PaperBench: Evaluating AI’s Ability to Replicate AI Research.”</span> <em>arXiv Preprint arXiv: 2504.01848</em>. <a href="https://doi.org/10.48550/arXiv.2504.01848">https://doi.org/10.48550/arXiv.2504.01848</a>.
</div>
<div id="ref-stechly2024chain" class="csl-entry" role="listitem">
Stechly, Kaya, Karthik Valmeekam, and Subbarao Kambhampati. 2024. <span>“Chain of Thoughtlessness? An Analysis of Cot in Planning.”</span> <em>The Thirty-Eighth Annual Conference on Neural Information Processing Systems</em>. <a href="https://doi.org/10.48550/arXiv.2405.04776">https://doi.org/10.48550/arXiv.2405.04776</a>.
</div>
<div id="ref-steiner2019organic" class="csl-entry" role="listitem">
Steiner, Sebastian, Jakob Wolf, Stefan Glatzel, Anna Andreou, Jarosław M. Granda, Graham Keenan, Trevor Hinkley, et al. 2019. <span>“Organic Synthesis in a Modular Robotic System Driven by a Chemical Programming Language.”</span> <em>Science</em> 363 (6423): eaav2211. <a href="https://doi.org/10.1126/science.aav2211">https://doi.org/10.1126/science.aav2211</a>.
</div>
<div id="ref-autoprotocol2023" class="csl-entry" role="listitem">
Strateos. 2023. <span>“Autoprotocol Specification.”</span> <a href="https://autoprotocol.org/specification/">https://autoprotocol.org/specification/</a>.
</div>
<div id="ref-strieth-kalthoff2024delocalized" class="csl-entry" role="listitem">
Strieth-Kalthoff, Felix, Han Hao, Vandana Rathore, Joshua Derasp, Théophile Gaudin, Nicholas H. Angello, Martin Seifrid, et al. 2024. <span>“Delocalized, Asynchronous, Closed-Loop Discovery of Organic Laser Emitters.”</span> <em>Science</em> 384 (6697): eadk9227. <a href="https://doi.org/10.1126/science.adk9227">https://doi.org/10.1126/science.adk9227</a>.
</div>
<div id="ref-sun2025synllama" class="csl-entry" role="listitem">
Sun, Kunyang, Dorian Bagni, Joseph M. Cavanagh, Yingze Wang, Jacob M. Sawyer, Andrew Gritsevskiy, Oufan Zhang, and Teresa Head-Gordon. 2025. <span>“<span>SynLlama</span>: <span>Generating</span> <span>Synthesizable</span> <span>Molecules</span> and <span>Their</span> <span>Analogs</span> with <span>Large</span> <span>Language</span> <span>Models</span>.”</span> <em>Arxiv Preprint arXiv: 2503.12602</em>, April. <a href="https://doi.org/10.48550/arXiv.2503.12602">https://doi.org/10.48550/arXiv.2503.12602</a>.
</div>
<div id="ref-sypetkowski2024scalability" class="csl-entry" role="listitem">
Sypetkowski, Maciej, Frederik Wenkel, Farimah Poursafaei, Nia Dickson, Karush Suri, Philip Fradkin, and Dominique Beaini. 2024. <span>“On the Scalability of Gnns for Molecular Graphs.”</span> <em>Advances in Neural Information Processing Systems</em> 37: 19870–906. <a href="https://doi.org/10.48550/arXiv.2404.11568">https://doi.org/10.48550/arXiv.2404.11568</a>.
</div>
<div id="ref-Taylor2023brief" class="csl-entry" role="listitem">
Taylor, Connor J., Alexander Pomberger, Kobi C. Felton, Rachel Grainger, Magda Barecka, Thomas W. Chamberlain, Richard A. Bourne, Christopher N. Johnson, and Alexei A. Lapkin. 2023. <span>“A Brief Introduction to Chemical Reaction Optimization.”</span> <em>Chemical Reviews</em> 123 (6): 3089–3126. <a href="https://doi.org/10.1021/acs.chemrev.2c00798">https://doi.org/10.1021/acs.chemrev.2c00798</a>.
</div>
<div id="ref-danish_gov2024hypothesis" class="csl-entry" role="listitem">
The Danish National Committee on Health Research Ethics. 2024. <span>“Hypothesis-Generating Research.”</span> <a href="https://researchethics.dk/guidelines/-guidance-on-hypothesis-generating-research">https://researchethics.dk/guidelines/-guidance-on-hypothesis-generating-research</a>.
</div>
<div id="ref-tian2024scicode" class="csl-entry" role="listitem">
Tian, Minyang, Luyu Gao, Shizhuo Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, et al. 2024. <span>“Scicode: A Research Coding Benchmark Curated by Scientists.”</span> <em>Advances in Neural Information Processing Systems</em> 37: 30624–50. <a href="https://doi.org/10.48550/arXiv.2407.13168">https://doi.org/10.48550/arXiv.2407.13168</a>.
</div>
<div id="ref-Tom2024SDL" class="csl-entry" role="listitem">
Tom, Gary, Stefan P. Schmid, Sterling G. Baird, Yang Cao, Kourosh Darvish, Han Hao, Stanley Lo, et al. 2024. <span>“Self-Driving Laboratories for Chemistry and Materials Science.”</span> <em>Chemical Reviews</em> 124 (16): 9633–732. <a href="https://doi.org/10.1021/acs.chemrev.4c00055">https://doi.org/10.1021/acs.chemrev.4c00055</a>.
</div>
<div id="ref-trewartha2022quantifying" class="csl-entry" role="listitem">
Trewartha, Amalie, Nicholas Walker, Haoyan Huo, Sanghoon Lee, Kevin Cruse, John Dagdelen, Alexander Dunn, Kristin A Persson, Gerbrand Ceder, and Anubhav Jain. 2022. <span>“Quantifying the Advantage of Domain-Specific Pre-Training on Named Entity Recognition Tasks in Materials Science.”</span> <em>Patterns</em> 3 (4).
</div>
<div id="ref-tu2025askcos" class="csl-entry" role="listitem">
Tu, Zhengkai, Sourabh J Choure, Mun Hong Fong, Jihye Roh, Itai Levin, Kevin Yu, Joonyoung F Joung, et al. 2025. <span>“<span class="nocase">ASKCOS: an open source software suite for synthesis planning</span>.”</span> <em>arXiv Preprint arXiv:2501.01835</em>. <a href="https://doi.org/10.48550/arXiv.2501.01835">https://doi.org/10.48550/arXiv.2501.01835</a>.
</div>
<div id="ref-vanherck2025assessment" class="csl-entry" role="listitem">
Van Herck, Joren, Marı́a Victoria Gil, Kevin Maik Jablonka, Alex Abrudan, Andy S. Anker, Mehrdad Asgari, Ben Blaiszik, et al. 2025. <span>“<span class="nocase">Assessment of fine-tuned large language models for real-world chemistry and material science applications</span>.”</span> <em>Chemical Science</em> 16 (2): 670–84. <a href="https://doi.org/10.1039/D4SC04401K">https://doi.org/10.1039/D4SC04401K</a>.
</div>
<div id="ref-vangala2024suitability" class="csl-entry" role="listitem">
Vangala, Sarveswara Rao, Sowmya Ramaswamy Krishnan, Navneet Bung, Dhandapani Nandagopal, Gomathi Ramasamy, Satyam Kumar, Sridharan Sankaran, Rajgopal Srinivasan, and Arijit Roy. 2024. <span>“Suitability of Large Language Models for Extraction of High-Quality Chemical Reaction Dataset from Patent Literature.”</span> <em>Journal of Cheminformatics</em> 16 (1): 131. <a href="https://doi.org/10.1186/s13321-024-00928-8">https://doi.org/10.1186/s13321-024-00928-8</a>.
</div>
<div id="ref-Vaucher2020AutoExtraction" class="csl-entry" role="listitem">
Vaucher, Alain C., Federico Zipoli, Joppe Geluykens, Vishnu H. Nair, Philippe Schwaller, Teodoro Laino, et al. 2020. <span>“Automated Extraction of Chemical Synthesis Actions from Experimental Procedures.”</span> <em>Nature Communications</em> 11 (1). <a href="https://doi.org/10.1038/s41467-020-17266-6">https://doi.org/10.1038/s41467-020-17266-6</a>.
</div>
<div id="ref-vriza2023polybot" class="csl-entry" role="listitem">
Vriza, Aikaterini, Henry C. Chan, Jie Xu, Keith L. Barnett, Ian Staffell, Oleksandr Stanevich, Siqi Du, et al. 2023. <span>“Self-Driving Laboratory for Polymer Electronics.”</span> <em>Chemistry of Materials</em> 35 (8): 3046–56. <a href="https://doi.org/10.1021/acs.chemmater.2c03593">https://doi.org/10.1021/acs.chemmater.2c03593</a>.
</div>
<div id="ref-wan2024tokens" class="csl-entry" role="listitem">
Wan, Yuwei, Tong Xie, Nan Wu, Wenjie Zhang, Chunyu Kit, and Bram Hoex. 2024. <span>“From Tokens to Materials: Leveraging Language Models for Scientific Discovery.”</span> <em>arXiv Preprint arXiv: 2410.16165</em>. <a href="https://doi.org/10.48550/arXiv.2410.16165">https://doi.org/10.48550/arXiv.2410.16165</a>.
</div>
<div id="ref-Wang_2021" class="csl-entry" role="listitem">
Wang, Anthony Yu-Tung, Steven K. Kauwe, Ryan J. Murdock, and Taylor D. Sparks. 2021. <span>“<span class="nocase">Compositionally restricted attention-based network for materials property predictions</span>.”</span> <em>Npj Computational Materials</em> 7 (1). <a href="https://doi.org/10.1038/s41524-021-00545-1">https://doi.org/10.1038/s41524-021-00545-1</a>.
</div>
<div id="ref-wang2025polybot" class="csl-entry" role="listitem">
Wang, Chengshi, Yeon-Ju Kim, Aikaterini Vriza, Rohit Batra, Arun Baskaran, Naisong Shan, Nan Li, et al. 2025. <span>“Autonomous Platform for Solution Processing of Electronic Polymers.”</span> <em>Nature Communications</em> 16 (1): 1498. <a href="https://doi.org/10.1038/s41467-024-55655-3">https://doi.org/10.1038/s41467-024-55655-3</a>.
</div>
<div id="ref-wang2024planning" class="csl-entry" role="listitem">
Wang, Evan, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang. 2024. <span>“Planning in Natural Language Improves Llm Search for Code Generation.”</span> <em>arXiv Preprint arXiv:2409.03733</em>. <a href="https://doi.org/10.48550/arXiv.2409.03733">https://doi.org/10.48550/arXiv.2409.03733</a>.
</div>
<div id="ref-wang2025llm0augmented" class="csl-entry" role="listitem">
Wang, Haorui, Jeff Guo, Lingkai Kong, Rampi Ramprasad, Philippe Schwaller, Yuanqi Du, and Chao Zhang. 2025. <span>“LLM-Augmented Chemical Synthesis and Design Decision Programs.”</span> <em>arXiv Preprint arXiv: 2505.07027</em>. <a href="https://doi.org/10.48550/arXiv.2505.07027">https://doi.org/10.48550/arXiv.2505.07027</a>.
</div>
<div id="ref-wang2024efficient" class="csl-entry" role="listitem">
Wang, Haorui, Marta Skreta, Cher Tian Ser, Wenhao Gao, Lingkai Kong, Felix Strieth-Kalthoff, Chenru Duan, et al. 2025. <span>“Efficient Evolutionary Search over Chemical Space with Large Language Models.”</span> <em>The Thirteenth International Conference on Learning Representations, <span>ICLR</span> 2025, Singapore, April 24-28, 2025</em>. <a href="https://doi.org/10.48550/arXiv.2406.16976">https://doi.org/10.48550/arXiv.2406.16976</a>.
</div>
<div id="ref-wang2023scimon0" class="csl-entry" role="listitem">
Wang, Qingyun, Doug Downey, Heng Ji, and Tom Hope. 2023. <span>“SciMON: Scientific Inspiration Machines Optimized for Novelty.”</span> <em>arXiv Preprint arXiv: 2305.14259</em>. <a href="https://doi.org/10.48550/arXiv.2305.14259">https://doi.org/10.48550/arXiv.2305.14259</a>.
</div>
<div id="ref-wang2022ulsa" class="csl-entry" role="listitem">
Wang, Zhenbin, Kevin Cruse, Yifei Fei, Aaron Chia, Yihuang Zeng, Haozhe Huo, Tianxiao He, Bowen Deng, Olga Kononova, and Gerbrand Ceder. 2022. <span>“<span>ULSA</span>: Unified Language of Synthesis Actions for the Representation of Inorganic Synthesis Protocols.”</span> <em>Digital Discovery</em> 1 (3): 313–24. <a href="https://doi.org/10.1039/D2DD00049D">https://doi.org/10.1039/D2DD00049D</a>.
</div>
<div id="ref-warr2014short" class="csl-entry" role="listitem">
Warr, Wendy A. 2014. <span>“<span class="nocase">A short review of chemical reaction database systems, computer-aided synthesis design, reaction prediction and synthetic feasibility</span>.”</span> <em>Molecular Informatics</em> 33 (6-7): 469–76. <a href="https://doi.org/10.1002/minf.201400052">https://doi.org/10.1002/minf.201400052</a>.
</div>
<div id="ref-weininger1988smiles" class="csl-entry" role="listitem">
Weininger, David. 1988. <span>“<span>SMILES</span>, a Chemical Language and Information System. 1. <span>Introduction</span> to Methodology and Encoding Rules.”</span> <em>Journal of Chemical Information and Computer Sciences</em> 28 (1). <a href="https://doi.org/10.1021/ci00057a005">https://doi.org/10.1021/ci00057a005</a>.
</div>
<div id="ref-wellawatte2025human" class="csl-entry" role="listitem">
Wellawatte, Geemi P, and Philippe Schwaller. 2025. <span>“<span class="nocase">Human interpretable structure-property relationships in chemistry using explainable machine learning and large language models</span>.”</span> <em>Communications Chemistry</em> 8 (1): 11. <a href="https://doi.org/10.1038/s42004-024-01393-y">https://doi.org/10.1038/s42004-024-01393-y</a>.
</div>
<div id="ref-wellawatte2022model" class="csl-entry" role="listitem">
Wellawatte, Geemi P, Aditi Seshadri, and Andrew D White. 2022. <span>“Model Agnostic Generation of Counterfactual Explanations for Molecules.”</span> <em>Chemical Science</em> 13 (13): 3697–3705. <a href="https://doi.org/10.1039/d1sc05259d">https://doi.org/10.1039/d1sc05259d</a>.
</div>
<div id="ref-pylabrobot" class="csl-entry" role="listitem">
Wierenga, Rick P., Stefan M. Golas, Wilson Ho, Connor W. Coley, and Kevin M. Esvelt. 2023. <span>“PyLabRobot: An Open-Source, Hardware-Agnostic Interface for Liquid-Handling Robots and Accessories.”</span> <em>Device</em> 1 (4): 100111. <a href="https://doi.org/10.1016/j.device.2023.100111">https://doi.org/10.1016/j.device.2023.100111</a>.
</div>
<div id="ref-wilbraham2021chemPU" class="csl-entry" role="listitem">
Wilbraham, Liam, S. Hessam M. Mehr, and Leroy Cronin. 2021. <span>“Digitizing Chemistry Using the Chemical Processing Unit: From Synthesis to Discovery.”</span> <em>Accounts of Chemical Research</em> 54 (2): 253–62. <a href="https://doi.org/10.1021/acs.accounts.0c00674">https://doi.org/10.1021/acs.accounts.0c00674</a>.
</div>
<div id="ref-wu2025large" class="csl-entry" role="listitem">
Wu, Tongwei, Yao Sun, Xiaoxi Guo, Lin Tian, Yanning Zhang, Haitao Zhao, and Yuen Wu. 2025. <span>“A Large Language Models-Guided Grand Canonical DFT Framework for Accelerating the Discovery of Efficient Electrocatalysts.”</span>
</div>
<div id="ref-wu2018moleculenet" class="csl-entry" role="listitem">
Wu, Zhenqin, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay Pande. 2018. <span>“<span class="nocase">MoleculeNet: a benchmark for molecular machine learning</span>.”</span> <em>Chemical Science</em> 9 (2): 513–30. <a href="https://doi.org/10.1039/c7sc02664a">https://doi.org/10.1039/c7sc02664a</a>.
</div>
<div id="ref-xie2025darwin" class="csl-entry" role="listitem">
Xie, Tong, Yuwei Wan, Yixuan Liu, Yuchen Zeng, Shaozhou Wang, Wenjie Zhang, Clara Grazian, et al. 2025. <span>“<span>DARWIN</span> 1.5: <span>Large</span> <span>Language</span> <span>Models</span> as <span>Materials</span> <span>Science</span> <span>Adapted</span> <span>Learners</span>.”</span> <em>Arxvi Preprint arXiv:2412.11970</em>, January. <a href="https://doi.org/10.48550/arXiv.2412.11970">https://doi.org/10.48550/arXiv.2412.11970</a>.
</div>
<div id="ref-yamada2025ai" class="csl-entry" role="listitem">
Yamada, Yutaro, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. 2025. <span>“The AI Scientist-V2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search.”</span> <em>arXiv Preprint arXiv: 2504.08066</em>. <a href="https://doi.org/10.48550/arXiv.2504.08066">https://doi.org/10.48550/arXiv.2504.08066</a>.
</div>
<div id="ref-Yan2020auto" class="csl-entry" role="listitem">
Yan, Cong, and Yeye He. 2020. <span>“Auto-Suggest: Learning-to-Recommend Data Preparation Steps Using Data Science Notebooks.”</span> <em>Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data</em>, SIGMOD/PODS ’20, May. <a href="https://doi.org/10.1145/3318464.3389738">https://doi.org/10.1145/3318464.3389738</a>.
</div>
<div id="ref-yang2023large" class="csl-entry" role="listitem">
Yang, Chengrun, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. 2023. <span>“Large Language Models as Optimizers.”</span> <em>arXiv Preprint arXiv: 2309.03409</em>. <a href="https://doi.org/10.48550/arXiv.2309.03409">https://doi.org/10.48550/arXiv.2309.03409</a>.
</div>
<div id="ref-yang2025moose0chem20" class="csl-entry" role="listitem">
Yang, Zonglin, Wanhao Liu, Ben Gao, Yujie Liu, Wei Li, Tong Xie, Lidong Bing, Wanli Ouyang, Erik Cambria, and Dongzhan Zhou. 2025. <span>“MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search.”</span> <em>arXiv Preprint arXiv: 2505.19209</em>. <a href="https://doi.org/10.48550/arXiv.2505.19209">https://doi.org/10.48550/arXiv.2505.19209</a>.
</div>
<div id="ref-yang2025moose" class="csl-entry" role="listitem">
Yang, Zonglin, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, and Dongzhan Zhou. 2025. <span>“MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses.”</span> <em>The Thirteenth International Conference on Learning Representations, <span>ICLR</span></em>. <a href="https://doi.org/10.48550/arXiv.2410.07076">https://doi.org/10.48550/arXiv.2410.07076</a>.
</div>
<div id="ref-yoshikai2024novel" class="csl-entry" role="listitem">
Yoshikai, Yasuhiro, Tadahaya Mizuno, Shumpei Nemoto, and Hiroyuki Kusuhara. 2024. <span>“A Novel Molecule Generative Model of VAE Combined with Transformer for Unseen Structure Generation.”</span> <em>arXiv Preprint arXiv: 2402.11950</em>. <a href="https://doi.org/10.48550/arXiv.2402.11950">https://doi.org/10.48550/arXiv.2402.11950</a>.
</div>
<div id="ref-Yoshikawa2023CLAIRify" class="csl-entry" role="listitem">
Yoshikawa, Naruki, Marta Skreta, Kourosh Darvish, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bjørn Kristensen, Andrew Zou Li, et al. 2023. <span>“Large Language Models for Chemistry Robotics.”</span> <em>Autonomous Robots</em> 47 (8): 1057–86. <a href="https://doi.org/10.1007/s10514-023-10136-2">https://doi.org/10.1007/s10514-023-10136-2</a>.
</div>
<div id="ref-yu2024llasmol" class="csl-entry" role="listitem">
Yu, Botao, Frazier N. Baker, Ziqi Chen, Xia Ning, and Huan Sun. 2024. <span>“<span class="nocase">LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset</span>.”</span> <em>arXiv Preprint arXiv: 2402.09391</em>. <a href="https://doi.org/10.48550/arXiv.2402.09391">https://doi.org/10.48550/arXiv.2402.09391</a>.
</div>
<div id="ref-yu2025collaborative" class="csl-entry" role="listitem">
Yu, Jiajun, Yizhen Zheng, Huan Yee Koh, Shirui Pan, Tianyue Wang, and Haishuai Wang. 2025. <span>“Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2503.03503">https://doi.org/10.48550/arXiv.2503.03503</a>.
</div>
<div id="ref-zhang2024chemllm" class="csl-entry" role="listitem">
Zhang, Di, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, et al. 2024. <span>“<span class="nocase">Chemllm: A chemical large language model</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2402.06852">https://doi.org/10.48550/arXiv.2402.06852</a>.
</div>
<div id="ref-zhang2025darwin" class="csl-entry" role="listitem">
Zhang, Jenny, Shengran Hu, Cong Lu, Robert Lange, and Jeff Clune. 2025. <span>“Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2505.22954">https://doi.org/10.48550/arXiv.2505.22954</a>.
</div>
<div id="ref-zhang2024omni0" class="csl-entry" role="listitem">
Zhang, Jenny, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. 2024. <span>“OMNI: Open-Endedness via Models of Human Notions of Interestingness.”</span> <em>International Conference on Learning Representations</em>. <a href="https://doi.org/10.48550/arXiv.2306.01711">https://doi.org/10.48550/arXiv.2306.01711</a>.
</div>
<div id="ref-zhang2024fine" class="csl-entry" role="listitem">
Zhang, Wei, Qinggong Wang, Xiangtai Kong, Jiacheng Xiong, Shengkun Ni, Duanhua Cao, Buying Niu, et al. 2024. <span>“Fine-Tuning Large Language Models for Chemical Text Mining.”</span> <em>Chemical Science</em> 15 (27): 10600–10611. <a href="https://doi.org/10.1039/D4SC00924J">https://doi.org/10.1039/D4SC00924J</a>.
</div>
<div id="ref-zhang2025large" class="csl-entry" role="listitem">
Zhang, Yu, Yang Han, Shuai Chen, Ruijie Yu, Xin Zhao, Xianbin Liu, Kaipeng Zeng, et al. 2025. <span>“Large Language Models to Accelerate Organic Chemistry Synthesis.”</span> <em>Nature Machine Intelligence</em>. <a href="https://doi.org/10.1038/s42256-025-01066-y">https://doi.org/10.1038/s42256-025-01066-y</a>.
</div>
<div id="ref-zhao2024chemdfm" class="csl-entry" role="listitem">
Zhao, Zihan, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Yi Xia, Bo Chen, et al. 2024. <span>“ChemDFM: A Large Language Foundation Model for Chemistry.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2401.14818">https://doi.org/10.48550/arXiv.2401.14818</a>.
</div>
<div id="ref-zheng2025large" class="csl-entry" role="listitem">
Zheng, Yizhen, Huan Yee Koh, Jiaxin Ju, Anh T. N. Nguyen, Lauren T. May, Geoffrey I. Webb, and Shirui Pan. 2025. <span>“<span class="nocase">Large language models for scientific discovery in molecular property prediction</span>.”</span> <em>Nature Machine Intelligence</em> 7 (3): 437–47. <a href="https://doi.org/10.1038/s42256-025-00994-z">https://doi.org/10.1038/s42256-025-00994-z</a>.
</div>
<div id="ref-Zheng2024image" class="csl-entry" role="listitem">
Zheng, Zhiling, Zhiguo He, Omar Khattab, Nakul Rampal, Matei A. Zaharia, Christian Borgs, Jennifer T. Chayes, and Omar M. Yaghi. 2024. <span>“Image and Data Mining in Reticular Chemistry Powered by GPT-4V.”</span> <em>Digital Discovery</em> 3 (3): 491–501. <a href="https://doi.org/10.1039/d3dd00239j">https://doi.org/10.1039/d3dd00239j</a>.
</div>
<div id="ref-zheng2023chatgpt" class="csl-entry" role="listitem">
Zheng, Zhiling, Oufan Zhang, C. Borgs, J. Chayes, and O. Yaghi. 2023. <span>“ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis.”</span> <em>Journal of the American Chemical Society</em>. <a href="https://doi.org/10.1021/jacs.3c05819">https://doi.org/10.1021/jacs.3c05819</a>.
</div>
<div id="ref-zhou2025tempest0" class="csl-entry" role="listitem">
Zhou, Andy, and Ron Arel. 2025. <span>“Tempest: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2503.10619">https://doi.org/10.48550/arXiv.2503.10619</a>.
</div>
<div id="ref-zhou2024one-preference-fits-all" class="csl-entry" role="listitem">
Zhou, Zhanhui, Jie Liu, Jing Shao, Xiangyu Yue, Chao Yang, Wanli Ouyang, and Yu Qiao. 2024. <span>“Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization.”</span> <em>Arxiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2310.03708">https://doi.org/10.48550/arXiv.2310.03708</a>.
</div>
<div id="ref-zhu20243m-diffusion" class="csl-entry" role="listitem">
Zhu, Huaisheng, Teng Xiao, and Vasant G. Honavar. 2024. <span>“<span>3M</span>-<span>Diffusion</span>: <span>Latent</span> <span>Multi</span>-<span>Modal</span> <span>Diffusion</span> for <span>Language</span>-<span>Guided</span> <span>Molecular</span> <span>Structure</span> <span>Generation</span>.”</span> <em>Arxiv Preprint</em>, October. <a href="https://doi.org/10.48550/arXiv.2403.07179">https://doi.org/10.48550/arXiv.2403.07179</a>.
</div>
<div id="ref-Zou2025ElAgente" class="csl-entry" role="listitem">
Zou, Yunheng, Austin H. Cheng, Abdulrahman Aldossary, Jiaru Bai, Shi Xuan Leong, Jorge Arturo Campos-Gonzalez-Angulo, Changhyeok Choi, et al. 2025. <span>“El Agente: An Autonomous Agent for Quantum Chemistry.”</span> <em>Matter</em> 8 (7): 102263. <a href="https://doi.org/10.1016/j.matt.2025.102263">https://doi.org/10.1016/j.matt.2025.102263</a>.
</div>
<div id="ref-zunger2019beware" class="csl-entry" role="listitem">
Zunger, Alex. 2019. <span>“<span class="nocase">Beware of plausible predictions of fantasy materials</span>.”</span> <em>Nature</em> 566 (7745): 447–49. <a href="https://doi.org/10.1038/d41586-019-00676-y">https://doi.org/10.1038/d41586-019-00676-y</a>.
</div>
</div>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./04-evals.html" class="pagination-link" aria-label="Evaluations">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Evaluations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./06-safety.html" class="pagination-link" aria-label="Implications of GPMs: Education, Safety, and Ethics">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implications of GPMs: Education, Safety, and Ethics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>