<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Accelerating Applications – General Purpose Models for the Chemical Sciences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./07-safety.html" rel="next">
<link href="./05-applications.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-fffb2cfd06bc0bcd22fa5e79382abad9.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./06-accelerating_applications.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Accelerating Applications</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">General Purpose Models for the Chemical Sciences</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">General purpose models for the chemical sciences</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-data_taxonomy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Shape and Structure of Chemical Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Building Principles of GPMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-evals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evaluations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-accelerating_applications.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Accelerating Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-safety.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implications of GPMs: Education, Safety, and Ethics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-outlook_conclusions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Outlook and Conclusions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec:prediction" id="toc-sec:prediction" class="nav-link active" data-scroll-target="#sec\:prediction"><span class="header-section-number">6.1</span> Property Prediction</a>
  <ul>
  <li><a href="#prompting" id="toc-prompting" class="nav-link" data-scroll-target="#prompting"><span class="header-section-number">6.1.1</span> Prompting</a>
  <ul>
  <li><a href="#s-as-feature-extractors" id="toc-s-as-feature-extractors" class="nav-link" data-scroll-target="#s-as-feature-extractors"><span class="header-section-number">6.1.1.1</span> LLMs as Feature Extractors</a></li>
  </ul></li>
  <li><a href="#sec:prediction_FT" id="toc-sec:prediction_FT" class="nav-link" data-scroll-target="#sec\:prediction_FT"><span class="header-section-number">6.1.2</span> Fine-Tuning</a>
  <ul>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section"><span class="header-section-number">6.1.2.1</span> language-interfaced finetuning (LIFT)</a></li>
  <li><a href="#foundational-s-and-s" id="toc-foundational-s-and-s" class="nav-link" data-scroll-target="#foundational-s-and-s"><span class="header-section-number">6.1.2.2</span> Foundational GNNs and machine-learning interatomic potential (MLIP)s</a></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations"><span class="header-section-number">6.1.2.3</span> Limitations</a></li>
  </ul></li>
  <li><a href="#agents" id="toc-agents" class="nav-link" data-scroll-target="#agents"><span class="header-section-number">6.1.3</span> Agents</a></li>
  <li><a href="#sec:property_core_limits" id="toc-sec:property_core_limits" class="nav-link" data-scroll-target="#sec\:property_core_limits"><span class="header-section-number">6.1.4</span> Core Limitations</a></li>
  </ul></li>
  <li><a href="#sec:mol_generation" id="toc-sec:mol_generation" class="nav-link" data-scroll-target="#sec\:mol_generation"><span class="header-section-number">6.2</span> Molecular and Material Generation</a>
  <ul>
  <li><a href="#sec:generation" id="toc-sec:generation" class="nav-link" data-scroll-target="#sec\:generation"><span class="header-section-number">6.2.1</span> Generation</a>
  <ul>
  <li><a href="#prompting-1" id="toc-prompting-1" class="nav-link" data-scroll-target="#prompting-1"><span class="header-section-number">6.2.1.1</span> Prompting</a></li>
  <li><a href="#fine-tuning" id="toc-fine-tuning" class="nav-link" data-scroll-target="#fine-tuning"><span class="header-section-number">6.2.1.2</span> Fine-Tuning</a></li>
  <li><a href="#diffusion-and-flow-matching" id="toc-diffusion-and-flow-matching" class="nav-link" data-scroll-target="#diffusion-and-flow-matching"><span class="header-section-number">6.2.1.3</span> Diffusion and Flow Matching</a></li>
  <li><a href="#reinforcement-learning-and-preference-optimization" id="toc-reinforcement-learning-and-preference-optimization" class="nav-link" data-scroll-target="#reinforcement-learning-and-preference-optimization"><span class="header-section-number">6.2.1.4</span> Reinforcement Learning and Preference Optimization</a></li>
  <li><a href="#agents-1" id="toc-agents-1" class="nav-link" data-scroll-target="#agents-1"><span class="header-section-number">6.2.1.5</span> Agents</a></li>
  </ul></li>
  <li><a href="#validation" id="toc-validation" class="nav-link" data-scroll-target="#validation"><span class="header-section-number">6.2.2</span> Validation</a>
  <ul>
  <li><a href="#general-validation" id="toc-general-validation" class="nav-link" data-scroll-target="#general-validation"><span class="header-section-number">6.2.2.1</span> General validation</a></li>
  <li><a href="#conditional-generation-validation" id="toc-conditional-generation-validation" class="nav-link" data-scroll-target="#conditional-generation-validation"><span class="header-section-number">6.2.2.2</span> Conditional Generation Validation</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec:retrosynthesis" id="toc-sec:retrosynthesis" class="nav-link" data-scroll-target="#sec\:retrosynthesis"><span class="header-section-number">6.3</span> Retrosynthesis</a></li>
  <li><a href="#sec:llm-optimizers" id="toc-sec:llm-optimizers" class="nav-link" data-scroll-target="#sec\:llm-optimizers"><span class="header-section-number">6.4</span> LLMs as Optimizers</a>
  <ul>
  <li><a href="#llms-as-surrogate-models" id="toc-llms-as-surrogate-models" class="nav-link" data-scroll-target="#llms-as-surrogate-models"><span class="header-section-number">6.4.1</span> LLMs as Surrogate Models</a></li>
  <li><a href="#llms-as-next-candidate-generators" id="toc-llms-as-next-candidate-generators" class="nav-link" data-scroll-target="#llms-as-next-candidate-generators"><span class="header-section-number">6.4.2</span> LLMs as Next Candidate Generators</a></li>
  <li><a href="#sec:opt-llm-know-source" id="toc-sec:opt-llm-know-source" class="nav-link" data-scroll-target="#sec\:opt-llm-know-source"><span class="header-section-number">6.4.3</span> LLMs as Prior Knowledge Sources</a></li>
  <li><a href="#how-to-face-optimization-problems" id="toc-how-to-face-optimization-problems" class="nav-link" data-scroll-target="#how-to-face-optimization-problems"><span class="header-section-number">6.4.4</span> How to Face Optimization Problems?</a></li>
  </ul></li>
  </ul>
<div class="quarto-other-links"><h2>Other Links</h2><ul><li><a href="https://lamalab.org/"><i class="bi bi-globe"></i>Visit our website</a></li><li><a href="https://x.com/jablonkagroup"><i class="bi bi-twitter-x"></i>Follow us on X (Twitter)</a></li><li><a href="https://forms.fillout.com/t/eoGA7AhnAKus"><i class="bi bi-person-badge"></i>We are hiring!</a></li><li><a href="mailto:contact@lamalab.org"><i class="bi bi-mailbox"></i>Contact us</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Accelerating Applications</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>The application of accelerated approaches in the scientific discovery cycle (see <a href="#fig:applications" data-reference-type="ref+Label" data-reference="fig:applications">[fig:applications]</a>) hinges on their ability to streamline and enhance each stage of the process. However, a fundamental challenge in effectively implementing these approaches lies in the choice of machine-readable representation.</p>
<p>This challenge is particularly evident in the representation of molecules and materials, which must balance computational efficiency with the preservation of structural, compositional, and functional properties. Take, for example, the high-temperature superconductor YBa2Cu3O_7-x. While atomic positions and coordinates are theoretically sufficient to solve the Schrödinger equation and describe this material, such a representation may not provide the adaptability necessary for diverse tasks. What defines a good representation depends on the problem. [<span class="citation" data-cites="huang2016understanding">Huang and Lilienfeld (<a href="09-references.html#ref-huang2016understanding" role="doc-biblioref">2016</a>)</span>]. A representation designed to predict critical temperature must efficiently encode the relationship between oxygen stoichiometry and superconducting properties, emphasizing features like oxygen vacancy patterns and charge transfer mechanisms. Conversely, a representation for structural stability might prioritize different geometric or bonding characteristics.</p>
<p>This tension has led to three primary strategies for representing molecules and materials (read <a href="03-architectures.html#sec:common_representations" data-reference-type="ref+Label" data-reference="sec:common_representations">[sec:common_representations]</a> to learn in detail about the different representations that currently exist). First, domain-specific text-based formats—such as simplified molecular input line entry system (SMILES) [<span class="citation" data-cites="weininger1988smiles">Weininger (<a href="09-references.html#ref-weininger1988smiles" role="doc-biblioref">1988</a>)</span>], self-referencing embedded strings (SELFIES) [<span class="citation" data-cites="krenn2020self">Krenn et al. (<a href="09-references.html#ref-krenn2020self" role="doc-biblioref">2020</a>)</span>], and crystallographic information file (CIF) [<span class="citation" data-cites="hall1991crystallographic">Hall, Allen, and Brown (<a href="09-references.html#ref-hall1991crystallographic" role="doc-biblioref">1991</a>)</span>]—offer compact, machine-readable encodings of structural information. While these necessarily omit certain physical details, their computational tractability has enabled breakthroughs, as demonstrated by <span class="citation" data-cites="jablonka2024leveraging">Jablonka et al. (<a href="09-references.html#ref-jablonka2024leveraging" role="doc-biblioref">2024</a>)</span> in their large language model (LLM)-based generation of valid molecular and material structures.</p>
<p>Yet, the question remains: Which representation is optimal for a given task? Future advances in accelerated discovery will likely hinge on adaptive representations that dynamically balance these competing demands.</p>
<section id="sec:prediction" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="sec:prediction"><span class="header-section-number">6.1</span> Property Prediction</h2>
<p>general-purpose model (GPM)s have emerged as a powerful tool for predicting molecular and material properties, offering an alternative to traditional quantum mechanical calculations or specialized machine learning (ML) models. Current GPM-driven property prediction tasks span both classification and regression. Unlike conventional approaches that rely on task-specific architectures and extensively labeled data, GPMs have demonstrated strong generalization capabilities across diverse domains, efficiently adapting to various prediction tasks. Their success extends to multiple datasets, from standardized benchmarks such as <code>MoleculeNet</code> [<span class="citation" data-cites="wu2018moleculenet">Wu et al. (<a href="09-references.html#ref-wu2018moleculenet" role="doc-biblioref">2018</a>)</span>], to curated datasets targeting specific applications such as antibacterial activity [<span class="citation" data-cites="chithrananda2020chemberta">Chithrananda, Grand, and Ramsundar (<a href="09-references.html#ref-chithrananda2020chemberta" role="doc-biblioref">2020</a>)</span>] or photovoltaic efficiency[<span class="citation" data-cites="aneesh2025semantic">Aneesh et al. (<a href="09-references.html#ref-aneesh2025semantic" role="doc-biblioref">2025</a>)</span>].</p>
<p>Three key methodologies have been explored to adapt LLMs for property prediction: prompting techniques (see <a href="03-architectures.html#sec:prompting" data-reference-type="ref+Label" data-reference="sec:prompting">[sec:prompting]</a>), fine-tuning (see <a href="03-architectures.html#sec:fine-tuning" data-reference-type="ref+Label" data-reference="sec:fine-tuning">[sec:fine-tuning]</a>) on domain-specific data, and retrieval-augmented generation (RAG) (see <a href="03-architectures.html#sec:rag" data-reference-type="ref+Label" data-reference="sec:rag">[sec:rag]</a>) approaches that combine LLMs with external knowledge bases.</p>
<div class="minipage">
<p><strong>Key:</strong> P = prompting; FT = fine-tuned model; RAG = retrieval-augmented generation; C = Classification; R = Regression</p>
</div>
<section id="prompting" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="prompting"><span class="header-section-number">6.1.1</span> Prompting</h3>
<p>Prompt engineering involves designing targeted instructions to guide GPMs in performing specialized tasks without altering their underlying parameters by leveraging their embedded knowledge. In molecular and materials science, this strategy goes beyond simply asking a model to predict properties. It also includes carefully structured prompts to elicit detailed molecular and material descriptions directly from the model’s pre-trained knowledge.</p>
<p><span class="citation" data-cites="liu2025integrating">H. Liu et al. (<a href="09-references.html#ref-liu2025integrating" role="doc-biblioref">2025</a>)</span> conducted a comprehensive evaluation of different prompting techniques to predict the properties of organic small molecules and crystal materials. Some of these techniques included domain-knowledge (prior knowledge was embedded in the prompt), expert (role-play instructions), and few-shot chain-of-thought (CoT) (the text<em>“Let’s think step by step”</em> is added) prompting. Of these, domain knowledge achieved maximum performance. However, their evaluation was limited to a relatively small set of molecules and tasks, and the effectiveness of their domain-knowledge approach may not generalize to other molecular property domains.</p>
<p>Building on these foundational prompting strategies, few-shot prompting approaches leverage in-context learning (ICL) to enhance performance through selected examples <span class="citation" data-cites="liu2024moleculargpt">Y. Liu et al. (<a href="09-references.html#ref-liu2024moleculargpt" role="doc-biblioref">2024</a>)</span> used SMILES string representations of molecules with few-shot ICL, retrieving structurally similar molecules as demonstrations to enhance property prediction. This approach highlights how ICL can transfer knowledge from similar molecule examples without requiring model fine-tuning for each task. However, the effectiveness of ICL depends on the quality of retrieved examples.</p>
<p><span class="citation" data-cites="fifty2023incontext">Fifty, Leskovec, and Thrun (<a href="09-references.html#ref-fifty2023incontext" role="doc-biblioref">2023</a>)</span> moved beyond direct text prompting of molecules and introduced context-aware molecule prediction (CAMP): an ICL algorithm that uses a two-stage encoding approach without relying on pre-trained LLMs. First, a specialized message-passing neural network (MPNN) encodes molecule graphs into molecular embeddings rather than processing them as raw text. These embeddings are then fed into a transformer encoder, which learns contextualized representations across the support set (a small collection of labeled molecule-property pairs) and the unlabeled query molecules. They demonstrated CAMP’s ability to outperform existing few-shot learning baselines by providing relevant molecular examples within the prompt context. However, this approach is constrained by the context-length limitations of the underlying language model (LM)s and the challenge of selecting optimal demonstration examples.</p>
<p>More sophisticated approaches have leveraged prompting as part of multi-modal frameworks. The <code>LLM4SD</code> pipeline by <span class="citation" data-cites="zheng2025large">Zheng et al. (<a href="09-references.html#ref-zheng2025large" role="doc-biblioref">2025</a>)</span> employs specialized prompts to guide LMs through their pre-trained knowledge on scientific literature, generating known rules (e.g., molecules weighing under 500&nbsp;Da are more likely to pass the blood-brain barrier) that transform molecules into feature vectors (e.g.&nbsp;CCO could translate to a vector <span class="math inline">\([2,46.07,1,1]\)</span> where each number represents a feature of the molecule, in this example [# C, MW, # H-bond donors, # H-bond acceptors]) for use with a random forest model, which they consider “interpretable”. This approach outperformed specialized state-of-the-art (SOTA) models across <span class="math inline">\(58\)</span> benchmark tasks, while providing interpretable reasoning about prediction logic (see <a href="#tab:property_prediction_models" data-reference-type="ref+Label" data-reference="tab:property_prediction_models">[tab:property_prediction_models]</a> for properties predicted by this model). However, its reliance on rule extraction may limit its ability to capture complex, non-linear relationships that specialized deep learning models can identify.</p>
<section id="s-as-feature-extractors" class="level4" data-number="6.1.1.1">
<h4 data-number="6.1.1.1" class="anchored" data-anchor-id="s-as-feature-extractors"><span class="header-section-number">6.1.1.1</span> LLMs as Feature Extractors</h4>
<p>Another emerging application of LLMs is their use as “feature extractors”, where they generate textual or embedded representations of molecules or materials. For instance, in materials science, <span class="citation" data-cites="aneesh2025semantic">Aneesh et al. (<a href="09-references.html#ref-aneesh2025semantic" role="doc-biblioref">2025</a>)</span> employed LLMs to generate text embeddings of perovskite solar cell compositions. These embeddings were subsequently used to train a graph neural network (GNN) for predicting power conversion efficiency, demonstrating the potential of LLMs to enhance feature representation in materials informatics. Similarly, in the molecular domain, <span class="citation" data-cites="srinivas2024crossmodal">Srinivas and Runkana (<a href="09-references.html#ref-srinivas2024crossmodal" role="doc-biblioref">2024b</a>)</span> used zero-shot LLM prompting (see <a href="#box:%20cot_prompting" data-reference-type="ref+Label" data-reference="box: cot_prompting">[box: cot_prompting]</a> for prompt examples) to generate detailed textual descriptions of molecular functional groups, which are used to train a small LM. This LM is used to compute text-level embeddings of molecules. Simultaneously, they generate molecular graph-level embeddings from SMILES string molecular graph inputs. They finally integrate the graph and text-level embeddings to produce a semantically enriched embedding.</p>
<p><img src="media/eq_images/cot_prompting.png" alt="Promptbox: box: cot_prompting" width="100%"></p>
<p>In a different implementation of fine-tuning, <span class="citation" data-cites="balaji2023gptmolberta">Balaji et al. (<a href="09-references.html#ref-balaji2023gptmolberta" role="doc-biblioref">2023</a>)</span> used <code>ChatGPT</code> to generate text descriptions of molecules that were then used to train a <code>RoBERTa</code> (125M) model for property prediction, showing how LM-generated representations can access latent spaces that SMILES strings alone might not capture. Similarly, <span class="citation" data-cites="li2024unveiling">Z. Li et al. (<a href="09-references.html#ref-li2024unveiling" role="doc-biblioref">2024</a>)</span> introduced the <code>MoleX</code> framework, which fine-tunes <code>ChemBERTa-2</code>[<span class="citation" data-cites="ahmad2022chemberta">Ahmad et al. (<a href="09-references.html#ref-ahmad2022chemberta" role="doc-biblioref">2022</a>)</span>] on Group SELFIES [<span class="citation" data-cites="cheng2023group">Cheng et al. (<a href="09-references.html#ref-cheng2023group" role="doc-biblioref">2023</a>)</span>] (a functional group-based molecular representation) to then extract a single LLM-derived embedding of molecules that captures the chemical semantics at the functional group level. This allowed them to determine which functional groups or fragments contribute to molecular properties, which in turn can be converted into reliable explanations of said properties.</p>
</section>
</section>
<section id="sec:prediction_FT" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="sec:prediction_FT"><span class="header-section-number">6.1.2</span> Fine-Tuning</h3>
<figure id="fig:gptchem" class="figure">
<img src="media/figures/property_gptchem.png" alt="" width="100%" class="figure-img">
<figcaption>
<strong>Fine-tuned <code>GPT-3</code> for predicting solid-solution formation in high-entropy alloys</strong> Performance comparison of different ML approaches as a function of the number of training points. Results are shown for <code>Automatminer</code> (blue), <code>CrabNet</code> transformer (orange), fine-tuned <code>GPT-3</code> (red), with error bars showing standard error of the mean. The non-Google test set shows the fine-tuned <code>GPT-3</code> model tested on compounds without an exact Google search match (dark red). The dashed line shows performance using random forest. <code>GPT-3</code> achieves comparable accuracy to traditional approaches with significantly fewer training examples. Data adapted from <span class="citation" data-cites="jablonka2024leveraging">Jablonka et al. (<a href="09-references.html#ref-jablonka2024leveraging" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
<section id="section" class="level4" data-number="6.1.2.1">
<h4 data-number="6.1.2.1" class="anchored" data-anchor-id="section"><span class="header-section-number">6.1.2.1</span> language-interfaced finetuning (LIFT)</h4>
<p><span class="citation" data-cites="dinh2022lift">Dinh et al. (<a href="09-references.html#ref-dinh2022lift" role="doc-biblioref">2022</a>)</span> showed that reformulating regression and classification as questions &amp; answers (Q&amp;A) tasks enables the use of unmodified model architecture while improving performance (see <a href="03-architectures.html#sec:fine-tuning" data-reference-type="ref+Label" data-reference="sec:fine-tuning">[sec:fine-tuning]</a> for a deeper discussion of LIFT). In recognizing the scarcity of experimental data and acknowledging the persistence of this limitation, <span class="citation" data-cites="jablonka2024leveraging">Jablonka et al. (<a href="09-references.html#ref-jablonka2024leveraging" role="doc-biblioref">2024</a>)</span> designed a LIFT-based framework using <code>GPT-3</code> fine-tuned on task-specific small datasets (see <a href="#tab:property_prediction_models" data-reference-type="ref+Label" data-reference="tab:property_prediction_models">[tab:property_prediction_models]</a>). They seminally demonstrated that fine-tuned <code>GPT-3</code> can match or surpass specialized ML models in various chemistry tasks. A key finding was fine-tuned <code>GPT-3</code>’s ability to generalize beyond training data. When tested on compounds absent from Google Search (and likely its training data), it performed well, proving that it was not simply recalling memorized information (see <a href="#fig:gptchem" data-reference-type="ref+Label" data-reference="fig:gptchem">1</a>).</p>
<p>In a follow-up to <span class="citation" data-cites="jablonka2024leveraging">Jablonka et al. (<a href="09-references.html#ref-jablonka2024leveraging" role="doc-biblioref">2024</a>)</span>’s work, <span class="citation" data-cites="vanherck2025assessment">Van Herck et al. (<a href="09-references.html#ref-vanherck2025assessment" role="doc-biblioref">2025</a>)</span> systematically evaluated this approach across 22 diverse real-world chemistry case studies using three open-source models. They demonstrate that fine-tuned LLMs can effectively predict various material properties. For example, they achieved <span class="math inline">\(96\%\)</span> accuracy in predicting the adhesive free-energy of polymers, outperforming traditional ML methods like random forest (<span class="math inline">\(90\%\)</span> accuracy). When predicting properties of monomers using SMILES notation, the fine-tuned models reached average accuracies of <span class="math inline">\(84\%\)</span> across four different properties. Particularly notable was the ability of LLMs to work with non-standard inputs, like in a protein phase separation study they did, where raw protein sequences could be directly input without pre-processing and achieve <span class="math inline">\(95\%\)</span> prediction accuracy. At the same time, when training datasets were very small (15 data points), the predictive accuracy of all fine-tuned models was lower than the random baseline (e.g.&nbsp;MOF synthesis). These case studies preliminarily demonstrate that these models can achieve predictive performance with some small datasets, work with various chemical representations (SMILES, metal-organic framework (MOF)id, and International Union of Pure and Applied Chemistry (IUPAC) names), and can outperform traditional ML approaches for some material property prediction tasks.</p>
<p>In the materials domain, <code>LLMprop</code> fine-tunes <code>T5</code>[<span class="citation" data-cites="raffel2020exploring">Raffel et al. (<a href="09-references.html#ref-raffel2020exploring" role="doc-biblioref">2020</a>)</span>] to predict crystalline material properties from text descriptions generated by <code>Robocrystallographer</code>[<span class="citation" data-cites="ganose2019robocrystallographer">Ganose and Jain (<a href="09-references.html#ref-ganose2019robocrystallographer" role="doc-biblioref">2019</a>)</span>]. By discarding <code>T5</code>’s decoder and adding task-specific prediction heads, the approach reduces computational overhead while leveraging the model’s ability to process structured crystal descriptions. The method demonstrates that natural language representations can effectively capture key material features, offering an alternative to traditional graph-based models like GNNs.</p>
<p>Fine-tuning has been used to adapt selective state space model (SSM)s like Mamba (see <a href="03-architectures.html#sec:example_architectures" data-reference-type="ref+Label" data-reference="sec:example_architectures">[sec:example_architectures]</a>). By pre-training on 91 million molecules, the Mamba-based model <span class="math inline">\(\text{O}_{SMI}-{\text{SSM}-}336\textit{M}\)</span> outperformed transformer methods (<code>Yield-BERT</code>[<span class="citation" data-cites="krzyzanowski2025exploring">Krzyzanowski, Pickett, and Pogány (<a href="09-references.html#ref-krzyzanowski2025exploring" role="doc-biblioref">2025</a>)</span>]) in reaction yield prediction (e.g., Buchwald-Hartwig cross-coupling) and achieved competitive results in molecular property prediction benchmarks.[<span class="citation" data-cites="soares2025mamba-based">Soares et al. (<a href="09-references.html#ref-soares2025mamba-based" role="doc-biblioref">2025</a>)</span>]</p>
</section>
<section id="foundational-s-and-s" class="level4" data-number="6.1.2.2">
<h4 data-number="6.1.2.2" class="anchored" data-anchor-id="foundational-s-and-s"><span class="header-section-number">6.1.2.2</span> Foundational GNNs and machine-learning interatomic potential (MLIP)s</h4>
<p>The fine-tuning approach has been applied to “foundational GNNs” [<span class="citation" data-cites="sypetkowski2024scalability">Sypetkowski et al. (<a href="09-references.html#ref-sypetkowski2024scalability" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="shoghi2023molecules">Shoghi et al. (<a href="09-references.html#ref-shoghi2023molecules" role="doc-biblioref">2023</a>)</span>] and MLIPs, approaches distinct from GPMs. For example, [<span class="citation" data-cites="shoghi2023molecules">Shoghi et al. (<a href="09-references.html#ref-shoghi2023molecules" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="sypetkowski2024scalability">Sypetkowski et al. (<a href="09-references.html#ref-sypetkowski2024scalability" role="doc-biblioref">2024</a>)</span>] show SOTA performance on property prediction tasks. “Foundational” MLIPs pre-trained on large datasets encompassing many chemical elements can be fine-tuned for specific downstream tasks [<span class="citation" data-cites="batatia2022mace">Batatia et al. (<a href="09-references.html#ref-batatia2022mace" role="doc-biblioref">2022</a>)</span>], such as calculating sublimation enthalpies of molecular crystal polymorphs [<span class="citation" data-cites="kaur2025data">Kaur et al. (<a href="09-references.html#ref-kaur2025data" role="doc-biblioref">2025</a>)</span>].</p>
</section>
<section id="limitations" class="level4" data-number="6.1.2.3">
<h4 data-number="6.1.2.3" class="anchored" data-anchor-id="limitations"><span class="header-section-number">6.1.2.3</span> Limitations</h4>
<p>One central challenge is finding balance in datasets. In practical applications, researchers often have many more examples of poor-performing materials than optimal ones, resulting in unbalanced datasets that can diminish model performance. <span class="citation" data-cites="vanherck2025assessment">Van Herck et al. (<a href="09-references.html#ref-vanherck2025assessment" role="doc-biblioref">2025</a>)</span> point out that in the catalyzed cleavage reaction study, only <span class="math inline">\(3.8\%\)</span> of catalysts were labeled as “good”, forcing researchers to reduce their training set significantly to maintain balance. They also note that LLMs struggle with highly complex or noisy datasets, as seen in their study of catalytic isomerization, where even after hyperparameter optimization, the models failed to achieve meaningful predictive power due to the high noise in the experimental data and limited sample size. Finally, they note that although LLMs can work with different chemical representations, the choice of representation significantly impacts performance. For example, when predicting polymerization rates, models using SMILES notation significantly outperformed those using IUPAC names, indicating that representation selection remains an important consideration.</p>
<p>Fine-tuning effectively adapts LLMs to specialized chemistry tasks, but its dependence on static datasets hinders adaptability to new or evolving knowledge. RAG, whose fundamentals are described in detail in <a href="03-architectures.html#sec:rag" data-reference-type="ref+Label" data-reference="sec:rag">[sec:rag]</a>, overcomes these limitations by dynamically integrating external data sources, enabling more flexible and up-to-date reasoning.</p>
</section>
</section>
<section id="agents" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="agents"><span class="header-section-number">6.1.3</span> Agents</h3>
<p>Caldas Ramos et al.&nbsp;introduce <code>MAPI-LLM</code>, a framework that processes natural-language queries about material properties using an LLM to decide which of the available tools such as the Materials Project application programming interface (API), the Reaction-Network package, or Google Search to use to generate a response. [<span class="citation" data-cites="Jablonka2023">Jablonka et al. (<a href="09-references.html#ref-Jablonka2023" role="doc-biblioref">2023</a>)</span>] <code>MAPI-LLM</code> employs a reasoning and acting (ReAct) prompt (see <a href="03-architectures.html#sec:arch_agents" data-reference-type="ref+Label" data-reference="sec:arch_agents">[sec:arch_agents]</a> to read more about ReAct), to convert prompts such as <em>“Is <span class="math inline">\(Fe_2O_3\)</span> magnetic?”</em> or <em>“What is the band gap of Mg(Fe3O3)2?”</em> into queries for Materials Project API. The system processes multi-step prompts through logical reasoning, for example, when asked <em>“If Mn2FeO3 is not metallic, what is its band gap?”</em>, the LLM system creates a two-step workflow to first verify metallicity before retrieving the band gap.</p>
<p>Building on this foundation of agent-based materials querying, <span class="citation" data-cites="chiang2024llamp">Chiang et al. (<a href="09-references.html#ref-chiang2024llamp" role="doc-biblioref">2024</a>)</span> advanced the approach with <code>LLaMP</code>, a framework that employs “hierarchical” ReAct agents to interact with computational and experimental data. This “hierarchical” framework employs a supervisor-assistant agent architecture where a complex problem is broken down and tasks are delegated to domain-specific agents. <code>LLaMP</code> addresses the challenge of hallucinations more effectively than standard LLM approaches by grounding responses in retrieved materials databases, retrieving materials data (e.g., crystal structures, elastic tensors) while counteracting systematic LLM biases in property predictions. These biases include the tendency for LLMs to overestimate certain properties like bulk moduli and to exhibit errors in bandgap predictions based on compositional patterns learned during training rather than physical principles.</p>
</section>
<section id="sec:property_core_limits" class="level3" data-number="6.1.4">
<h3 data-number="6.1.4" class="anchored" data-anchor-id="sec:property_core_limits"><span class="header-section-number">6.1.4</span> Core Limitations</h3>
<figure id="fig:property_limitations" class="figure">
<img src="media/figures/property_mattext.png" alt="" width="100%" class="figure-img">
<figcaption>
<strong>Normalized error distributions for materials property prediction models across different architectures</strong>. Each point represents the normalized error of a model on a specific property prediction task. Normalization was achieved with min/max values of each dataset to produce a range of errors between 0 and 1. The first column (blue) shows GNN based models, the second column (red) displays LLM approaches, and the third column (orange) represents other baseline methods and <span data-acronym-label="sota" data-acronym-form="singular+short">sota</span> models including <code>CrabNet</code>. <span class="citation" data-cites="Wang_2021">(<a href="09-references.html#ref-Wang_2021" role="doc-biblioref">A. Y.-T. Wang et al. 2021</a>)</span> Lower values indicate better predictive performance. Data adapted from <span class="citation" data-cites="alampara2024mattext">Alampara, Miret, and Jablonka (<a href="09-references.html#ref-alampara2024mattext" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
<p><span class="citation" data-cites="alampara2024mattext">Alampara, Miret, and Jablonka (<a href="09-references.html#ref-alampara2024mattext" role="doc-biblioref">2024</a>)</span> introduced <code>MatText</code>, a framework for evaluating LMs ability to predict properties of materials using text-based representations. Their findings indicate that current LLMs (including pre-trained <code>BERT</code> and fine-tuned <code>LLaMA-3-8B</code>) are effective for tasks relying purely on compositional information (e.g., element types and local bonding patterns), but struggle to leverage geometric or positional information encoded in text, as reflected in <a href="#fig:property_limitations" data-reference-type="ref+Label" data-reference="fig:property_limitations">2</a>. This observation suggests that transformer-based architectures may be fundamentally limited to applications where spatial understanding is not required. Their experiments with data scaling and text representations reveal that increasing pre-training data or adding geometric details fails to improve downstream property prediction, challenging the conventional assumption that larger models and datasets universally enhance performance. [<span class="citation" data-cites="frey2023neural">Frey et al. (<a href="09-references.html#ref-frey2023neural" role="doc-biblioref">2023</a>)</span>] Notably, <span class="citation" data-cites="frey2023neural">Frey et al. (<a href="09-references.html#ref-frey2023neural" role="doc-biblioref">2023</a>)</span> demonstrated power-law scaling in chemical LLMs, but <code>MatText</code>’s results imply that such scaling may not overcome architectural biases against geometric reasoning in materials tasks.[<span class="citation" data-cites="gruver2024promises">Gruver et al. (<a href="09-references.html#ref-gruver2024promises" role="doc-biblioref">2024</a>)</span>]</p>
</section>
</section>
<section id="sec:mol_generation" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="sec:mol_generation"><span class="header-section-number">6.2</span> Molecular and Material Generation</h2>
<figure id="fig:generation" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure21.png" alt="" width="100%" class="figure-img">
<figcaption>
<strong>Pipeline for molecular and materials generation</strong> The workflow begins with input structures represented in various formats, which are used to train <span data-acronym-label="ml" data-acronym-form="singular+short">ml</span> models to generate novel molecular and material structures. The generated structures should undergo a feedback loop through validation processes before being applied in the real world. Blue boxes indicate well-established areas of the pipeline with mature methodologies, while the red box represents critical bottlenecks.
</figcaption>
</figure>
<p>Early work in molecular and materials generation relied heavily on unconditional generation, where models produce novel structures without explicit guidance, relying solely on patterns learned from training data. For example, latent space sampling in autoencoders, where random vectors are decoded into new structures.[<span class="citation" data-cites="yoshikai2024novel">Yoshikai et al. (<a href="09-references.html#ref-yoshikai2024novel" role="doc-biblioref">2024</a>)</span>] These methods excel at exploring chemical space broadly but lack fine-grained control. This limitation underscores the need for conditional generation, using explicit prompts or constraints (e.g., property targets, structural fragments), to steer GPMs toward meaningful molecule or material designs. Beyond the generation step, as <a href="#fig:generation" data-reference-type="ref+Label" data-reference="fig:generation">3</a> shows, critical bottlenecks persist in synthesizability and physical consistency at the validation stage.</p>
<section id="sec:generation" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="sec:generation"><span class="header-section-number">6.2.1</span> Generation</h3>
<section id="prompting-1" class="level4" data-number="6.2.1.1">
<h4 data-number="6.2.1.1" class="anchored" data-anchor-id="prompting-1"><span class="header-section-number">6.2.1.1</span> Prompting</h4>
<p>While zero-shot and few-shot prompting strategies demonstrate promising flexibility for molecule generation, benchmark studies [<span class="citation" data-cites="guo2023large">Guo et al. (<a href="09-references.html#ref-guo2023large" role="doc-biblioref">2023</a>)</span>] reveal significant limitations that restrict their practical utility. <span class="citation" data-cites="guo2023large">Guo et al. (<a href="09-references.html#ref-guo2023large" role="doc-biblioref">2023</a>)</span> exposed fundamental gaps in LLMs’ molecular design capabilities through a systematic evaluation. <code>GPT-4</code> was reported to produce chemically valid SMILES <span class="math inline">\(89\%\)</span> of the time but achieving less than <span class="math inline">\(20\%\)</span> accuracy in matching the target specifications. This result is far below specialized models like <code>MolT5</code>[<span class="citation" data-cites="edwards2022translation">Edwards et al. (<a href="09-references.html#ref-edwards2022translation" role="doc-biblioref">2022</a>)</span>]. They conclude that this performance gap stems from LLMs’ inadequate understanding of SMILES syntax and structure-property relationships. Subsequent work by <span class="citation" data-cites="bhattacharya2024large">Bhattacharya et al. (<a href="09-references.html#ref-bhattacharya2024large" role="doc-biblioref">2024</a>)</span> explored whether systematic prompt engineering could overcome these limitations, demonstrating that these prompts could guide <code>Claude 3 Opus</code> to generate chemically valid molecules (<span class="math inline">\(97\%\)</span> syntactic validity) with controlled modifications, including fine-grained structural changes (median Tanimoto similarity <span class="math inline">\(0.67\)</span>–<span class="math inline">\(0.69\)</span>) and predictable electronic property shifts (0.14&nbsp;eV–0.27&nbsp;eV highest occupied molecular orbital (HOMO) energy changes). Hybrid approaches like <code>FrontierX</code> extend this method with knowledge-augmented prompting, where LLMs generate both molecule predictions and explanations that are used to fine-tune smaller LMs, with all resulting embeddings ultimately combined via hierarchical attention mechanisms to produce the final SMILES representation[<span class="citation" data-cites="srinivas2024crossing">Srinivas and Runkana (<a href="09-references.html#ref-srinivas2024crossing" role="doc-biblioref">2024a</a>)</span>]. It showed improved accuracy over pure prompting strategies but sacrificed the generalizability that makes LLMs attractive, as the model requires re-training for each new molecular domain.</p>
</section>
<section id="fine-tuning" class="level4" data-number="6.2.1.2">
<h4 data-number="6.2.1.2" class="anchored" data-anchor-id="fine-tuning"><span class="header-section-number">6.2.1.2</span> Fine-Tuning</h4>
<p>To overcome the limitations of prompting, fine-tuning has been adopted in molecular and materials generation, much like its use in property prediction with LIFT-based frameworks (see <a href="03-architectures.html#sec:fine-tuning" data-reference-type="ref+Label" data-reference="sec:fine-tuning">[sec:fine-tuning]</a> for a deeper explanation of LIFT and <a href="#sec:prediction_FT" data-reference-type="ref+Label" data-reference="sec:prediction_FT">1.1.2</a> for a discussion of LIFT applied to property prediction tasks). <span class="citation" data-cites="yu2024llasmol">B. Yu et al. (<a href="09-references.html#ref-yu2024llasmol" role="doc-biblioref">2024</a>)</span> demonstrated that systematic fine-tuning in various chemical tasks including molecule generation from captions can improve performance while remaining parameter-efficient, using only <span class="math inline">\(0.58\%\)</span> of trainable parameters via low-rank adaptation (LoRA).</p>
<p>The molecule-caption translation task (<code>Mol2Cap</code>), which involves generating textual descriptions from molecular representations and vice versa (Cap2Mol), has become a standard benchmark for evaluating GPMs for molecule generation. [<span class="citation" data-cites="edwards2022translation">Edwards et al. (<a href="09-references.html#ref-edwards2022translation" role="doc-biblioref">2022</a>)</span>] Under the “Mol2Cap”/“Cap2Mol” task paradigm, in-context molecule adaptation (ICMA) avoids domain-specific pre-training by combining retrieval-augmented in-context learning with fine-tuning on ICL examples.[<span class="citation" data-cites="li2025large">J. Li et al. (<a href="09-references.html#ref-li2025large" role="doc-biblioref">2025</a>)</span>] On the ChEBI-20[<span class="citation" data-cites="edwards2021text2mol">Edwards, Zhai, and Ji (<a href="09-references.html#ref-edwards2021text2mol" role="doc-biblioref">2021</a>)</span>] and PubChem324k[<span class="citation" data-cites="liu2023molca">Z. Liu et al. (<a href="09-references.html#ref-liu2023molca" role="doc-biblioref">2023</a>)</span>] datasets, ICMA nearly doubles baseline performance, with ICMA powered by<code>Mistral-7B</code> achieving a 0.581 bilingual evaluation understudy (BLEU) score in <code>Mol2Cap</code> and <span class="math inline">\(46.0\%\)</span> exact match in <code>Cap2Mol</code>.[<span class="citation" data-cites="li2025large">J. Li et al. (<a href="09-references.html#ref-li2025large" role="doc-biblioref">2025</a>)</span>] However, its reliance on retrieved examples raises concerns about generalization to novel scaffolds. Similarly, <code>MolReFlect</code> enhances fine-grained alignment through a teacher-student framework, where a larger LLM (e.g., <code>GPT-4</code>) extracts substructure-aware captions to guide a smaller model (<code>Mistral-7B</code>), improving <code>Cap2Mol</code> accuracy while reducing hallucinations.[<span class="citation" data-cites="li2024molreflect">J. Li et al. (<a href="09-references.html#ref-li2024molreflect" role="doc-biblioref">2024</a>)</span>] Meanwhile, <code>PEIT-LLM</code> extends the task to property-conditioned generation, using instructions (SMILES-text-property tuples) to optimize for captioning and prediction jointly.[<span class="citation" data-cites="lin2025property">Lin et al. (<a href="09-references.html#ref-lin2025property" role="doc-biblioref">2025</a>)</span>]</p>
<p>Fine-tuned LMs have shown promise in molecule and materials generation. However, their reliance on decoding and SMILES/SELFIES representations introduces fundamental limitations: degeneracy (multiple valid SMILES for the same molecule) and difficulty capturing complex structural relationships implicit in textual descriptions.</p>
</section>
<section id="diffusion-and-flow-matching" class="level4" data-number="6.2.1.3">
<h4 data-number="6.2.1.3" class="anchored" data-anchor-id="diffusion-and-flow-matching"><span class="header-section-number">6.2.1.3</span> Diffusion and Flow Matching</h4>
<p>Diffusion and flow-based models operate directly on latent representations, enabling more flexible generation of diverse and novel structures.[<span class="citation" data-cites="zhu20243m-diffusion">Zhu, Xiao, and Honavar (<a href="09-references.html#ref-zhu20243m-diffusion" role="doc-biblioref">2024</a>)</span>] Moreover, emerging hybrid architectures combine the strengths of LLMs with diffusion and flow matching models to overcome the limitations of each paradigm individually [<span class="citation" data-cites="sriram2024flowllm">Sriram et al. (<a href="09-references.html#ref-sriram2024flowllm" role="doc-biblioref">2024</a>)</span>].</p>
<p>Beyond text-based representations, <code>llamole</code> introduced a multimodal LLM approach capable of text and graph generation by integrating a base LLM with graph diffusion transformers and graph neural networks for multi-conditional molecular generation and retrosynthetic planning. Specifically they used different trigger (<code>&lt;design&gt;</code> and <code>&lt;retro&gt;</code>) and query (<code>&lt;query&gt;</code>) tokens for switching between them and improved success in synthesis success rates from <span class="math inline">\(5\%\)</span> to <span class="math inline">\(35\%\)</span> . [<span class="citation" data-cites="liu2024multimodal">G. Liu et al. (<a href="09-references.html#ref-liu2024multimodal" role="doc-biblioref">2024</a>)</span>]</p>
<p>A unique challenge with crystalline materials is generating a material that possesses both discrete (atom type) and continuous (atomic position and lattice geometry) variables. <span class="citation" data-cites="sriram2024flowllm">Sriram et al. (<a href="09-references.html#ref-sriram2024flowllm" role="doc-biblioref">2024</a>)</span> developed <code>FlowLLM</code> to address this challenge. They recognized that the respective strengths of LLMs, modeling discrete values and conditional prompting, and denoising models, modeling continuous values and equivariances, could be combined to create a hybrid architecture. A fine-tuned LLM is used to learn an effective base distribution of metastable crystals via text-based representations, which is then iteratively refined through Riemannian flow-matching (RFM) to optimize atomic coordinates and lattice parameters.[<span class="citation" data-cites="sriram2024flowllm">Sriram et al. (<a href="09-references.html#ref-sriram2024flowllm" role="doc-biblioref">2024</a>)</span>]</p>
</section>
<section id="reinforcement-learning-and-preference-optimization" class="level4" data-number="6.2.1.4">
<h4 data-number="6.2.1.4" class="anchored" data-anchor-id="reinforcement-learning-and-preference-optimization"><span class="header-section-number">6.2.1.4</span> Reinforcement Learning and Preference Optimization</h4>
<p>Translating GPM generated outputs to the real world requires designing molecules and materials with specific target properties. reinforcement learning (RL) and preference optimization techniques[<span class="citation" data-cites="lee2024fine-tuning">D. Lee and Cho (<a href="09-references.html#ref-lee2024fine-tuning" role="doc-biblioref">2024</a>)</span>] have emerged as powerful solutions for this challenge. For instance, <span class="citation" data-cites="jang2025can">Jang et al. (<a href="09-references.html#ref-jang2025can" role="doc-biblioref">2025</a>)</span> combined supervised fine-tuning (SFT) and RL using proximal policy optimization (PPO) to generate diverse molecular sequences auto-regressively. This approach excels in exploring a broad chemical space, but incurs high computational costs due to its reliance on iterative, sequence-based generation. In contrast, <span class="citation" data-cites="cavanagh2024smileyllama">Cavanagh et al. (<a href="09-references.html#ref-cavanagh2024smileyllama" role="doc-biblioref">2024</a>)</span> employed direct preference optimization (DPO) with SFT to fine-tune LLMs for molecular design, leveraging SMILES representations to optimize drug-like properties (e.g., hydrogen bond donors/acceptors and LogP). While DPO reduces computational overhead in comparison to PPO, it trades off molecular diversity, a key strength of the work by <span class="citation" data-cites="jang2025can">Jang et al. (<a href="09-references.html#ref-jang2025can" role="doc-biblioref">2025</a>)</span>, due to the inherent constraints of preference-based fine-tuning.</p>
<p>Beyond these methods, energy ranking alignment (ERA) introduces a different optimization paradigm. [<span class="citation" data-cites="chennakesavalu2025aligning">Chennakesavalu et al. (<a href="09-references.html#ref-chennakesavalu2025aligning" role="doc-biblioref">2025</a>)</span>] Unlike PPO or DPO, ERA uses gradient-based objectives to guide word-by-word generation with explicit reward functions, converging to a physics-inspired probability distribution that allows fine control over the generation process. In single-property optimization tasks, ERA successfully aligned molecular transformers to generate compounds with targeted chemical properties (QED, LogP, ring count, molar refractivity) while maintaining <span class="math inline">\(59-84\%\)</span> chemical validity without regularization. For multi-objective optimization, it achieved precise control over property trade-offs using weighted energy functions.</p>
<p><span class="citation" data-cites="calanzone2025mol-moe">Calanzone, D’Oro, and Bacon (<a href="09-references.html#ref-calanzone2025mol-moe" role="doc-biblioref">2025</a>)</span> also address the challenge of multi-objective molecular generation with <code>MOL-MOE</code>, a mixture of experts (MoE) framework (see <a href="03-architectures.html#sec:arch-moes" data-reference-type="ref+Label" data-reference="sec:arch-moes">[sec:arch-moes]</a> to learn more about MoE architectures). <code>MOL-MOE</code> dynamically combines property-specific expert models at test time using preference-guided routers toward drug-relevant molecular properties enabling flexible steering across multiple objectives without re-training. Compared to alternatives like <code>MORLHF</code>[<span class="citation" data-cites="zhou2024one-preference-fits-all">Zhou et al. (<a href="09-references.html#ref-zhou2024one-preference-fits-all" role="doc-biblioref">2024</a>)</span>], SFT with rewards-in-context, and simple model merging such as Rewarded Soups[<span class="citation" data-cites="rame2023rewarded">Ramé et al. (<a href="09-references.html#ref-rame2023rewarded" role="doc-biblioref">2023</a>)</span>]), <code>MOL-MOE</code> achieves superior performance in both property optimization and steerability—particularly in out-of-distribution scenarios where other methods struggle.</p>
<p><code>CrystalFormer-RL</code> uses RL fine-tuning to optimize <code>CrystalFormer</code>[<span class="citation" data-cites="cao2024space">Cao et al. (<a href="09-references.html#ref-cao2024space" role="doc-biblioref">2024</a>)</span>], a transformer-based crystal generator, with rewards from discriminative models (e.g., property predictors)[<span class="citation" data-cites="cao2025crystalformer-rl">Cao and Wang (<a href="09-references.html#ref-cao2025crystalformer-rl" role="doc-biblioref">2025</a>)</span>]. RL improves stability (lower energy above convex hull) and enables property-guided generation (e.g., high dielectric constant + band gap). Here, RL fine-tuning is shown to outperform supervised fine-tuning, enhancing both novel material discovery and retrieval of high-performing candidates from the pre-training dataset.</p>
</section>
<section id="agents-1" class="level4" data-number="6.2.1.5">
<h4 data-number="6.2.1.5" class="anchored" data-anchor-id="agents-1"><span class="header-section-number">6.2.1.5</span> Agents</h4>
<p>Agent-based frameworks leveraging LLMs, deeply explained in <a href="03-architectures.html#sec:agents" data-reference-type="ref+Label" data-reference="sec:agents">[sec:agents]</a>, have emerged as approaches for autonomous molecular and materials generation, demonstrating capabilities that extend beyond simple prompting or fine-tuning by incorporating iterative feedback loops, tool integration, and human-artificial intelligence (AI) collaboration. The <code>dZiner</code> framework implements this approach for the inverse design of materials, where agents input initial SMILES strings with optimization task descriptions and generate validated candidate molecules by retrieving domain knowledge from the literature.[<span class="citation" data-cites="ansari2024dziner">Ansari et al. (<a href="09-references.html#ref-ansari2024dziner" role="doc-biblioref">2024</a>)</span>] It also uses domain-expert surrogate models to evaluate the required property in the new molecule/material. These surrogate models are highly customizable to the desired property and give the user the option to train their own ML model or using an existing SOTA model. <span class="citation" data-cites="ansari2024dziner">Ansari et al. (<a href="09-references.html#ref-ansari2024dziner" role="doc-biblioref">2024</a>)</span> demonstrated <code>dZiner</code>’s capabilities in generating surfactants for critical micelle concentration reduction, WDR5 inhibitors, and optimizing MOF organic linkers for CO2 adsorption. The <code>CLADD</code> framework adopts a RAG-enhanced multi-agent approach where specialized teams including “Planning”, “Knowledge Graph”, and “Molecular Understanding” collaborate to dynamically retrieve and integrate external biochemical knowledge for drug discovery tasks without requiring domain-specific fine-tuning.[<span class="citation" data-cites="lee2025rag-enhanced">N. Lee et al. (<a href="09-references.html#ref-lee2025rag-enhanced" role="doc-biblioref">2025</a>)</span>]</p>
</section>
</section>
<section id="validation" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="validation"><span class="header-section-number">6.2.2</span> Validation</h3>
<section id="general-validation" class="level4" data-number="6.2.2.1">
<h4 data-number="6.2.2.1" class="anchored" data-anchor-id="general-validation"><span class="header-section-number">6.2.2.1</span> General validation</h4>
<p>The most fundamental validation approaches use cheminformatics tools like <code>RDKit</code> to verify molecular validity. <code>RDKit</code> provides robust tools for validating molecules through its ability to parse and sanitize molecules from SMILES strings. If a step in the SMILES to structure conversion process fails, then the molecule is considered invalid. More sophisticated validation involves quantum mechanical calculations to compute molecular properties such as formation energies[<span class="citation" data-cites="kingsbury2022flexible">Kingsbury et al. (<a href="09-references.html#ref-kingsbury2022flexible" role="doc-biblioref">2022</a>)</span>]. These computationally expensive operations provide deeper insights into whether generated structures are viable. Models are also evaluated for their ability to generate unique molecules by calculating the proportion of unique molecules in generated sets, often using molecular fingerprints or structural descriptors.</p>
<p>The gold standard for validation is experimental synthesis, but significant gaps exist between computational generation and laboratory realization. Preliminarily, metrics like Tanimoto similarity and Fréchet ChemNet distance [<span class="citation" data-cites="preuer2018frechet">Preuer et al. (<a href="09-references.html#ref-preuer2018frechet" role="doc-biblioref">2018</a>)</span>] quantify structural resemblance, which can indicate synthetic feasibility when training data consists of known compounds. Retrosynthesis prediction algorithms attempt to bridge this gap by evaluating synthetic accessibility and proposing potential synthesis routes (see <a href="#sec:retrosynthesis" data-reference-type="ref+Label" data-reference="sec:retrosynthesis">1.3</a>). However, these methods still face limitations in accurately predicting real-world synthesizability [<span class="citation" data-cites="zunger2019beware">Zunger (<a href="09-references.html#ref-zunger2019beware" role="doc-biblioref">2019</a>)</span>].</p>
</section>
<section id="conditional-generation-validation" class="level4" data-number="6.2.2.2">
<h4 data-number="6.2.2.2" class="anchored" data-anchor-id="conditional-generation-validation"><span class="header-section-number">6.2.2.2</span> Conditional Generation Validation</h4>
<p>Beyond establishing the general validity of generated molecules, evaluation methods can assess both their novelty relative to training data and their ability to meet specific design goals. For inverse design tasks, such as optimizing binding affinity or solubility, the <em>de novo</em> molecule generation benchmark GuacaMol differentiates between <em>distribution-learning</em> (e.g., generating diverse, valid molecules) and <em>goal-directed</em> optimization (e.g., rediscovering known drugs or meeting multi-objective constraints) [<span class="citation" data-cites="brown2019guacamol">Brown et al. (<a href="09-references.html#ref-brown2019guacamol" role="doc-biblioref">2019</a>)</span>]. In the materials paradigm, frameworks such as <code>MatBench Discovery</code> evaluate analogous challenges such as stability, electronic properties, and synthesizability, but adapt metrics to periodic systems, such as energy above hull or band gap prediction accuracy[<span class="citation" data-cites="riebesell2025framework">Riebesell et al. (<a href="09-references.html#ref-riebesell2025framework" role="doc-biblioref">2025</a>)</span>]. Recently, they introduced the “discovery acceleration factor”, which quantifies how effective a model is at finding stable structures relative to a random baseline.</p>
</section>
</section>
</section>
<section id="sec:retrosynthesis" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="sec:retrosynthesis"><span class="header-section-number">6.3</span> Retrosynthesis</h2>
<p>The practical utility of GPMs for generating molecules and materials remains limited by a persistent gap in their synthetic feasibility. Early work by <span class="citation" data-cites="schwaller2021mapping">Schwaller et al. (<a href="09-references.html#ref-schwaller2021mapping" role="doc-biblioref">2021</a>)</span> laid important groundwork by demonstrating how attention-based neural networks can learn meaningful representations of chemical reactions, enabling accurate classification and prediction of reaction outcomes. Their model, trained on millions of reactions from patent and literature data, showed that learned reaction embeddings were capable of capturing nuanced chemical relationships.</p>
<p>Recent efforts have built on this foundation by integrating synthesizability directly into molecular and materials generation pipelines that leverage both domain-specific tools and GPMs. For example, <span class="citation" data-cites="sun2025synllama">Sun et al. (<a href="09-references.html#ref-sun2025synllama" role="doc-biblioref">2025</a>)</span> adapted <code>Llama-3.1-8B</code> and <code>Llama-3.2-1B</code> to predict retrosynthetic pathways and identify commercially available building blocks for experimentally validated SARS-CoV-2 Mpro inhibitors. Similarly, <span class="citation" data-cites="liu2024multimodal">G. Liu et al. (<a href="09-references.html#ref-liu2024multimodal" role="doc-biblioref">2024</a>)</span> introduced a multimodal framework that combines reaction databases with chemical intuition encoded in LLMs, improving the prioritization of high-yield, low-cost synthetic routes.</p>
<p>More recent work has explored how fully fine-tuned LLMs can serve as comprehensive chemistry assistants for experimental guidance. <span class="citation" data-cites="zhang2025large">Zhang et al. (<a href="09-references.html#ref-zhang2025large" role="doc-biblioref">2025</a>)</span> developed <code>Chemma</code>, a fine-tuned <code>LLaMA-2-7B</code> model trained on 1.28 million chemical reaction question-answer pairs. Through an active learning framework that incorporates experimental feedback (see <a href="03-architectures.html#sec:rl" data-reference-type="ref+Label" data-reference="sec:rl">[sec:rl]</a> to learn more about RL), human-<code>Chemma</code> collaboration successfully optimized an unreported Suzuki-Miyaura cross-coupling reaction within only 15 experimental runs.</p>
<p>Predictive retrosynthesis has also extended to the inorganic domain. <span class="citation" data-cites="kim2024large">Kim, Jung, and Schrier (<a href="09-references.html#ref-kim2024large" role="doc-biblioref">2024</a>)</span> demonstrated that fine-tuned <code>GPT-3.5</code> and <code>GPT-4</code> can predict both the synthesizability of inorganic compounds from their chemical formulas and select appropriate precursors for synthesis, achieving performance comparable to specialized ML models with minimal development time and cost. In a follow-up work, they extended this approach to structure-based predictions of inorganic crystal polymorphs, where LLMs provided human-readable explanations for their synthesizability assessments[<span class="citation" data-cites="kim2025explainable">Kim, Schrier, and Jung (<a href="09-references.html#ref-kim2025explainable" role="doc-biblioref">2025</a>)</span>]. Notably, their structure-aware models correctly identified twelve hypothetical compounds as non-synthesizable despite their thermodynamic stability, perfectly matching experimental outcomes where synthesis attempts failed.</p>
<p>Beyond retrosynthetic prediction, LLMs have also been deployed as reasoning engines for autonomous design. <span class="citation" data-cites="bran2024augmenting">Bran et al. (<a href="09-references.html#ref-bran2024augmenting" role="doc-biblioref">2024</a>)</span> developed <code>ChemCrow</code>, an LLM-based system that autonomously plans and executes the synthesis of novel compounds by integrating specialized tools like a retrosynthesis planner (see <a href="05-applications.html#sec:planning" data-reference-type="ref+Label" data-reference="sec:planning">[sec:planning]</a> to read more about this capability of <code>ChemCrow</code> and its limitations) and reaction predictors. This approach mirrors the iterative experimental design cycle employed by human chemists, but is equipped with the scalability of automation. Notably, systems like <code>ChemCrow</code> rely on high-quality reaction data to ground their reasoning in empirically viable chemistry, which, depending on the design space, could be a limitation.</p>
</section>
<section id="sec:llm-optimizers" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="sec:llm-optimizers"><span class="header-section-number">6.4</span> LLMs as Optimizers</h2>
<figure id="fig:optimization" class="figure">
<img src="media/figures/rescaled_figures/chemrev_figure22.png" alt="" width="100%" class="figure-img">
<figcaption>
<strong>Overview of the iterative optimization loop that mirrors the structure of the optimization section</strong>. The blue boxes contain the different roles that the <span data-acronym-label="llm" data-acronym-form="plural+short">llms</span> play in the loop, and which are described in the main text. References in which the use of LLMs for that step are detailed inside the small boxes inside each of the components of the loop. The example shown is about obtaining molecules with high <code>logP</code>.
</figcaption>
</figure>
<p>Discovering novel compounds and reactions in chemistry and materials science has long relied on iterative trial-and-error processes rooted in existing domain knowledge [<span class="citation" data-cites="Taylor2023brief">Taylor et al. (<a href="09-references.html#ref-Taylor2023brief" role="doc-biblioref">2023</a>)</span>]. While, as explained in <a href="#sec:retrosynthesis" data-reference-type="ref+Label" data-reference="sec:retrosynthesis">1.3</a>, those methods are used to accelerate this process, optimization methods help improve conditions, binding affinity, etc. These approaches are slow and labor-intensive. Traditional data-driven methods aimed to address these limitations by combining predictive ML models with optimization frameworks such as Bayesian optimization (BO) or evolutionary algorithm (EA)s. These frameworks balance exploration of uncharted regions in chemical space with exploitation of known high-performing regions [<span class="citation" data-cites="Li2024sequential">X. Li et al. (<a href="09-references.html#ref-Li2024sequential" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="Hse2021gryffin">Häse et al. (<a href="09-references.html#ref-Hse2021gryffin" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="Shields2021bayesian">Shields et al. (<a href="09-references.html#ref-Shields2021bayesian" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="Griffiths2020constrained">Griffiths and Hernández-Lobato (<a href="09-references.html#ref-Griffiths2020constrained" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="RajabiKochi2025adaptive">Rajabi-Kochi et al. (<a href="09-references.html#ref-RajabiKochi2025adaptive" role="doc-biblioref">2025</a>)</span>].</p>
<p>Recent advances in LLMs have unlocked potential for addressing optimization challenges in chemistry and related domains [<span class="citation" data-cites="fernando2023promptbreeder0">Fernando et al. (<a href="09-references.html#ref-fernando2023promptbreeder0" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="yang2023large">Yang et al. (<a href="09-references.html#ref-yang2023large" role="doc-biblioref">2023</a>)</span>; <span class="citation" data-cites="chen2024instruct">Chen et al. (<a href="09-references.html#ref-chen2024instruct" role="doc-biblioref">2024</a>)</span>]. A key strength of LLMs lies in their capacity to frame optimization tasks through natural language, which enhances knowledge incorporation, improves candidate comparisons, and increases interpretability. This aligns well with chemical problem-solving, where complex phenomena, such as reaction pathways or material behaviors, are often poorly captured by standard nomenclature; however, they can still be intuitively explained through natural language. Moreover, GPMs’ general capabilities provide flexibility beyond classical methods, which have to be trained from scratch if the optimization problem or any of its variables changes. By encoding domain-specific knowledge—including reaction rules, thermodynamic principles, and structure-property relationships—into structured prompts, LLMs can synergize expertise with their ability to navigate complex chemical optimization problems.</p>
<p>Current LLM applications in chemistry optimization vary in scope and methodology. Many studies integrate LLMs into BO frameworks, where models guide experimental design by predicting promising candidates [<span class="citation" data-cites="rankovic2023bochemian">Ranković and Schwaller (<a href="09-references.html#ref-rankovic2023bochemian" role="doc-biblioref">2023</a>)</span>]. Others employ genetic algorithm (GA)s or hybrid strategies that combine LLM-generated hypotheses with computational screening [<span class="citation" data-cites="cisse2025language0based">Cissé et al. (<a href="09-references.html#ref-cisse2025language0based" role="doc-biblioref">2025</a>)</span>].</p>
<section id="llms-as-surrogate-models" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="llms-as-surrogate-models"><span class="header-section-number">6.4.1</span> LLMs as Surrogate Models</h3>
<p>A prominent LLM-driven strategy positions these models as surrogate models within optimization loops. Typically implemented as Gaussian process regression (GPR), surrogate models learn from prior data to approximate costly feature-outcome landscapes, which are often computationally and time-consuming to evaluate, thereby guiding the acquisition. LLMs offer major advantages in this role primarily through strong low-data performance. Their ICL capability enables task demonstration with minimal prompt examples while leveraging chemical knowledge from pre-training to generate accurate predictions. This allows GPMs to compensate for sparse experimental data effectively.</p>
<p><span class="citation" data-cites="ramos2023bayesian">Ramos et al. (<a href="09-references.html#ref-ramos2023bayesian" role="doc-biblioref">2023</a>)</span> demonstrated the viability of this paradigm through a simple yet effective framework that combines ICL using only one example in the prompt with a BO workflow. Their BO-ICL approach uses few-shot examples formatted as question-answer pairs, where the LLM generates candidate solutions conditioned on prior successful iterations. These candidates are ranked using an acquisition function, with top-<span class="math inline">\(k\)</span> selections integrated into subsequent prompts to refine predictions iteratively. Remarkably, this method achieved high performance in optimizing catalytic reaction conditions, even matching the top-1 accuracies observed in experimental benchmarks. This emphasizes the potential of LLMs as accessible, ICL optimizers when coupled with well-designed prompts.</p>
<p>To address limitations in base LLMs’ inherent chemical knowledge—particularly their grasp of specialized representations like SMILES or structure-property mappings—<span class="citation" data-cites="yu2025collaborative">J. Yu et al. (<a href="09-references.html#ref-yu2025collaborative" role="doc-biblioref">2025</a>)</span> introduced a hybrid architecture augmenting pre-trained LLMs with task-specific embedding and prediction layers. These layers, fine-tuned on domain data, align latent representations of input-output pairs (denoted as <code>&lt;x&gt;</code> and <code>&lt;y&gt;</code> in prompts), enabling the model to map chemical structures and properties into a unified, interpretable space. Crucially, the added layers enhance chemical reasoning without sacrificing the flexibility of ICL, allowing the system to adapt to trends across iterations, similarly to what was done by <span class="citation" data-cites="ramos2023bayesian">Ramos et al. (<a href="09-references.html#ref-ramos2023bayesian" role="doc-biblioref">2023</a>)</span>. In their evaluations of molecular optimization benchmarks, such as the practical molecular optimization (PMO) [<span class="citation" data-cites="gao2022sample">Gao et al. (<a href="09-references.html#ref-gao2022sample" role="doc-biblioref">2022</a>)</span>], they revealed improvements over conventional methods, including BO-Gaussian process (GP), RL methods, and GA.</p>
<p><span class="citation" data-cites="yu2025collaborative">J. Yu et al. (<a href="09-references.html#ref-yu2025collaborative" role="doc-biblioref">2025</a>)</span> further highlighted the framework’s extensibility to diverse black-box optimization challenges beyond chemistry. This represents one of the most important advantages of using LLMs as orchestrators of the optimization process. The flexibility of natural language in this process enables the procedure to be applied to any optimization process. In contrast, classical methods are constrained to the specific task for which they are designed due to the need to train the surrogate model.</p>
</section>
<section id="llms-as-next-candidate-generators" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="llms-as-next-candidate-generators"><span class="header-section-number">6.4.2</span> LLMs as Next Candidate Generators</h3>
<p>Recent studies demonstrate the potential of LLMs to enhance EAs [<span class="citation" data-cites="lu2024generative">Lu et al. (<a href="09-references.html#ref-lu2024generative" role="doc-biblioref">2025</a>)</span>] and BO [<span class="citation" data-cites="amin2025towards">Amin, Raja, and Krishnapriyan (<a href="09-references.html#ref-amin2025towards" role="doc-biblioref">2025</a>)</span>] frameworks by leveraging their embedded chemical knowledge and ability to integrate prior information, thereby reducing computational effort while improving output quality. Within EAs, LLMs refine molecular candidates through mutations (modifying molecular substructures) or crossovers (combining parent molecules). In BO frameworks, they serve as acquisition functions, utilizing surrogate model predictions—both mean and uncertainty—to select optimal molecules or reaction conditions for evaluation.</p>
<p>For molecule optimization, <span class="citation" data-cites="yu2025collaborative">J. Yu et al. (<a href="09-references.html#ref-yu2025collaborative" role="doc-biblioref">2025</a>)</span> introduced <code>MultiModel</code>, a dual-LLM system where one model proposes candidates and the other supplies domain knowledge (see <a href="#sec:opt-llm-know-source" data-reference-type="ref+Label" data-reference="sec:opt-llm-know-source">1.4.3</a>). By fine-tuning the “worker” LLM to recognize molecular scaffolds and target properties, and expanding the training pool to include a million-size pre-training dataset, they achieved hit rates exceeding <span class="math inline">\(90\%\)</span>. Similarly, <span class="citation" data-cites="wang2024efficient">H. Wang, Skreta, et al. (<a href="09-references.html#ref-wang2024efficient" role="doc-biblioref">2025</a>)</span> developed <code>MoLLEO</code>, integrating an LLM into an EA to replace random mutations with LLM-guided modifications. Here, <code>GPT-4</code> generated optimized offspring from parent molecules, significantly accelerating convergence to high fitness scores. Notably, while domain-specialized models (<code>BioT5</code>, <code>MoleculeSTM</code>) underperformed, the general-purpose <code>GPT-4</code> excelled—a finding that underscores the context-dependent utility of LLMs</p>
<p>In a related approach, <span class="citation" data-cites="lu2024generative">Lu et al. (<a href="09-references.html#ref-lu2024generative" role="doc-biblioref">2025</a>)</span> showed that well-designed prompts—incorporating task-specific constraints, objectives, and few-shot examples—enable general LLMs (<code>Claude-Sonnet</code>, <code>o1-preview</code>) to generate high-quality candidates without fine-tuning, outperforming both random selection and vanilla GAs in functional transition metal complexes (TMC) design.</p>
</section>
<section id="sec:opt-llm-know-source" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="sec:opt-llm-know-source"><span class="header-section-number">6.4.3</span> LLMs as Prior Knowledge Sources</h3>
<p>A key advantage of integrating LLMs into optimization frameworks is their ability to encode and deploy prior knowledge within the optimization loop. As illustrated in <a href="#fig:optimization" data-reference-type="ref+Label" data-reference="fig:optimization">4</a>, this knowledge can be directed into either the surrogate model or candidate generation module, significantly reducing the number of optimization steps required through high-quality guidance.</p>
<p>For example, <span class="citation" data-cites="yu2025collaborative">J. Yu et al. (<a href="09-references.html#ref-yu2025collaborative" role="doc-biblioref">2025</a>)</span> deployed a “research” agent that leverages <code>Google</code> search and <code>RDKit</code> to verify and rank molecules generated by “worker” agents against target features and properties. Their results demonstrate substantial improvements when this filtering mechanism is applied.</p>
<p>Similarly, <span class="citation" data-cites="cisse2025language0based">Cissé et al. (<a href="09-references.html#ref-cisse2025language0based" role="doc-biblioref">2025</a>)</span> introduced <code>BORA</code>, which contextualizes conventional black-box BO using an LLM. <code>BORA</code> maintains standard BO as the core driver but strategically activates the LLM when progress stalls. This leverages the model’s ICL capabilities to hypothesize promising search regions and propose new samples, regulated by a lightweight heuristic policy that manages costs and incorporates domain knowledge (or user input). Evaluations on synthetic benchmarks such as the catalyst optimization task for hydrogen generation show that <code>BORA</code> accelerates exploration, improves convergence, and outperforms existing LLM-BO hybrids.</p>
</section>
<section id="how-to-face-optimization-problems" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4" class="anchored" data-anchor-id="how-to-face-optimization-problems"><span class="header-section-number">6.4.4</span> How to Face Optimization Problems?</h3>
<p>Published works explore different ways of using LLMs for optimization problems in chemistry, from simple approaches, such as just prompting the model with some initial random set of experimental candidates and iterating [<span class="citation" data-cites="ramos2023bayesian">Ramos et al. (<a href="09-references.html#ref-ramos2023bayesian" role="doc-biblioref">2023</a>)</span>], to fine-tuning models in BO fashion [<span class="citation" data-cites="rankovic2025gollum0">Ranković and Schwaller (<a href="09-references.html#ref-rankovic2025gollum0" role="doc-biblioref">2025</a>)</span>]. The most efficient initial point is by relying entirely on a ICL approach, which allows one to obtain a first signal rapidly. Such initial results will enable to determine whether a more complex, computationally intensive approach is necessary or whether prompt engineering is reliable enough for the application. Fine-tuning can be used as a way to enhance the chemical knowledge of the LLMs and can lead to improvements in optimization tasks where the model requires such knowledge to choose or generate better candidates. Fine-tuning might not be a game-changer for other approaches that rely more on sampling methods [<span class="citation" data-cites="wang2025llm0augmented">H. Wang, Guo, et al. (<a href="09-references.html#ref-wang2025llm0augmented" role="doc-biblioref">2025</a>)</span>].</p>
<p>While some initial works showed that LLMs trained specifically on chemistry perform better for optimization tasks [<span class="citation" data-cites="kristiadi2024sober">Kristiadi et al. (<a href="09-references.html#ref-kristiadi2024sober" role="doc-biblioref">2024</a>)</span>], other works showed that a GPM such as <code>GPT-4</code> combined with an EA outperformed all other models [<span class="citation" data-cites="wang2024efficient">H. Wang, Skreta, et al. (<a href="09-references.html#ref-wang2024efficient" role="doc-biblioref">2025</a>)</span>]. Is it better to incorporate a general model or a chemistry LM into the optimization frameworks? We hypothesize that for models of the same size (in number of parameters) and similar training size—attending to peta floating point operations per second (PFLOP)s—a chemical LM (a specialized model) will consistently outperform general models. If the models differ significantly in size, the larger model will typically perform better.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-ahmad2022chemberta" class="csl-entry" role="listitem">
Ahmad, Walid, Elana Simon, Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. 2022. <span>“<span class="nocase">Chemberta-2: Towards chemical foundation models</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2209.01712">https://doi.org/10.48550/arXiv.2209.01712</a>.
</div>
<div id="ref-alampara2024mattext" class="csl-entry" role="listitem">
Alampara, Nawaf, Santiago Miret, and Kevin Maik Jablonka. 2024. <span>“<span class="nocase">MatText: Do language models need more than text &amp; scale for materials modeling?</span>”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2406.17295">https://doi.org/10.48550/arXiv.2406.17295</a>.
</div>
<div id="ref-amin2025towards" class="csl-entry" role="listitem">
Amin, Ishan, Sanjeev Raja, and Aditi Krishnapriyan. 2025. <span>“<span class="nocase">Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians</span>.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2501.09009">https://doi.org/10.48550/arXiv.2501.09009</a>.
</div>
<div id="ref-aneesh2025semantic" class="csl-entry" role="listitem">
Aneesh, Anagha, Nawaf Alampara, José A. Márquez, and Kevin Maik Jablonka. 2025. <span>“Semantic Device Graphs for Perovskite Solar Cell Design.”</span> <em>The Thirsteenth International Conference on Learning Representations Workshop on AI for Materials Science, <span>ICLR-AI4MAT</span></em>. <a href="https://openreview.net/forum?id=AGCClISEXL&amp;referrer=%5Bthe%20profile%20of%20Anagha%20Aneesh%5D(%2Fprofile%3Fid%3D~Anagha_Aneesh1)">https://openreview.net/forum?id=AGCClISEXL&amp;referrer=%5Bthe%20profile%20of%20Anagha%20Aneesh%5D(%2Fprofile%3Fid%3D~Anagha_Aneesh1)</a>.
</div>
<div id="ref-ansari2024dziner" class="csl-entry" role="listitem">
Ansari, Mehrad, Jeffrey Watchorn, Carla E. Brown, and Joseph S. Brown. 2024. <span>“<span class="nocase">dZiner</span>: <span>Rational</span> <span>Inverse</span> <span>Design</span> of <span>Materials</span> with <span>AI</span> <span>Agents</span>.”</span> <em>Arxiv Preprint</em>, October. <a href="https://doi.org/10.48550/arXiv.2410.03963">https://doi.org/10.48550/arXiv.2410.03963</a>.
</div>
<div id="ref-balaji2023gptmolberta" class="csl-entry" role="listitem">
Balaji, Suryanarayanan, Rishikesh Magar, Yayati Jadhav, and Amir Barati Farimani. 2023. <span>“<span>GPT</span>-<span>MolBERTa</span>: <span>GPT</span> <span>Molecular</span> <span>Features</span> <span>Language</span> <span>Model</span> for Molecular Property Prediction.”</span> <em>Arxiv Preprint arXiv:2310.03030</em>, October. <a href="https://doi.org/10.48550/arXiv.2310.03030">https://doi.org/10.48550/arXiv.2310.03030</a>.
</div>
<div id="ref-batatia2022mace" class="csl-entry" role="listitem">
Batatia, Ilyes, D. Kov’acs, G. Simm, C. Ortner, and Gábor Csányi. 2022. <span>“MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields.”</span> <em>Neural Information Processing Systems</em>. <a href="https://doi.org/10.48550/arXiv.2206.07697">https://doi.org/10.48550/arXiv.2206.07697</a>.
</div>
<div id="ref-bhattacharya2024large" class="csl-entry" role="listitem">
Bhattacharya, Debjyoti, Harrison J. Cassady, Michael A. Hickner, and Wesley F. Reinhart. 2024. <span>“Large <span>Language</span> <span>Models</span> as <span>Molecular</span> <span>Design</span> <span>Engines</span>.”</span> <em>Journal of Chemical Information and Modeling</em> 64 (18): 7086–96. <a href="https://doi.org/10.1021/acs.jcim.4c01396">https://doi.org/10.1021/acs.jcim.4c01396</a>.
</div>
<div id="ref-bran2024augmenting" class="csl-entry" role="listitem">
Bran, Andres M., Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and Philippe Schwaller. 2024. <span>“Augmenting Large Language Models with Chemistry Tools.”</span> <em>Nature Machine Intelligence</em> 6 (5). <a href="https://doi.org/10.1038/s42256-024-00832-8">https://doi.org/10.1038/s42256-024-00832-8</a>.
</div>
<div id="ref-brown2019guacamol" class="csl-entry" role="listitem">
Brown, Nathan, Marco Fiscato, Marwin H. S. Segler, and Alain C. Vaucher. 2019. <span>“GuacaMol: Benchmarking Models for de Novo Molecular Design.”</span> <em>Journal of Chemical Information and Modeling</em> 59 (3): 1096–1108. <a href="https://doi.org/10.1021/acs.jcim.8b00839">https://doi.org/10.1021/acs.jcim.8b00839</a>.
</div>
<div id="ref-calanzone2025mol-moe" class="csl-entry" role="listitem">
Calanzone, Diego, Pierluca D’Oro, and Pierre-Luc Bacon. 2025. <span>“Mol-<span>MoE</span>: <span>Training</span> <span>Preference</span>-<span>Guided</span> <span>Routers</span> for <span>Molecule</span> <span>Generation</span>.”</span> <em>Arxiv Preprint arXiv:2502.05633</em>, February. <a href="https://doi.org/10.48550/arXiv.2502.05633">https://doi.org/10.48550/arXiv.2502.05633</a>.
</div>
<div id="ref-cao2024space" class="csl-entry" role="listitem">
Cao, Zhendong, Xiaoshan Luo, Jian Lv, and Lei Wang. 2024. <span>“Space Group Informed Transformer for Crystalline Materials Generation.”</span> <em>arXiv Preprint arXiv: 2403.15734</em>. <a href="https://doi.org/10.48550/arXiv.2403.15734">https://doi.org/10.48550/arXiv.2403.15734</a>.
</div>
<div id="ref-cao2025crystalformer-rl" class="csl-entry" role="listitem">
Cao, Zhendong, and Lei Wang. 2025. <span>“<span>CrystalFormer</span>-<span>RL</span>: <span>Reinforcement</span> <span>Fine</span>-<span>Tuning</span> for <span>Materials</span> <span>Design</span>.”</span> <em>Arxiv Preprint arXiv:2504.02367</em>, April. <a href="https://doi.org/10.48550/arXiv.2504.02367">https://doi.org/10.48550/arXiv.2504.02367</a>.
</div>
<div id="ref-cavanagh2024smileyllama" class="csl-entry" role="listitem">
Cavanagh, Joseph M., Kunyang Sun, Andrew Gritsevskiy, Dorian Bagni, Thomas D. Bannister, and Teresa Head-Gordon. 2024. <span>“SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration.”</span> <em>arXiv Preprint arXiv: 2409.02231</em>. <a href="https://doi.org/10.48550/arXiv.2409.02231">https://doi.org/10.48550/arXiv.2409.02231</a>.
</div>
<div id="ref-chen2024instruct" class="csl-entry" role="listitem">
Chen, Lichang, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. 2024. <span>“InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models.”</span> <em>Forty-First International Conference on Machine Learning, <span>ICML</span> 2024</em>. <a href="https://openreview.net/forum?id=rADFNrIss3">https://openreview.net/forum?id=rADFNrIss3</a>.
</div>
<div id="ref-cheng2023group" class="csl-entry" role="listitem">
Cheng, Austin H, Andy Cai, Santiago Miret, Gustavo Malkomes, Mariano Phielipp, and Alán Aspuru-Guzik. 2023. <span>“Group SELFIES: A Robust Fragment-Based Molecular String Representation.”</span> <em>Digital Discovery</em> 2 (3): 748–58. <a href="https://doi.org/10.1039/D3DD00012E">https://doi.org/10.1039/D3DD00012E</a>.
</div>
<div id="ref-chennakesavalu2025aligning" class="csl-entry" role="listitem">
Chennakesavalu, Shriram, Frank Hu, Sebastian Ibarraran, and Grant M. Rotskoff. 2025. <span>“Aligning <span>Transformers</span> with <span>Continuous</span> <span>Feedback</span> via <span>Energy</span> <span>Rank</span> <span>Alignment</span>.”</span> <em>Arxiv Preprint arXiv:2405.12961</em>, May. <a href="https://doi.org/10.48550/arXiv.2405.12961">https://doi.org/10.48550/arXiv.2405.12961</a>.
</div>
<div id="ref-chiang2024llamp" class="csl-entry" role="listitem">
Chiang, Yuan, Elvis Hsieh, Chia-Hong Chou, and Janosh Riebesell. 2024. <span>“<span class="nocase"><span>LLaMP</span>: <span>Large</span> <span>Language</span> <span>Model</span> <span>Made</span> <span>Powerful</span> for <span>High</span>-fidelity <span>Materials</span> <span>Knowledge</span> <span>Retrieval</span> and <span>Distillation</span></span>.”</span> <em>Arxiv</em>, October. <a href="https://doi.org/10.48550/arXiv.2401.17244">https://doi.org/10.48550/arXiv.2401.17244</a>.
</div>
<div id="ref-chithrananda2020chemberta" class="csl-entry" role="listitem">
Chithrananda, Seyone, Gabriel Grand, and Bharath Ramsundar. 2020. <span>“<span class="nocase"><span>ChemBERTa</span>: <span>Large</span>-<span>Scale</span> <span>Self</span>-<span>Supervised</span> <span>Pretraining</span> for <span>Molecular</span> <span>Property</span> <span>Prediction</span></span>.”</span> <em>Arxiv</em>, October. <a href="https://doi.org/10.48550/arXiv.2010.09885">https://doi.org/10.48550/arXiv.2010.09885</a>.
</div>
<div id="ref-cisse2025language0based" class="csl-entry" role="listitem">
Cissé, Abdoulatif, Xenophon Evangelopoulos, Vladimir V. Gusev, and Andrew I. Cooper. 2025. <span>“Language-Based Bayesian Optimization Research Assistant (BORA).”</span> <em>arXiv Preprint arXiv: 2501.16224</em>. <a href="https://doi.org/10.48550/arXiv.2501.16224">https://doi.org/10.48550/arXiv.2501.16224</a>.
</div>
<div id="ref-dinh2022lift" class="csl-entry" role="listitem">
Dinh, Tuan, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dimitris Papailiopoulos, and Kangwook Lee. 2022. <span>“<span class="nocase"><span>LIFT</span>: <span>Language</span>-<span>Interfaced</span> <span>Fine</span>-<span>Tuning</span> for <span>Non</span>-language <span>Machine</span> <span>Learning</span> <span>Tasks</span></span>.”</span> <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em> 35: 11763–84. <a href="https://doi.org/10.48550/arXiv.2206.06565">https://doi.org/10.48550/arXiv.2206.06565</a>.
</div>
<div id="ref-edwards2022translation" class="csl-entry" role="listitem">
Edwards, Carl, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. 2022. <span>“Translation Between Molecules and Natural Language.”</span> <em>Arxiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2204.11817">https://doi.org/10.48550/arXiv.2204.11817</a>.
</div>
<div id="ref-edwards2021text2mol" class="csl-entry" role="listitem">
Edwards, Carl, ChengXiang Zhai, and Heng Ji. 2021. <span>“<span>T</span>ext2<span>M</span>ol: Cross-Modal Molecule Retrieval with Natural Language Queries.”</span> <em>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, November, 595–607. <a href="https://doi.org/10.18653/v1/2021.emnlp-main.47">https://doi.org/10.18653/v1/2021.emnlp-main.47</a>.
</div>
<div id="ref-fernando2023promptbreeder0" class="csl-entry" role="listitem">
Fernando, Chrisantha, Dylan Banarse, H. Michalewski, Simon Osindero, and Tim Rocktäschel. 2023. <span>“Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution.”</span> <em>International Conference on Machine Learning</em>. <a href="https://doi.org/10.48550/arXiv.2309.16797">https://doi.org/10.48550/arXiv.2309.16797</a>.
</div>
<div id="ref-fifty2023incontext" class="csl-entry" role="listitem">
Fifty, Christopher, Jure Leskovec, and Sebastian Thrun. 2023. <span>“<span class="nocase">In-Context Learning for Few-Shot Molecular Property Prediction</span>.”</span> <em>arXiv Preprint arXiv: 2310.08863</em>. <a href="https://doi.org/10.48550/arXiv.2310.08863">https://doi.org/10.48550/arXiv.2310.08863</a>.
</div>
<div id="ref-frey2023neural" class="csl-entry" role="listitem">
Frey, Nathan C., Ryan Soklaski, Simon Axelrod, Siddharth Samsi, Rafael Gómez-Bombarelli, Connor W. Coley, and Vijay Gadepally. 2023. <span>“Neural Scaling of Deep Chemical Models.”</span> <em>Nature Machine Intelligence</em> 5 (11): 1297–1305. <a href="https://doi.org/10.1038/s42256-023-00740-3">https://doi.org/10.1038/s42256-023-00740-3</a>.
</div>
<div id="ref-ganose2019robocrystallographer" class="csl-entry" role="listitem">
Ganose, Alex M, and Anubhav Jain. 2019. <span>“<span class="nocase">Robocrystallographer: automated crystal structure text descriptions and analysis</span>.”</span> <em>MRS Communications</em> 9 (3): 874–81. <a href="https://doi.org/10.1557/mrc.2019.94">https://doi.org/10.1557/mrc.2019.94</a>.
</div>
<div id="ref-gao2022sample" class="csl-entry" role="listitem">
Gao, Wenhao, Tianfan Fu, Jimeng Sun, and Connor W. Coley. 2022. <span>“Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization.”</span> <em>Neural Information Processing Systems</em>. <a href="https://doi.org/10.48550/arXiv.2206.12411">https://doi.org/10.48550/arXiv.2206.12411</a>.
</div>
<div id="ref-Griffiths2020constrained" class="csl-entry" role="listitem">
Griffiths, Ryan-Rhys, and José Miguel Hernández-Lobato. 2020. <span>“Constrained Bayesian Optimization for Automatic Chemical Design Using Variational Autoencoders.”</span> <em>Chemical Science</em> 11 (2): 577–86. <a href="https://doi.org/10.1039/c9sc04026a">https://doi.org/10.1039/c9sc04026a</a>.
</div>
<div id="ref-gruver2024promises" class="csl-entry" role="listitem">
Gruver, Nate, Marc Anton Finzi, Dylan Sam, J. Zico Kolter, Ben Athiwaratkun, and Andrew Gordon Wilson. 2024. <span>“The Promises and Pitfalls of Language Models for Structured Numerical Data.”</span> <em>OpenReview.net</em>, October. <a href="https://openreview.net/forum?id=SZpygmv3G1">https://openreview.net/forum?id=SZpygmv3G1</a>.
</div>
<div id="ref-guo2023large" class="csl-entry" role="listitem">
Guo, Taicheng, Kehan Guo, B. Nan, Zhengwen Liang, Zhichun Guo, N. Chawla, O. Wiest, and Xiangliang Zhang. 2023. <span>“<span class="nocase">What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks</span>.”</span> <em>Neural Information Processing Systems</em>. <a href="https://doi.org/10.48550/arXiv.2305.18365">https://doi.org/10.48550/arXiv.2305.18365</a>.
</div>
<div id="ref-hall1991crystallographic" class="csl-entry" role="listitem">
Hall, S. R., F. H. Allen, and I. D. Brown. 1991. <span>“The Crystallographic Information File (<span>CIF</span>): A New Standard Archive File for Crystallography.”</span> <em>Acta Crystallographica Section A</em> 47 (6): 655–85. <a href="https://doi.org/10.1107/S010876739101067X">https://doi.org/10.1107/S010876739101067X</a>.
</div>
<div id="ref-Hse2021gryffin" class="csl-entry" role="listitem">
Häse, Florian, Matteo Aldeghi, Riley J. Hickman, Loı̈c M. Roch, and Alán Aspuru-Guzik. 2021. <span>“G&lt;scp&gt;ryffin&lt;/Scp&gt;: An Algorithm for Bayesian Optimization of Categorical Variables Informed by Expert Knowledge.”</span> <em>Applied Physics Reviews</em> 8 (3). <a href="https://doi.org/10.1063/5.0048164">https://doi.org/10.1063/5.0048164</a>.
</div>
<div id="ref-huang2016understanding" class="csl-entry" role="listitem">
Huang, Bing, and O. Anatole von Lilienfeld. 2016. <span>“Understanding Molecular Representations in Machine Learning: The Role of Uniqueness and Target Similarity.”</span> <em>arXiv Preprint arXiv: 1608.06194</em>. <a href="https://doi.org/10.48550/arXiv.1608.06194">https://doi.org/10.48550/arXiv.1608.06194</a>.
</div>
<div id="ref-Jablonka2023" class="csl-entry" role="listitem">
Jablonka, Kevin Maik, Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar, Joshua D. Bocarsly, Andres M. Bran, Stefan Bringuier, et al. 2023. <span>“<span class="nocase">14 examples of how LLMs can transform materials science and chemistry: a reflection on a large language model hackathon</span>.”</span> <em>Digital Discovery</em> 2 (5): 1233–50. <a href="https://doi.org/10.1039/d3dd00113j">https://doi.org/10.1039/d3dd00113j</a>.
</div>
<div id="ref-jablonka2024leveraging" class="csl-entry" role="listitem">
Jablonka, Kevin Maik, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. 2024. <span>“<span class="nocase">Leveraging large language models for predictive chemistry</span>.”</span> <em>Nature Machine Intelligence</em> 6 (2): 161–69. <a href="https://doi.org/10.1038/s42256-023-00788-1">https://doi.org/10.1038/s42256-023-00788-1</a>.
</div>
<div id="ref-jang2025can" class="csl-entry" role="listitem">
Jang, Hyosoon, Yunhui Jang, Jaehyung Kim, and Sungsoo Ahn. 2025. <span>“Can <span>LLMs</span> <span>Generate</span> <span>Diverse</span> <span>Molecules</span>? <span>Towards</span> <span>Alignment</span> with <span>Structural</span> <span>Diversity</span>.”</span> <em>Arxiv Preprint arXiv:2410.03138</em>, February. <a href="https://doi.org/10.48550/arXiv.2410.03138">https://doi.org/10.48550/arXiv.2410.03138</a>.
</div>
<div id="ref-kaur2025data" class="csl-entry" role="listitem">
Kaur, Harveen, Flaviano Della Pia, Ilyes Batatia, Xavier R Advincula, Benjamin X Shi, Jinggang Lan, Gábor Csányi, Angelos Michaelides, and Venkat Kapil. 2025. <span>“Data-Efficient Fine-Tuning of Foundational Models for First-Principles Quality Sublimation Enthalpies.”</span> <em>Faraday Discussions</em> 256: 120–38. <a href="https://doi.org/10.1039/d4fd00107a">https://doi.org/10.1039/d4fd00107a</a>.
</div>
<div id="ref-kim2024large" class="csl-entry" role="listitem">
Kim, Seongmin, Yousung Jung, and Joshua Schrier. 2024. <span>“Large Language Models for Inorganic Synthesis Predictions.”</span> <em>Journal of the American Chemical Society</em>.
</div>
<div id="ref-kim2025explainable" class="csl-entry" role="listitem">
Kim, Seongmin, Joshua Schrier, and Yousung Jung. 2025. <span>“Explainable Synthesizability Prediction of Inorganic Crystal Polymorphs Using Large Language Models.”</span> <em>Angewandte Chemie International Edition</em>. <a href="https://doi.org/10.1002/anie.202423950">https://doi.org/10.1002/anie.202423950</a>.
</div>
<div id="ref-kingsbury2022flexible" class="csl-entry" role="listitem">
Kingsbury, Ryan S., Andrew S. Rosen, Ayush S. Gupta, Jason M. Munro, Shyue Ping Ong, Anubhav Jain, Shyam Dwaraknath, Matthew K. Horton, and Kristin A. Persson. 2022. <span>“A Flexible and Scalable Scheme for Mixing Computed Formation Energies from Different Levels of Theory.”</span> <em>Npj Computational Materials</em>. <a href="https://doi.org/10.1038/s41524-022-00881-w">https://doi.org/10.1038/s41524-022-00881-w</a>.
</div>
<div id="ref-krenn2020self" class="csl-entry" role="listitem">
Krenn, Mario, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. 2020. <span>“<span class="nocase">Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation</span>.”</span> <em>Machine Learning: Science and Technology</em> 1 (4): 045024. <a href="https://doi.org/10.1088/2632-2153/aba947">https://doi.org/10.1088/2632-2153/aba947</a>.
</div>
<div id="ref-kristiadi2024sober" class="csl-entry" role="listitem">
Kristiadi, Agustinus, Felix Strieth-Kalthoff, Marta Skreta, Pascal Poupart, Alán Aspuru-Guzik, and Geoff Pleiss. 2024. <span>“A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization over Molecules?”</span> <em>Forty-First International Conference on Machine Learning, <span>ICML</span> 2024</em>. <a href="https://doi.org/10.48550/arXiv.2402.05015">https://doi.org/10.48550/arXiv.2402.05015</a>.
</div>
<div id="ref-krzyzanowski2025exploring" class="csl-entry" role="listitem">
Krzyzanowski, Adrian, Stephen D. Pickett, and Peter Pogány. 2025. <span>“Exploring <span>BERT</span> for Reaction Yield Prediction: Evaluating the Impact of Tokenization, Molecular Representation, and Pretraining Data Augmentation.”</span> <em>Journal of Chemical Information and Modeling</em> 65 (9): 4381–4402. <a href="https://doi.org/10.1021/acs.jcim.5c00359">https://doi.org/10.1021/acs.jcim.5c00359</a>.
</div>
<div id="ref-lee2024fine-tuning" class="csl-entry" role="listitem">
Lee, Daeseok, and Yongjun Cho. 2024. <span>“<span>FINE</span>-<span>TUNING</span> <span>POCKET</span>-<span>CONDITIONED</span> <span>3D</span> <span>MOLECULE</span> <span>GENERATION</span> <span>VIA</span> <span>REINFORCEMENT</span> <span>LEARNING</span>.”</span> <em>The Twelfth International Conference on Learning Representations Workshop on Generative and Experimental Perspectives for Biomolecular Design, <span>ICLR-GEM</span></em>. <a href="https://openreview.net/forum?id=hlzRzr9ksu">https://openreview.net/forum?id=hlzRzr9ksu</a>.
</div>
<div id="ref-lee2025rag-enhanced" class="csl-entry" role="listitem">
Lee, Namkyeong, Edward De Brouwer, Ehsan Hajiramezanali, Tommaso Biancalani, Chanyoung Park, and Gabriele Scalia. 2025. <span>“RAG-Enhanced Collaborative LLM Agents for Drug Discovery.”</span> <em>arXiv Preprint arXiv: 2502.17506</em>. <a href="https://doi.org/10.48550/arXiv.2502.17506">https://doi.org/10.48550/arXiv.2502.17506</a>.
</div>
<div id="ref-li2025large" class="csl-entry" role="listitem">
Li, Jiatong, Wei Liu, Zhihao Ding, Wenqi Fan, Yuqiang Li, and Qing Li. 2025. <span>“Large <span>Language</span> <span>Models</span> Are in-<span>Context</span> <span>Molecule</span> <span>Learners</span>.”</span> <em>IEEE Transactions on Knowledge and Data Engineering</em> 37 (7). <a href="https://doi.org/10.1109/TKDE.2025.3557697">https://doi.org/10.1109/TKDE.2025.3557697</a>.
</div>
<div id="ref-li2024molreflect" class="csl-entry" role="listitem">
Li, Jiatong, Yunqing Liu, Wei Liu, Jingdi Le, Di Zhang, Wenqi Fan, Dongzhan Zhou, Yuqiang Li, and Qing Li. 2024. <span>“<span>MolReFlect</span>: <span>Towards</span> <span>In</span>-<span>Context</span> <span>Fine</span>-Grained <span>Alignments</span> Between <span>Molecules</span> and <span>Texts</span>.”</span> <em>Arxiv Preprint arXiv:2411.14721</em>, November. <a href="https://doi.org/10.48550/arXiv.2411.14721">https://doi.org/10.48550/arXiv.2411.14721</a>.
</div>
<div id="ref-Li2024sequential" class="csl-entry" role="listitem">
Li, Xiaobo, Yu Che, Linjiang Chen, Tao Liu, Kewei Wang, Lunjie Liu, Haofan Yang, Edward O. Pyzer-Knapp, and Andrew I. Cooper. 2024. <span>“Sequential Closed-Loop Bayesian Optimization as a Guide for Organic Molecular Metallophotocatalyst Formulation Discovery.”</span> <em>Nature Chemistry</em> 16 (8): 1286–94. <a href="https://doi.org/10.1038/s41557-024-01546-5">https://doi.org/10.1038/s41557-024-01546-5</a>.
</div>
<div id="ref-li2024unveiling" class="csl-entry" role="listitem">
Li, Zhuoran, Xu Sun, Wanyu Lin, and Jiannong Cao. 2024. <span>“<span class="nocase">Unveiling Molecular Secrets: An LLM-Augmented Linear Model for Explainable and Calibratable Molecular Property Prediction</span>.”</span> <em>arXiv Preprint arXiv: 2410.08829</em>. <a href="https://doi.org/10.48550/arXiv.2410.08829">https://doi.org/10.48550/arXiv.2410.08829</a>.
</div>
<div id="ref-lin2025property" class="csl-entry" role="listitem">
Lin, Xuan, Long Chen, Yile Wang, Xiangxiang Zeng, and Philip S. Yu. 2025. <span>“Property <span>Enhanced</span> <span>Instruction</span> <span>Tuning</span> for <span>Multi</span>-Task <span>Molecule</span> <span>Generation</span> with <span>Large</span> <span>Language</span> <span>Models</span>.”</span> <em>Arxiv Preprint arXiv:2412.18084</em>, May. <a href="https://doi.org/10.48550/arXiv.2412.18084">https://doi.org/10.48550/arXiv.2412.18084</a>.
</div>
<div id="ref-liu2024multimodal" class="csl-entry" role="listitem">
Liu, Gang, Michael Sun, Wojciech Matusik, Meng Jiang, and Jie Chen. 2024. <span>“Multimodal <span>Large</span> <span>Language</span> <span>Models</span> for <span>Inverse</span> <span>Molecular</span> <span>Design</span> with <span>Retrosynthetic</span> <span>Planning</span>.”</span> <em>Arxiv Preprint arXiv: 2410.04223</em>, October. <a href="https://doi.org/10.48550/arXiv.2410.04223">https://doi.org/10.48550/arXiv.2410.04223</a>.
</div>
<div id="ref-liu2025integrating" class="csl-entry" role="listitem">
Liu, Hongxuan, Haoyu Yin, Zhiyao Luo, and Xiaonan Wang. 2025. <span>“Integrating Chemistry Knowledge in Large Language Models via Prompt Engineering.”</span> <em>Synthetic and Systems Biotechnology</em> 10 (1): 23–38. <a href="https://doi.org/10.1016/j.synbio.2024.07.004">https://doi.org/10.1016/j.synbio.2024.07.004</a>.
</div>
<div id="ref-liu2024moleculargpt" class="csl-entry" role="listitem">
Liu, Yuyan, Sirui Ding, Sheng Zhou, Wenqi Fan, and Qiaoyu Tan. 2024. <span>“<span>MolecularGPT</span>: <span>Open</span> <span>Large</span> <span>Language</span> <span>Model</span> (<span>LLM</span>) for <span>Few</span>-<span>Shot</span> <span>Molecular</span> <span>Property</span> <span>Prediction</span>.”</span> <em>Arxiv Preprint arXiv:2406.12950</em>, October. <a href="https://doi.org/10.48550/arXiv.2406.12950">https://doi.org/10.48550/arXiv.2406.12950</a>.
</div>
<div id="ref-liu2023molca" class="csl-entry" role="listitem">
Liu, Zhiyuan, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. 2023. <span>“<span>MolCA</span>: <span>Molecular</span> <span>Graph</span>-<span>Language</span> <span>Modeling</span> with <span>Cross</span>-<span>Modal</span> <span>Projector</span> and <span>Uni</span>-<span>Modal</span> <span>Adapter</span>.”</span> <em>arXiv Preprint arXiv:2310.12798v4</em>, October. <a href="https://doi.org/10.48550/arXiv.2310.12798">https://doi.org/10.48550/arXiv.2310.12798</a>.
</div>
<div id="ref-lu2024generative" class="csl-entry" role="listitem">
Lu, Jieyu, Zhangde Song, Qiyuan Zhao, Yuanqi Du, Yirui Cao, Haojun Jia, and Chenru Duan. 2025. <span>“Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge and Reasoning Capability of Large Language Models.”</span> <em>Journal of the American Chemical Society</em>, July. <a href="https://doi.org/10.1021/jacs.5c02097">https://doi.org/10.1021/jacs.5c02097</a>.
</div>
<div id="ref-preuer2018frechet" class="csl-entry" role="listitem">
Preuer, Kristina, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Günter Klambauer. 2018. <span>“Fréchet ChemNet Distance: A Metric for Generative Models for Molecules in Drug Discovery.”</span> <em>Journal of Chemical Information and Modeling</em> 58 (9): 1736–41. <a href="https://doi.org/10.1021/acs.jcim.8b00234">https://doi.org/10.1021/acs.jcim.8b00234</a>.
</div>
<div id="ref-raffel2020exploring" class="csl-entry" role="listitem">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. <span>“<span class="nocase">Exploring the limits of transfer learning with a unified text-to-text transformer</span>.”</span> <em>Journal of Machine Learning Research</em> 21 (140): 1–67. <a href="https://www.jmlr.org/papers/v21/20-074.html">https://www.jmlr.org/papers/v21/20-074.html</a>.
</div>
<div id="ref-RajabiKochi2025adaptive" class="csl-entry" role="listitem">
Rajabi-Kochi, Mahyar, Negareh Mahboubi, Aseem Partap Singh Gill, and Seyed Mohamad Moosavi. 2025. <span>“Adaptive Representation of Molecules and Materials in Bayesian Optimization.”</span> <em>Chemical Science</em> 16 (13): 5464–74. <a href="https://doi.org/10.1039/d5sc00200a">https://doi.org/10.1039/d5sc00200a</a>.
</div>
<div id="ref-rame2023rewarded" class="csl-entry" role="listitem">
Ramé, Alexandre, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, and Matthieu Cord. 2023. <span>“Rewarded Soups: Towards <span>Pareto</span>-Optimal Alignment by Interpolating Weights Fine-Tuned on Diverse Rewards.”</span> <em>Arxiv Preprint arXiv:2306.04488</em>, October. <a href="https://doi.org/10.48550/arXiv.2306.04488">https://doi.org/10.48550/arXiv.2306.04488</a>.
</div>
<div id="ref-ramos2023bayesian" class="csl-entry" role="listitem">
Ramos, Mayk Caldas, Shane S. Michtavy, Marc D. Porosoff, and Andrew D. White. 2023. <span>“Bayesian Optimization of Catalysis with in-Context Learning.”</span> <em>arXiv Preprint arXiv: 2304.05341</em>. <a href="https://doi.org/10.48550/arXiv.2304.05341">https://doi.org/10.48550/arXiv.2304.05341</a>.
</div>
<div id="ref-rankovic2023bochemian" class="csl-entry" role="listitem">
Ranković, Bojana, and Philippe Schwaller. 2023. <span>“BoChemian: Large Language Model Embeddings for Bayesian Optimization of Chemical Reactions.”</span> <em>NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World</em>. <a href="https://openreview.net/forum?id=A1RVn1m3J3">https://openreview.net/forum?id=A1RVn1m3J3</a>.
</div>
<div id="ref-rankovic2025gollum0" class="csl-entry" role="listitem">
———. 2025. <span>“GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning Through Bayesian Optimization.”</span> <em>arXiv Preprint arXiv: 2504.06265</em>. <a href="https://doi.org/10.48550/arXiv.2504.06265">https://doi.org/10.48550/arXiv.2504.06265</a>.
</div>
<div id="ref-riebesell2025framework" class="csl-entry" role="listitem">
Riebesell, Janosh, Rhys E. A. Goodall, Philipp Benner, Yuan Chiang, Bowen Deng, Gerbrand Ceder, Mark Asta, Alpha A. Lee, Anubhav Jain, and Kristin A. Persson. 2025. <span>“A Framework to Evaluate Machine Learning Crystal Stability Predictions.”</span> <em>Nature Machine Intelligence</em>. <a href="https://doi.org/10.1038/s42256-025-01055-1">https://doi.org/10.1038/s42256-025-01055-1</a>.
</div>
<div id="ref-schwaller2021mapping" class="csl-entry" role="listitem">
Schwaller, Philippe, Daniel Probst, Alain C. Vaucher, Vishnu H. Nair, David Kreutter, Teodoro Laino, and Jean-Louis Reymond. 2021. <span>“Mapping the Space of Chemical Reactions Using Attention-Based Neural Networks.”</span> <em>Nature Machine Intelligence</em> 3 (2): 144–52. <a href="https://doi.org/10.1038/s42256-020-00284-w">https://doi.org/10.1038/s42256-020-00284-w</a>.
</div>
<div id="ref-Shields2021bayesian" class="csl-entry" role="listitem">
Shields, Benjamin J., Jason Stevens, Jun Li, Marvin Parasram, Farhan Damani, Jesus I. Martinez Alvarado, Jacob M. Janey, Ryan P. Adams, and Abigail G. Doyle. 2021. <span>“Bayesian Reaction Optimization as a Tool for Chemical Synthesis.”</span> <em>Nature</em> 590 (7844): 89–96. <a href="https://doi.org/10.1038/s41586-021-03213-y">https://doi.org/10.1038/s41586-021-03213-y</a>.
</div>
<div id="ref-shoghi2023molecules" class="csl-entry" role="listitem">
Shoghi, Nima, Adeesh Kolluru, John R. Kitchin, Zachary W. Ulissi, C. L. Zitnick, and Brandon M. Wood. 2023. <span>“From Molecules to Materials: Pre-Training Large Generalizable Models for Atomic Property Prediction.”</span> <em>International Conference on Learning Representations</em>. <a href="https://doi.org/10.48550/arXiv.2310.16802">https://doi.org/10.48550/arXiv.2310.16802</a>.
</div>
<div id="ref-soares2025mamba-based" class="csl-entry" role="listitem">
Soares, Eduardo, Emilio Vital Brazil, Victor Shirasuna, Dmitry Zubarev, Renato Cerqueira, and Kristin Schmidt. 2025. <span>“A Mamba-Based Foundation Model for Materials.”</span> <em>Npj Artificial Intelligence</em> 1 (1): 1–8. <a href="https://doi.org/10.1038/s44387-025-00009-7">https://doi.org/10.1038/s44387-025-00009-7</a>.
</div>
<div id="ref-srinivas2024crossing" class="csl-entry" role="listitem">
Srinivas, Sakhinana Sagar, and Venkataramana Runkana. 2024a. <span>“Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based de Novo Molecule Design.”</span> <em>arXiv Preprint arXiv: 2408.11866</em>. <a href="https://doi.org/10.48550/arXiv.2408.11866">https://doi.org/10.48550/arXiv.2408.11866</a>.
</div>
<div id="ref-srinivas2024crossmodal" class="csl-entry" role="listitem">
———. 2024b. <span>“Cross-<span>Modal</span> <span>Learning</span> for <span>Chemistry</span> <span>Property</span> <span>Prediction</span>: <span>Large</span> <span>Language</span> <span>Models</span> <span>Meet</span> <span>Graph</span> <span>Machine</span> <span>Learning</span>.”</span> <em>Arxiv Preprint arXiv: 2408.14964</em>, August. <a href="https://doi.org/10.48550/arXiv.2408.14964">https://doi.org/10.48550/arXiv.2408.14964</a>.
</div>
<div id="ref-sriram2024flowllm" class="csl-entry" role="listitem">
Sriram, Anuroop, Benjamin Kurt Miller, Ricky T. Q. Chen, and Brandon M. Wood. 2024. <span>“<span>FlowLLM</span>: <span>Flow</span> <span>Matching</span> for <span>Material</span> <span>Generation</span> with <span>Large</span> <span>Language</span> <span>Models</span> as <span>Base</span> <span>Distributions</span>.”</span> <em>Arxiv Preprint arXiv</em>, October. <a href="https://doi.org/10.48550/arXiv.2410.23405">https://doi.org/10.48550/arXiv.2410.23405</a>.
</div>
<div id="ref-sun2025synllama" class="csl-entry" role="listitem">
Sun, Kunyang, Dorian Bagni, Joseph M. Cavanagh, Yingze Wang, Jacob M. Sawyer, Andrew Gritsevskiy, Oufan Zhang, and Teresa Head-Gordon. 2025. <span>“<span>SynLlama</span>: <span>Generating</span> <span>Synthesizable</span> <span>Molecules</span> and <span>Their</span> <span>Analogs</span> with <span>Large</span> <span>Language</span> <span>Models</span>.”</span> <em>Arxiv Preprint arXiv: 2503.12602</em>, April. <a href="https://doi.org/10.48550/arXiv.2503.12602">https://doi.org/10.48550/arXiv.2503.12602</a>.
</div>
<div id="ref-sypetkowski2024scalability" class="csl-entry" role="listitem">
Sypetkowski, Maciej, Frederik Wenkel, Farimah Poursafaei, Nia Dickson, Karush Suri, Philip Fradkin, and Dominique Beaini. 2024. <span>“On the Scalability of Gnns for Molecular Graphs.”</span> <em>Advances in Neural Information Processing Systems</em> 37: 19870–906. <a href="https://doi.org/10.48550/arXiv.2404.11568">https://doi.org/10.48550/arXiv.2404.11568</a>.
</div>
<div id="ref-Taylor2023brief" class="csl-entry" role="listitem">
Taylor, Connor J., Alexander Pomberger, Kobi C. Felton, Rachel Grainger, Magda Barecka, Thomas W. Chamberlain, Richard A. Bourne, Christopher N. Johnson, and Alexei A. Lapkin. 2023. <span>“A Brief Introduction to Chemical Reaction Optimization.”</span> <em>Chemical Reviews</em> 123 (6): 3089–3126. <a href="https://doi.org/10.1021/acs.chemrev.2c00798">https://doi.org/10.1021/acs.chemrev.2c00798</a>.
</div>
<div id="ref-vanherck2025assessment" class="csl-entry" role="listitem">
Van Herck, Joren, Marı́a Victoria Gil, Kevin Maik Jablonka, Alex Abrudan, Andy S. Anker, Mehrdad Asgari, Ben Blaiszik, et al. 2025. <span>“<span class="nocase">Assessment of fine-tuned large language models for real-world chemistry and material science applications</span>.”</span> <em>Chemical Science</em> 16 (2): 670–84. <a href="https://doi.org/10.1039/D4SC04401K">https://doi.org/10.1039/D4SC04401K</a>.
</div>
<div id="ref-Wang_2021" class="csl-entry" role="listitem">
Wang, Anthony Yu-Tung, Steven K. Kauwe, Ryan J. Murdock, and Taylor D. Sparks. 2021. <span>“<span class="nocase">Compositionally restricted attention-based network for materials property predictions</span>.”</span> <em>Npj Computational Materials</em> 7 (1). <a href="https://doi.org/10.1038/s41524-021-00545-1">https://doi.org/10.1038/s41524-021-00545-1</a>.
</div>
<div id="ref-wang2025llm0augmented" class="csl-entry" role="listitem">
Wang, Haorui, Jeff Guo, Lingkai Kong, Rampi Ramprasad, Philippe Schwaller, Yuanqi Du, and Chao Zhang. 2025. <span>“LLM-Augmented Chemical Synthesis and Design Decision Programs.”</span> <em>arXiv Preprint arXiv: 2505.07027</em>. <a href="https://doi.org/10.48550/arXiv.2505.07027">https://doi.org/10.48550/arXiv.2505.07027</a>.
</div>
<div id="ref-wang2024efficient" class="csl-entry" role="listitem">
Wang, Haorui, Marta Skreta, Cher Tian Ser, Wenhao Gao, Lingkai Kong, Felix Strieth-Kalthoff, Chenru Duan, et al. 2025. <span>“Efficient Evolutionary Search over Chemical Space with Large Language Models.”</span> <em>The Thirteenth International Conference on Learning Representations, <span>ICLR</span> 2025, Singapore, April 24-28, 2025</em>. <a href="https://doi.org/10.48550/arXiv.2406.16976">https://doi.org/10.48550/arXiv.2406.16976</a>.
</div>
<div id="ref-weininger1988smiles" class="csl-entry" role="listitem">
Weininger, David. 1988. <span>“<span>SMILES</span>, a Chemical Language and Information System. 1. <span>Introduction</span> to Methodology and Encoding Rules.”</span> <em>Journal of Chemical Information and Computer Sciences</em> 28 (1). <a href="https://doi.org/10.1021/ci00057a005">https://doi.org/10.1021/ci00057a005</a>.
</div>
<div id="ref-wu2018moleculenet" class="csl-entry" role="listitem">
Wu, Zhenqin, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay Pande. 2018. <span>“<span class="nocase">MoleculeNet: a benchmark for molecular machine learning</span>.”</span> <em>Chemical Science</em> 9 (2): 513–30. <a href="https://doi.org/10.1039/c7sc02664a">https://doi.org/10.1039/c7sc02664a</a>.
</div>
<div id="ref-yang2023large" class="csl-entry" role="listitem">
Yang, Chengrun, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. 2023. <span>“Large Language Models as Optimizers.”</span> <em>arXiv Preprint arXiv: 2309.03409</em>. <a href="https://doi.org/10.48550/arXiv.2309.03409">https://doi.org/10.48550/arXiv.2309.03409</a>.
</div>
<div id="ref-yoshikai2024novel" class="csl-entry" role="listitem">
Yoshikai, Yasuhiro, Tadahaya Mizuno, Shumpei Nemoto, and Hiroyuki Kusuhara. 2024. <span>“A Novel Molecule Generative Model of VAE Combined with Transformer for Unseen Structure Generation.”</span> <em>arXiv Preprint arXiv: 2402.11950</em>. <a href="https://doi.org/10.48550/arXiv.2402.11950">https://doi.org/10.48550/arXiv.2402.11950</a>.
</div>
<div id="ref-yu2024llasmol" class="csl-entry" role="listitem">
Yu, Botao, Frazier N. Baker, Ziqi Chen, Xia Ning, and Huan Sun. 2024. <span>“<span class="nocase">LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset</span>.”</span> <em>arXiv Preprint arXiv: 2402.09391</em>. <a href="https://doi.org/10.48550/arXiv.2402.09391">https://doi.org/10.48550/arXiv.2402.09391</a>.
</div>
<div id="ref-yu2025collaborative" class="csl-entry" role="listitem">
Yu, Jiajun, Yizhen Zheng, Huan Yee Koh, Shirui Pan, Tianyue Wang, and Haishuai Wang. 2025. <span>“Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2503.03503">https://doi.org/10.48550/arXiv.2503.03503</a>.
</div>
<div id="ref-zhang2025large" class="csl-entry" role="listitem">
Zhang, Yu, Yang Han, Shuai Chen, Ruijie Yu, Xin Zhao, Xianbin Liu, Kaipeng Zeng, et al. 2025. <span>“Large Language Models to Accelerate Organic Chemistry Synthesis.”</span> <em>Nature Machine Intelligence</em>. <a href="https://doi.org/10.1038/s42256-025-01066-y">https://doi.org/10.1038/s42256-025-01066-y</a>.
</div>
<div id="ref-zheng2025large" class="csl-entry" role="listitem">
Zheng, Yizhen, Huan Yee Koh, Jiaxin Ju, Anh T. N. Nguyen, Lauren T. May, Geoffrey I. Webb, and Shirui Pan. 2025. <span>“<span class="nocase">Large language models for scientific discovery in molecular property prediction</span>.”</span> <em>Nature Machine Intelligence</em> 7 (3): 437–47. <a href="https://doi.org/10.1038/s42256-025-00994-z">https://doi.org/10.1038/s42256-025-00994-z</a>.
</div>
<div id="ref-zhou2024one-preference-fits-all" class="csl-entry" role="listitem">
Zhou, Zhanhui, Jie Liu, Jing Shao, Xiangyu Yue, Chao Yang, Wanli Ouyang, and Yu Qiao. 2024. <span>“Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization.”</span> <em>Arxiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2310.03708">https://doi.org/10.48550/arXiv.2310.03708</a>.
</div>
<div id="ref-zhu20243m-diffusion" class="csl-entry" role="listitem">
Zhu, Huaisheng, Teng Xiao, and Vasant G. Honavar. 2024. <span>“<span>3M</span>-<span>Diffusion</span>: <span>Latent</span> <span>Multi</span>-<span>Modal</span> <span>Diffusion</span> for <span>Language</span>-<span>Guided</span> <span>Molecular</span> <span>Structure</span> <span>Generation</span>.”</span> <em>Arxiv Preprint</em>, October. <a href="https://doi.org/10.48550/arXiv.2403.07179">https://doi.org/10.48550/arXiv.2403.07179</a>.
</div>
<div id="ref-zunger2019beware" class="csl-entry" role="listitem">
Zunger, Alex. 2019. <span>“<span class="nocase">Beware of plausible predictions of fantasy materials</span>.”</span> <em>Nature</em> 566 (7745): 447–49. <a href="https://doi.org/10.1038/d41586-019-00676-y">https://doi.org/10.1038/d41586-019-00676-y</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./05-applications.html" class="pagination-link" aria-label="Applications">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Applications</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./07-safety.html" class="pagination-link" aria-label="Implications of GPMs: Education, Safety, and Ethics">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implications of GPMs: Education, Safety, and Ethics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><img src="https://raw.githubusercontent.com/lamalab-org/lamalab.github.io/main/static/png-file.png" alt="Lab for AI in Materials Science logo" style="height:1.4rem;vertical-align:middle;margin-right:0.4rem;"></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Copyright © 2025 Lab for AI in Materials Science</p>
</div>
  </div>
</footer>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>